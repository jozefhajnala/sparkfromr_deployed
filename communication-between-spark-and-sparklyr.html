<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Communication between Spark and sparklyr | Using Spark from R for performance with arbitrary code</title>
  <meta name="description" content="This bookdown publication attempts to provide practical insights into using the sparklyr interface to gain the benefits of Apache Spark while still retaining the ability to use R code organized in custom-built functions and packages." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Communication between Spark and sparklyr | Using Spark from R for performance with arbitrary code" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This bookdown publication attempts to provide practical insights into using the sparklyr interface to gain the benefits of Apache Spark while still retaining the ability to use R code organized in custom-built functions and packages." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Communication between Spark and sparklyr | Using Spark from R for performance with arbitrary code" />
  
  <meta name="twitter:description" content="This bookdown publication attempts to provide practical insights into using the sparklyr interface to gain the benefits of Apache Spark while still retaining the ability to use R code organized in custom-built functions and packages." />
  

<meta name="author" content="Jozef Hajnala" />


<meta name="date" content="2020-01-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="connecting-and-using-a-local-spark-instance.html"/>
<link rel="next" href="non-translated-functions-with-spark-apply.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1149069-22"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1149069-22');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="static/css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Spark from R for performance</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i><b>1.1</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="setting-up-spark-with-r-and-sparklyr.html"><a href="setting-up-spark-with-r-and-sparklyr.html"><i class="fa fa-check"></i><b>2</b> Setting up Spark with R and sparklyr</a><ul>
<li class="chapter" data-level="2.1" data-path="setting-up-spark-with-r-and-sparklyr.html"><a href="setting-up-spark-with-r-and-sparklyr.html#interactive-manual-installation"><i class="fa fa-check"></i><b>2.1</b> Interactive manual installation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html"><i class="fa fa-check"></i><b>3</b> Using a ready-made Docker Image</a><ul>
<li class="chapter" data-level="3.1" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#installing-docker"><i class="fa fa-check"></i><b>3.1</b> Installing Docker</a></li>
<li class="chapter" data-level="3.2" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#using-the-docker-image-with-r"><i class="fa fa-check"></i><b>3.2</b> Using the Docker image with R</a><ul>
<li class="chapter" data-level="3.2.1" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#interactively-with-rstudio"><i class="fa fa-check"></i><b>3.2.1</b> Interactively with RStudio</a></li>
<li class="chapter" data-level="3.2.2" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#interactively-with-the-r-console"><i class="fa fa-check"></i><b>3.2.2</b> Interactively with the R console</a></li>
<li class="chapter" data-level="3.2.3" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#running-an-example-r-script"><i class="fa fa-check"></i><b>3.2.3</b> Running an example R script</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#interactively-with-the-spark-shell"><i class="fa fa-check"></i><b>3.3</b> Interactively with the Spark shell</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="connecting-and-using-a-local-spark-instance.html"><a href="connecting-and-using-a-local-spark-instance.html"><i class="fa fa-check"></i><b>4</b> Connecting and using a local Spark instance</a><ul>
<li class="chapter" data-level="4.1" data-path="connecting-and-using-a-local-spark-instance.html"><a href="connecting-and-using-a-local-spark-instance.html#packages-and-data"><i class="fa fa-check"></i><b>4.1</b> Packages and data</a></li>
<li class="chapter" data-level="4.2" data-path="connecting-and-using-a-local-spark-instance.html"><a href="connecting-and-using-a-local-spark-instance.html#connecting-to-spark-and-providing-it-with-data"><i class="fa fa-check"></i><b>4.2</b> Connecting to Spark and providing it with data</a></li>
<li class="chapter" data-level="4.3" data-path="connecting-and-using-a-local-spark-instance.html"><a href="connecting-and-using-a-local-spark-instance.html#first-glance-at-the-data"><i class="fa fa-check"></i><b>4.3</b> First glance at the data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html"><i class="fa fa-check"></i><b>5</b> Communication between Spark and sparklyr</a><ul>
<li class="chapter" data-level="5.1" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#sparklyr-as-a-spark-interface-provider"><i class="fa fa-check"></i><b>5.1</b> Sparklyr as a Spark interface provider</a></li>
<li class="chapter" data-level="5.2" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#an-r-function-translated-to-spark-sql"><i class="fa fa-check"></i><b>5.2</b> An R function translated to Spark SQL</a><ul>
<li class="chapter" data-level="5.2.1" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#how-does-spark-know-the-r-function-tolower"><i class="fa fa-check"></i><b>5.2.1</b> How does Spark know the R function <code>tolower()</code>?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#an-r-function-not-translated-to-spark-sql"><i class="fa fa-check"></i><b>5.3</b> An R function not translated to Spark SQL</a></li>
<li class="chapter" data-level="5.4" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#a-hive-built-in-function-not-existing-in-r"><i class="fa fa-check"></i><b>5.4</b> A Hive built-in function not existing in R</a></li>
<li class="chapter" data-level="5.5" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#using-non-translated-functions-with-sparklyr"><i class="fa fa-check"></i><b>5.5</b> Using non-translated functions with sparklyr</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html"><i class="fa fa-check"></i><b>6</b> Non-translated functions with spark_apply</a><ul>
<li class="chapter" data-level="6.1" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#what-is-so-important-about-this-distinction"><i class="fa fa-check"></i><b>6.1</b> What is so important about this distinction?</a></li>
<li class="chapter" data-level="6.2" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#what-happens-when-we-use-custom-functions-with-spark_apply"><i class="fa fa-check"></i><b>6.2</b> What happens when we use custom functions with <code>spark_apply()</code></a></li>
<li class="chapter" data-level="6.3" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#what-happens-when-we-use-translated-or-hive-built-in-functions"><i class="fa fa-check"></i><b>6.3</b> What happens when we use translated or Hive built-in functions</a></li>
<li class="chapter" data-level="6.4" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#which-r-functionality-is-currently-translated-and-built-in-to-hive"><i class="fa fa-check"></i><b>6.4</b> Which R functionality is currently translated and built-in to Hive</a></li>
<li class="chapter" data-level="6.5" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#making-serialization-faster-with-apache-arrow"><i class="fa fa-check"></i><b>6.5</b> Making serialization faster with Apache Arrow</a><ul>
<li class="chapter" data-level="6.5.1" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#what-is-apache-arrow-and-how-it-improves-performance"><i class="fa fa-check"></i><b>6.5.1</b> What is Apache Arrow and how it improves performance</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#conclusion-take-home-messages"><i class="fa fa-check"></i><b>6.6</b> Conclusion, take-home messages</a></li>
<li class="chapter" data-level="6.7" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#but-we-still-need-arbitrary-functions-to-run-fast"><i class="fa fa-check"></i><b>6.7</b> But we still need arbitrary functions to run fast</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html"><i class="fa fa-check"></i><b>7</b> Constructing functions by piping dplyr verbs</a><ul>
<li class="chapter" data-level="7.1" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#r-functions-as-combinations-of-dplyr-verbs-and-spark"><i class="fa fa-check"></i><b>7.1</b> R functions as combinations of dplyr verbs and Spark</a><ul>
<li class="chapter" data-level="7.1.1" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#trying-it-with-base-r-functions"><i class="fa fa-check"></i><b>7.1.1</b> Trying it with base R functions</a></li>
<li class="chapter" data-level="7.1.2" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#using-a-combination-of-supported-dplyr-verbs-and-operations"><i class="fa fa-check"></i><b>7.1.2</b> Using a combination of supported dplyr verbs and operations</a></li>
<li class="chapter" data-level="7.1.3" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#investigating-the-sql-translation-and-its-spark-plan"><i class="fa fa-check"></i><b>7.1.3</b> Investigating the SQL translation and its Spark plan</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#a-more-complex-use-case---joins-group-bys-and-aggregations"><i class="fa fa-check"></i><b>7.2</b> A more complex use case - Joins, group bys, and aggregations</a></li>
<li class="chapter" data-level="7.3" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#using-the-functions-with-local-versus-remote-datasets"><i class="fa fa-check"></i><b>7.3</b> Using the functions with local versus remote datasets</a><ul>
<li class="chapter" data-level="7.3.1" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#unified-front-end-different-back-ends"><i class="fa fa-check"></i><b>7.3.1</b> Unified front-end, different back-ends</a></li>
<li class="chapter" data-level="7.3.2" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#differences-in-na-and-nan-values"><i class="fa fa-check"></i><b>7.3.2</b> Differences in <code>NA</code> and <code>NaN</code> values</a></li>
<li class="chapter" data-level="7.3.3" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#dates-times-and-time-zones"><i class="fa fa-check"></i><b>7.3.3</b> Dates, times and time zones</a></li>
<li class="chapter" data-level="7.3.4" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#joins"><i class="fa fa-check"></i><b>7.3.4</b> Joins</a></li>
<li class="chapter" data-level="7.3.5" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#portability-of-used-methods"><i class="fa fa-check"></i><b>7.3.5</b> Portability of used methods</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#conclusion-take-home-messages-1"><i class="fa fa-check"></i><b>7.4</b> Conclusion, take-home messages</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html"><i class="fa fa-check"></i><b>8</b> Constructing SQL and executing it with Spark</a><ul>
<li class="chapter" data-level="8.1" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#r-functions-as-spark-sql-generators"><i class="fa fa-check"></i><b>8.1</b> R functions as Spark SQL generators</a></li>
<li class="chapter" data-level="8.2" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#executing-the-generated-queries-via-spark"><i class="fa fa-check"></i><b>8.2</b> Executing the generated queries via Spark</a><ul>
<li class="chapter" data-level="8.2.1" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#using-dbi-as-the-interface"><i class="fa fa-check"></i><b>8.2.1</b> Using DBI as the interface</a></li>
<li class="chapter" data-level="8.2.2" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#invoking-sql-on-a-spark-session-object"><i class="fa fa-check"></i><b>8.2.2</b> Invoking sql on a Spark session object</a></li>
<li class="chapter" data-level="8.2.3" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#using-tbl-with-dbplyrs-sql"><i class="fa fa-check"></i><b>8.2.3</b> Using tbl with dbplyr’s sql</a></li>
<li class="chapter" data-level="8.2.4" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#wrapping-the-tbl-approach-into-functions"><i class="fa fa-check"></i><b>8.2.4</b> Wrapping the tbl approach into functions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#where-sql-can-be-better-than-dbplyr-translation"><i class="fa fa-check"></i><b>8.3</b> Where SQL can be better than dbplyr translation</a><ul>
<li class="chapter" data-level="8.3.1" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#when-a-translation-is-not-there"><i class="fa fa-check"></i><b>8.3.1</b> When a translation is not there</a></li>
<li class="chapter" data-level="8.3.2" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#when-translation-does-not-provide-expected-results"><i class="fa fa-check"></i><b>8.3.2</b> When translation does not provide expected results</a></li>
<li class="chapter" data-level="8.3.3" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#when-portability-is-important"><i class="fa fa-check"></i><b>8.3.3</b> When portability is important</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><i class="fa fa-check"></i><b>9</b> Using the lower-level invoke API to manipulate Spark’s Java objects from R</a><ul>
<li class="chapter" data-level="9.1" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#the-invoke-api-of-sparklyr"><i class="fa fa-check"></i><b>9.1</b> The invoke() API of sparklyr</a></li>
<li class="chapter" data-level="9.2" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#getting-started-with-the-invoke-api"><i class="fa fa-check"></i><b>9.2</b> Getting started with the invoke API</a></li>
<li class="chapter" data-level="9.3" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#grouping-and-aggregation-with-invoke-chains"><i class="fa fa-check"></i><b>9.3</b> Grouping and aggregation with invoke chains</a><ul>
<li class="chapter" data-level="9.3.1" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#what-is-all-that-extra-code"><i class="fa fa-check"></i><b>9.3.1</b> What is all that extra code?</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#wrapping-the-invocations-into-r-functions"><i class="fa fa-check"></i><b>9.4</b> Wrapping the invocations into R functions</a></li>
<li class="chapter" data-level="9.5" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#reconstructing-variable-normalization"><i class="fa fa-check"></i><b>9.5</b> Reconstructing variable normalization</a></li>
<li class="chapter" data-level="9.6" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#where-invoke-can-be-better-than-dplyr-translation-or-sql"><i class="fa fa-check"></i><b>9.6</b> Where invoke can be better than dplyr translation or SQL</a></li>
<li class="chapter" data-level="9.7" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#conclusion"><i class="fa fa-check"></i><b>9.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><i class="fa fa-check"></i><b>10</b> Exploring the invoke API from R with Java reflection and examining invokes with logs</a><ul>
<li class="chapter" data-level="10.1" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#examining-available-methods-from-r"><i class="fa fa-check"></i><b>10.1</b> Examining available methods from R</a></li>
<li class="chapter" data-level="10.2" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-the-java-reflection-api-to-list-the-available-methods"><i class="fa fa-check"></i><b>10.2</b> Using the Java reflection API to list the available methods</a></li>
<li class="chapter" data-level="10.3" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#investigating-dataset-and-sparkcontext-class-methods"><i class="fa fa-check"></i><b>10.3</b> Investigating DataSet and SparkContext class methods</a><ul>
<li class="chapter" data-level="10.3.1" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-helpers-to-explore-the-methods"><i class="fa fa-check"></i><b>10.3.1</b> Using helpers to explore the methods</a></li>
<li class="chapter" data-level="10.3.2" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#unexported-helpers-provided-by-sparklyr"><i class="fa fa-check"></i><b>10.3.2</b> Unexported helpers provided by sparklyr</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#how-sparklyr-communicates-with-spark-invoke-logging"><i class="fa fa-check"></i><b>10.4</b> How sparklyr communicates with Spark, invoke logging</a><ul>
<li class="chapter" data-level="10.4.1" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-dplyr-verbs-translated-with-dbplyr"><i class="fa fa-check"></i><b>10.4.1</b> Using dplyr verbs translated with dbplyr</a></li>
<li class="chapter" data-level="10.4.2" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-dbi-to-send-queries"><i class="fa fa-check"></i><b>10.4.2</b> Using DBI to send queries</a></li>
<li class="chapter" data-level="10.4.3" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-the-invoke-interface"><i class="fa fa-check"></i><b>10.4.3</b> Using the invoke interface</a></li>
<li class="chapter" data-level="10.4.4" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#redirecting-the-invoke-logs"><i class="fa fa-check"></i><b>10.4.4</b> Redirecting the invoke logs</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#conclusion-1"><i class="fa fa-check"></i><b>10.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="combining-approaches-into-lazy-datasets.html"><a href="combining-approaches-into-lazy-datasets.html"><i class="fa fa-check"></i><b>11</b> Combining approaches into lazy datasets</a></li>
<li class="chapter" data-level="12" data-path="references-and-resources.html"><a href="references-and-resources.html"><i class="fa fa-check"></i><b>12</b> References and resources</a><ul>
<li class="chapter" data-level="12.1" data-path="references-and-resources.html"><a href="references-and-resources.html#online-resources"><i class="fa fa-check"></i><b>12.1</b> Online resources</a><ul>
<li class="chapter" data-level="12.1.1" data-path="references-and-resources.html"><a href="references-and-resources.html#getting-started-with-sparklyr"><i class="fa fa-check"></i><b>12.1.1</b> Getting started with sparklyr</a></li>
<li class="chapter" data-level="12.1.2" data-path="references-and-resources.html"><a href="references-and-resources.html#dbi-spark-sql-hive"><i class="fa fa-check"></i><b>12.1.2</b> DBI, Spark SQL, Hive</a></li>
<li class="chapter" data-level="12.1.3" data-path="references-and-resources.html"><a href="references-and-resources.html#docker"><i class="fa fa-check"></i><b>12.1.3</b> Docker</a></li>
<li class="chapter" data-level="12.1.4" data-path="references-and-resources.html"><a href="references-and-resources.html#spark-api-java-scala-and-friends"><i class="fa fa-check"></i><b>12.1.4</b> Spark API, Java, Scala and friends</a></li>
<li class="chapter" data-level="12.1.5" data-path="references-and-resources.html"><a href="references-and-resources.html#apache-arrow"><i class="fa fa-check"></i><b>12.1.5</b> Apache Arrow</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="references-and-resources.html"><a href="references-and-resources.html#physical-books"><i class="fa fa-check"></i><b>12.2</b> Physical Books</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i><b>13</b> Appendices</a><ul>
<li class="chapter" data-level="13.1" data-path="appendices.html"><a href="appendices.html#r-session-information"><i class="fa fa-check"></i><b>13.1</b> R session information</a></li>
<li class="chapter" data-level="13.2" data-path="appendices.html"><a href="appendices.html#setup-of-apache-arrow"><i class="fa fa-check"></i><b>13.2</b> Setup of Apache Arrow</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Using Spark from R for performance with arbitrary code</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="communication-between-spark-and-sparklyr" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Communication between Spark and sparklyr</h1>
<p>In this chapter, we will examine how the <span class="rpackage">sparklyr</span> interface communicates with the Spark instance and what this means for performance with regards to arbitrarily defined R functions. We will also look at how Apache Arrow can improve the performance of object serialization.</p>
<div id="sparklyr-as-a-spark-interface-provider" class="section level2">
<h2><span class="header-section-number">5.1</span> Sparklyr as a Spark interface provider</h2>
<p>The <span class="rpackage">sparklyr</span> package is an R-based <strong>interface</strong> to Apache Spark. The meaning of the word interface is very important in this context as the way we use this interface can significantly affect the performance benefits we get from using Spark.</p>
<p>To understand the meaning of the above a bit better, we will examine 3 very simple functions that are different in implementation but intend to provide the same results, and how they behave with regards to Spark. Our goal will be completely trivial - convert the <code>origin</code> column that contains the airport of origin of flights from uppercase to all lowercase. We will keep using the datasets from the <span class="rpackage">nycflights13</span> package for our examples.</p>
</div>
<div id="an-r-function-translated-to-spark-sql" class="section level2">
<h2><span class="header-section-number">5.2</span> An R function translated to Spark SQL</h2>
<p>Using the following <code>fun_implemented()</code> function will yield the expected results for both a local data frame <code>weather</code> and the remote Spark object referenced by <code>tbl_weather</code>.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="co"># An R function `tolower` translated to Spark SQL</span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2">fun_implemented &lt;-<span class="st"> </span><span class="cf">function</span>(df, col) {</a>
<a class="sourceLine" id="cb14-3" data-line-number="3">  df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>({{col}} <span class="op">:</span><span class="er">=</span><span class="st"> </span><span class="kw">tolower</span>({{col}}))</a>
<a class="sourceLine" id="cb14-4" data-line-number="4">}</a></code></pre></div>
<p>First, let us run <code>fun_implemented</code> for a local data frame in our R session. Note that the output of the command is <code>A tibble: 26,115 x 16</code>, meaning this is an object in our local R session.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="kw">fun_implemented</span>(weather, origin)</a></code></pre></div>
<pre><code>## # A tibble: 26,115 x 16
##       id origin  year month   day  hour  temp  dewp humid wind_dir wind_speed
##    &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
##  1     1 ewr     2013     1     1     1  39.0  26.1  59.4      270      10.4 
##  2     2 ewr     2013     1     1     2  39.0  27.0  61.6      250       8.06
##  3     3 ewr     2013     1     1     3  39.0  28.0  64.4      240      11.5 
##  4     4 ewr     2013     1     1     4  39.9  28.0  62.2      250      12.7 
##  5     5 ewr     2013     1     1     5  39.0  28.0  64.4      260      12.7 
##  6     6 ewr     2013     1     1     6  37.9  28.0  67.2      240      11.5 
##  7     7 ewr     2013     1     1     7  39.0  28.0  64.4      240      15.0 
##  8     8 ewr     2013     1     1     8  39.9  28.0  62.2      250      10.4 
##  9     9 ewr     2013     1     1     9  39.9  28.0  62.2      260      15.0 
## 10    10 ewr     2013     1     1    10  41    28.0  59.6      260      13.8 
## # … with 26,105 more rows, and 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;,
## #   pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;</code></pre>
<p>Next, we use it against a remote Spark DataFrame. Notice that here the output is a remote object with <code>Source: spark&lt;?&gt; [?? x 16]</code> and once again, Spark only executed the minimal work to show this printout, so we do not yet know how many lines in total are in the resulting DataFrame.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="kw">fun_implemented</span>(tbl_weather, origin)</a></code></pre></div>
<pre><code>## # Source: spark&lt;?&gt; [?? x 16]
##       id origin  year month   day  hour  temp  dewp humid wind_dir wind_speed
##    &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
##  1     1 ewr     2013     1     1     1  39.0  26.1  59.4      270      10.4 
##  2     2 ewr     2013     1     1     2  39.0  27.0  61.6      250       8.06
##  3     3 ewr     2013     1     1     3  39.0  28.0  64.4      240      11.5 
##  4     4 ewr     2013     1     1     4  39.9  28.0  62.2      250      12.7 
##  5     5 ewr     2013     1     1     5  39.0  28.0  64.4      260      12.7 
##  6     6 ewr     2013     1     1     6  37.9  28.0  67.2      240      11.5 
##  7     7 ewr     2013     1     1     7  39.0  28.0  64.4      240      15.0 
##  8     8 ewr     2013     1     1     8  39.9  28.0  62.2      250      10.4 
##  9     9 ewr     2013     1     1     9  39.9  28.0  62.2      260      15.0 
## 10    10 ewr     2013     1     1    10  41    28.0  59.6      260      13.8 
## # … with more rows, and 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;,
## #   pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;</code></pre>
<div id="how-does-spark-know-the-r-function-tolower" class="section level3">
<h3><span class="header-section-number">5.2.1</span> How does Spark know the R function <code>tolower()</code>?</h3>
<p>Actually, it does not. Our function call worked within Spark because the R function <code>tolower()</code> was translated by the functionality of the dbplyr package to Spark SQL - converting the R <code>tolower()</code> function to <code>LOWER</code>, which is a function available in Spark SQL. The resulting query was then sent to Spark to be executed.</p>
<p>This is the main mode of operation of the sparklyr interface - translating our R code to Spark SQL code and using Spark’s SQL API to execute it. We can see the actual translated SQL by running <code>sql_render()</code> on the above function call.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1">dbplyr<span class="op">::</span><span class="kw">sql_render</span>(</a>
<a class="sourceLine" id="cb19-2" data-line-number="2">  <span class="kw">fun_implemented</span>(tbl_weather, origin)</a>
<a class="sourceLine" id="cb19-3" data-line-number="3">)</a></code></pre></div>
<pre><code>## &lt;SQL&gt; SELECT `id`, LOWER(`origin`) AS `origin`, `year`, `month`, `day`, `hour`, `temp`, `dewp`, `humid`, `wind_dir`, `wind_speed`, `wind_gust`, `precip`, `pressure`, `visib`, `time_hour`
## FROM `weather`</code></pre>
</div>
</div>
<div id="an-r-function-not-translated-to-spark-sql" class="section level2">
<h2><span class="header-section-number">5.3</span> An R function not translated to Spark SQL</h2>
<p>Using the following <code>fun_r_only()</code> function will only yield the expected results for a local data frame <code>weather</code>. For the remote Spark object referenced by <code>tbl_weather</code> we will get an error:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="co"># An R function `casefold` not translated to Spark SQL</span></a>
<a class="sourceLine" id="cb21-2" data-line-number="2">fun_r_only &lt;-<span class="st"> </span><span class="cf">function</span>(df, col) {</a>
<a class="sourceLine" id="cb21-3" data-line-number="3">  df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>({{col}} <span class="op">:</span><span class="er">=</span><span class="st"> </span><span class="kw">casefold</span>({{col}}, <span class="dt">upper =</span> <span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb21-4" data-line-number="4">}</a></code></pre></div>
<p>The function executes successfully on a local R data frame as R knows the function <code>casefold()</code>:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1"><span class="kw">fun_r_only</span>(weather, origin)</a></code></pre></div>
<pre><code>## # A tibble: 26,115 x 16
##       id origin  year month   day  hour  temp  dewp humid wind_dir wind_speed
##    &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
##  1     1 ewr     2013     1     1     1  39.0  26.1  59.4      270      10.4 
##  2     2 ewr     2013     1     1     2  39.0  27.0  61.6      250       8.06
##  3     3 ewr     2013     1     1     3  39.0  28.0  64.4      240      11.5 
##  4     4 ewr     2013     1     1     4  39.9  28.0  62.2      250      12.7 
##  5     5 ewr     2013     1     1     5  39.0  28.0  64.4      260      12.7 
##  6     6 ewr     2013     1     1     6  37.9  28.0  67.2      240      11.5 
##  7     7 ewr     2013     1     1     7  39.0  28.0  64.4      240      15.0 
##  8     8 ewr     2013     1     1     8  39.9  28.0  62.2      250      10.4 
##  9     9 ewr     2013     1     1     9  39.9  28.0  62.2      260      15.0 
## 10    10 ewr     2013     1     1    10  41    28.0  59.6      260      13.8 
## # … with 26,105 more rows, and 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;,
## #   pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;</code></pre>
<p>Trying to execute <code>fun_r_only()</code> against a Spark DataFrame however errors:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1"><span class="kw">fun_r_only</span>(tbl_weather, origin)</a></code></pre></div>
<pre><code>## Error: org.apache.spark.sql.catalyst.parser.ParseException: 
## mismatched input &#39;AS&#39; expecting &#39;)&#39;(line 1, pos 38)
## 
## == SQL ==
## SELECT `id`, casefold(`origin`, FALSE AS `upper`) AS `origin`, `year`, `month`, `day`, `hour`, `temp`, `dewp`, `humid`, `wind_dir`, `wind_speed`, `wind_gust`, `precip`, `pressure`, `visib`, `time_hour`
...</code></pre>
<p>This is because there simply is no translation provided by dbplyr for the <code>casefold()</code> function. The generated Spark SQL will therefore not be valid and throw an error once the Spark SQL parser tries to parse it.</p>
</div>
<div id="a-hive-built-in-function-not-existing-in-r" class="section level2">
<h2><span class="header-section-number">5.4</span> A Hive built-in function not existing in R</h2>
<p>On the other hand, using the below <code>fun_hive_builtin()</code> function will only yield the expected results for the remote Spark object referenced by <code>tbl_weather</code>. For the local data frame <code>weather</code> we will get an error:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="co"># A Hive built-in function `lower` not existing in R</span></a>
<a class="sourceLine" id="cb26-2" data-line-number="2">fun_hive_builtin &lt;-<span class="st"> </span><span class="cf">function</span>(df, col) {</a>
<a class="sourceLine" id="cb26-3" data-line-number="3">  df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>({{col}} <span class="op">:</span><span class="er">=</span><span class="st"> </span><span class="kw">lower</span>({{col}}))</a>
<a class="sourceLine" id="cb26-4" data-line-number="4">}</a></code></pre></div>
<p>The function fails to execute on a local R data frame as R does not know the function <code>lower()</code>:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="kw">fun_hive_builtin</span>(weather, origin)</a></code></pre></div>
<pre><code>## Error in lower(~origin): could not find function &quot;lower&quot;</code></pre>
<p>However, against a Spark DataFrame the code works as desired:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" data-line-number="1"><span class="kw">fun_hive_builtin</span>(tbl_weather, origin)</a></code></pre></div>
<pre><code>## # Source: spark&lt;?&gt; [?? x 16]
##       id origin  year month   day  hour  temp  dewp humid wind_dir wind_speed
##    &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
##  1     1 ewr     2013     1     1     1  39.0  26.1  59.4      270      10.4 
##  2     2 ewr     2013     1     1     2  39.0  27.0  61.6      250       8.06
##  3     3 ewr     2013     1     1     3  39.0  28.0  64.4      240      11.5 
##  4     4 ewr     2013     1     1     4  39.9  28.0  62.2      250      12.7 
##  5     5 ewr     2013     1     1     5  39.0  28.0  64.4      260      12.7 
##  6     6 ewr     2013     1     1     6  37.9  28.0  67.2      240      11.5 
##  7     7 ewr     2013     1     1     7  39.0  28.0  64.4      240      15.0 
##  8     8 ewr     2013     1     1     8  39.9  28.0  62.2      250      10.4 
##  9     9 ewr     2013     1     1     9  39.9  28.0  62.2      260      15.0 
## 10    10 ewr     2013     1     1    10  41    28.0  59.6      260      13.8 
## # … with more rows, and 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;,
## #   pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;</code></pre>
<p>This is because, as seen above the function <code>lower()</code> does not exist in R itself. For a non-existing R function there obviously can be no dbplyr translation either. In this case, dbplyr keeps it as-is when translating to SQL, not doing any translation.</p>
<p>The SQL will be valid and executed without problems because <code>lower</code> is, in fact, a function built-in to Hive, so the following generated SQL is valid.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb31-1" data-line-number="1">dbplyr<span class="op">::</span><span class="kw">sql_render</span>(<span class="kw">fun_hive_builtin</span>(tbl_weather, origin))</a></code></pre></div>
<pre><code>## &lt;SQL&gt; SELECT `id`, lower(`origin`) AS `origin`, `year`, `month`, `day`, `hour`, `temp`, `dewp`, `humid`, `wind_dir`, `wind_speed`, `wind_gust`, `precip`, `pressure`, `visib`, `time_hour`
## FROM `weather`</code></pre>
</div>
<div id="using-non-translated-functions-with-sparklyr" class="section level2">
<h2><span class="header-section-number">5.5</span> Using non-translated functions with sparklyr</h2>
<p>It can easily happen that one of the functions we want to use falls into the category where it is neither translated or a Hive built-in function. In this case, there is another interface provided by sparklyr that can allow us to do that - the <code>spark_apply()</code> function. We will look into this interface in more detail in <a href="non-translated-functions-with-spark-apply.html">the next chapter</a>.</p>
<p>There is also a lower-level API provided by sparklyr allowing us to invoke Scala methods without using SQL translation. We discuss this API in detail in the <a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html">Using the lower-level invoke API to manipulate Spark’s Java objects from R</a> and <a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html">Exploring the invoke API from R with Java reflection and examining invokes with logs</a> chapters.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="connecting-and-using-a-local-spark-instance.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="non-translated-functions-with-spark-apply.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
