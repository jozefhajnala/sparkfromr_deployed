<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Constructing SQL and executing it with Spark | Using Spark from R for performance with arbitrary code</title>
  <meta name="description" content="This bookdown publication attempts to provide practical insights into using the sparklyr interface to gain the benefits of Apache Spark while still retaining the ability to use R code organized in custom-built functions and packages." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Constructing SQL and executing it with Spark | Using Spark from R for performance with arbitrary code" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This bookdown publication attempts to provide practical insights into using the sparklyr interface to gain the benefits of Apache Spark while still retaining the ability to use R code organized in custom-built functions and packages." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Constructing SQL and executing it with Spark | Using Spark from R for performance with arbitrary code" />
  
  <meta name="twitter:description" content="This bookdown publication attempts to provide practical insights into using the sparklyr interface to gain the benefits of Apache Spark while still retaining the ability to use R code organized in custom-built functions and packages." />
  

<meta name="author" content="Jozef Hajnala" />


<meta name="date" content="2020-01-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="constructing-functions-by-piping-dplyr-verbs.html"/>
<link rel="next" href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1149069-22"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1149069-22');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="static/css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Spark from R for performance</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i><b>1.1</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="setting-up-spark-with-r-and-sparklyr.html"><a href="setting-up-spark-with-r-and-sparklyr.html"><i class="fa fa-check"></i><b>2</b> Setting up Spark with R and sparklyr</a><ul>
<li class="chapter" data-level="2.1" data-path="setting-up-spark-with-r-and-sparklyr.html"><a href="setting-up-spark-with-r-and-sparklyr.html#interactive-manual-installation"><i class="fa fa-check"></i><b>2.1</b> Interactive manual installation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html"><i class="fa fa-check"></i><b>3</b> Using a ready-made Docker Image</a><ul>
<li class="chapter" data-level="3.1" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#installing-docker"><i class="fa fa-check"></i><b>3.1</b> Installing Docker</a></li>
<li class="chapter" data-level="3.2" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#using-the-docker-image-with-r"><i class="fa fa-check"></i><b>3.2</b> Using the Docker image with R</a><ul>
<li class="chapter" data-level="3.2.1" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#interactively-with-rstudio"><i class="fa fa-check"></i><b>3.2.1</b> Interactively with RStudio</a></li>
<li class="chapter" data-level="3.2.2" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#interactively-with-the-r-console"><i class="fa fa-check"></i><b>3.2.2</b> Interactively with the R console</a></li>
<li class="chapter" data-level="3.2.3" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#running-an-example-r-script"><i class="fa fa-check"></i><b>3.2.3</b> Running an example R script</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#interactively-with-the-spark-shell"><i class="fa fa-check"></i><b>3.3</b> Interactively with the Spark shell</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="connecting-and-using-a-local-spark-instance.html"><a href="connecting-and-using-a-local-spark-instance.html"><i class="fa fa-check"></i><b>4</b> Connecting and using a local Spark instance</a><ul>
<li class="chapter" data-level="4.1" data-path="connecting-and-using-a-local-spark-instance.html"><a href="connecting-and-using-a-local-spark-instance.html#packages-and-data"><i class="fa fa-check"></i><b>4.1</b> Packages and data</a></li>
<li class="chapter" data-level="4.2" data-path="connecting-and-using-a-local-spark-instance.html"><a href="connecting-and-using-a-local-spark-instance.html#connecting-to-spark-and-providing-it-with-data"><i class="fa fa-check"></i><b>4.2</b> Connecting to Spark and providing it with data</a></li>
<li class="chapter" data-level="4.3" data-path="connecting-and-using-a-local-spark-instance.html"><a href="connecting-and-using-a-local-spark-instance.html#first-glance-at-the-data"><i class="fa fa-check"></i><b>4.3</b> First glance at the data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html"><i class="fa fa-check"></i><b>5</b> Communication between Spark and sparklyr</a><ul>
<li class="chapter" data-level="5.1" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#sparklyr-as-a-spark-interface-provider"><i class="fa fa-check"></i><b>5.1</b> Sparklyr as a Spark interface provider</a></li>
<li class="chapter" data-level="5.2" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#an-r-function-translated-to-spark-sql"><i class="fa fa-check"></i><b>5.2</b> An R function translated to Spark SQL</a><ul>
<li class="chapter" data-level="5.2.1" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#how-does-spark-know-the-r-function-tolower"><i class="fa fa-check"></i><b>5.2.1</b> How does Spark know the R function <code>tolower()</code>?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#an-r-function-not-translated-to-spark-sql"><i class="fa fa-check"></i><b>5.3</b> An R function not translated to Spark SQL</a></li>
<li class="chapter" data-level="5.4" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#a-hive-built-in-function-not-existing-in-r"><i class="fa fa-check"></i><b>5.4</b> A Hive built-in function not existing in R</a></li>
<li class="chapter" data-level="5.5" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#using-non-translated-functions-with-sparklyr"><i class="fa fa-check"></i><b>5.5</b> Using non-translated functions with sparklyr</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html"><i class="fa fa-check"></i><b>6</b> Non-translated functions with spark_apply</a><ul>
<li class="chapter" data-level="6.1" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#what-is-so-important-about-this-distinction"><i class="fa fa-check"></i><b>6.1</b> What is so important about this distinction?</a></li>
<li class="chapter" data-level="6.2" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#what-happens-when-we-use-custom-functions-with-spark_apply"><i class="fa fa-check"></i><b>6.2</b> What happens when we use custom functions with <code>spark_apply()</code></a></li>
<li class="chapter" data-level="6.3" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#what-happens-when-we-use-translated-or-hive-built-in-functions"><i class="fa fa-check"></i><b>6.3</b> What happens when we use translated or Hive built-in functions</a></li>
<li class="chapter" data-level="6.4" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#which-r-functionality-is-currently-translated-and-built-in-to-hive"><i class="fa fa-check"></i><b>6.4</b> Which R functionality is currently translated and built-in to Hive</a></li>
<li class="chapter" data-level="6.5" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#making-serialization-faster-with-apache-arrow"><i class="fa fa-check"></i><b>6.5</b> Making serialization faster with Apache Arrow</a><ul>
<li class="chapter" data-level="6.5.1" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#what-is-apache-arrow-and-how-it-improves-performance"><i class="fa fa-check"></i><b>6.5.1</b> What is Apache Arrow and how it improves performance</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#conclusion-take-home-messages"><i class="fa fa-check"></i><b>6.6</b> Conclusion, take-home messages</a></li>
<li class="chapter" data-level="6.7" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#but-we-still-need-arbitrary-functions-to-run-fast"><i class="fa fa-check"></i><b>6.7</b> But we still need arbitrary functions to run fast</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html"><i class="fa fa-check"></i><b>7</b> Constructing functions by piping dplyr verbs</a><ul>
<li class="chapter" data-level="7.1" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#r-functions-as-combinations-of-dplyr-verbs-and-spark"><i class="fa fa-check"></i><b>7.1</b> R functions as combinations of dplyr verbs and Spark</a><ul>
<li class="chapter" data-level="7.1.1" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#trying-it-with-base-r-functions"><i class="fa fa-check"></i><b>7.1.1</b> Trying it with base R functions</a></li>
<li class="chapter" data-level="7.1.2" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#using-a-combination-of-supported-dplyr-verbs-and-operations"><i class="fa fa-check"></i><b>7.1.2</b> Using a combination of supported dplyr verbs and operations</a></li>
<li class="chapter" data-level="7.1.3" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#investigating-the-sql-translation-and-its-spark-plan"><i class="fa fa-check"></i><b>7.1.3</b> Investigating the SQL translation and its Spark plan</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#a-more-complex-use-case---joins-group-bys-and-aggregations"><i class="fa fa-check"></i><b>7.2</b> A more complex use case - Joins, group bys, and aggregations</a></li>
<li class="chapter" data-level="7.3" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#using-the-functions-with-local-versus-remote-datasets"><i class="fa fa-check"></i><b>7.3</b> Using the functions with local versus remote datasets</a><ul>
<li class="chapter" data-level="7.3.1" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#unified-front-end-different-back-ends"><i class="fa fa-check"></i><b>7.3.1</b> Unified front-end, different back-ends</a></li>
<li class="chapter" data-level="7.3.2" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#differences-in-na-and-nan-values"><i class="fa fa-check"></i><b>7.3.2</b> Differences in <code>NA</code> and <code>NaN</code> values</a></li>
<li class="chapter" data-level="7.3.3" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#dates-times-and-time-zones"><i class="fa fa-check"></i><b>7.3.3</b> Dates, times and time zones</a></li>
<li class="chapter" data-level="7.3.4" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#joins"><i class="fa fa-check"></i><b>7.3.4</b> Joins</a></li>
<li class="chapter" data-level="7.3.5" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#portability-of-used-methods"><i class="fa fa-check"></i><b>7.3.5</b> Portability of used methods</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#conclusion-take-home-messages-1"><i class="fa fa-check"></i><b>7.4</b> Conclusion, take-home messages</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html"><i class="fa fa-check"></i><b>8</b> Constructing SQL and executing it with Spark</a><ul>
<li class="chapter" data-level="8.1" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#r-functions-as-spark-sql-generators"><i class="fa fa-check"></i><b>8.1</b> R functions as Spark SQL generators</a></li>
<li class="chapter" data-level="8.2" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#executing-the-generated-queries-via-spark"><i class="fa fa-check"></i><b>8.2</b> Executing the generated queries via Spark</a><ul>
<li class="chapter" data-level="8.2.1" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#using-dbi-as-the-interface"><i class="fa fa-check"></i><b>8.2.1</b> Using DBI as the interface</a></li>
<li class="chapter" data-level="8.2.2" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#invoking-sql-on-a-spark-session-object"><i class="fa fa-check"></i><b>8.2.2</b> Invoking sql on a Spark session object</a></li>
<li class="chapter" data-level="8.2.3" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#using-tbl-with-dbplyrs-sql"><i class="fa fa-check"></i><b>8.2.3</b> Using tbl with dbplyr’s sql</a></li>
<li class="chapter" data-level="8.2.4" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#wrapping-the-tbl-approach-into-functions"><i class="fa fa-check"></i><b>8.2.4</b> Wrapping the tbl approach into functions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#where-sql-can-be-better-than-dbplyr-translation"><i class="fa fa-check"></i><b>8.3</b> Where SQL can be better than dbplyr translation</a><ul>
<li class="chapter" data-level="8.3.1" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#when-a-translation-is-not-there"><i class="fa fa-check"></i><b>8.3.1</b> When a translation is not there</a></li>
<li class="chapter" data-level="8.3.2" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#when-translation-does-not-provide-expected-results"><i class="fa fa-check"></i><b>8.3.2</b> When translation does not provide expected results</a></li>
<li class="chapter" data-level="8.3.3" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#when-portability-is-important"><i class="fa fa-check"></i><b>8.3.3</b> When portability is important</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><i class="fa fa-check"></i><b>9</b> Using the lower-level invoke API to manipulate Spark’s Java objects from R</a><ul>
<li class="chapter" data-level="9.1" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#the-invoke-api-of-sparklyr"><i class="fa fa-check"></i><b>9.1</b> The invoke() API of sparklyr</a></li>
<li class="chapter" data-level="9.2" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#getting-started-with-the-invoke-api"><i class="fa fa-check"></i><b>9.2</b> Getting started with the invoke API</a></li>
<li class="chapter" data-level="9.3" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#grouping-and-aggregation-with-invoke-chains"><i class="fa fa-check"></i><b>9.3</b> Grouping and aggregation with invoke chains</a><ul>
<li class="chapter" data-level="9.3.1" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#what-is-all-that-extra-code"><i class="fa fa-check"></i><b>9.3.1</b> What is all that extra code?</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#wrapping-the-invocations-into-r-functions"><i class="fa fa-check"></i><b>9.4</b> Wrapping the invocations into R functions</a></li>
<li class="chapter" data-level="9.5" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#reconstructing-variable-normalization"><i class="fa fa-check"></i><b>9.5</b> Reconstructing variable normalization</a></li>
<li class="chapter" data-level="9.6" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#where-invoke-can-be-better-than-dplyr-translation-or-sql"><i class="fa fa-check"></i><b>9.6</b> Where invoke can be better than dplyr translation or SQL</a></li>
<li class="chapter" data-level="9.7" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#conclusion"><i class="fa fa-check"></i><b>9.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><i class="fa fa-check"></i><b>10</b> Exploring the invoke API from R with Java reflection and examining invokes with logs</a><ul>
<li class="chapter" data-level="10.1" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#examining-available-methods-from-r"><i class="fa fa-check"></i><b>10.1</b> Examining available methods from R</a></li>
<li class="chapter" data-level="10.2" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-the-java-reflection-api-to-list-the-available-methods"><i class="fa fa-check"></i><b>10.2</b> Using the Java reflection API to list the available methods</a></li>
<li class="chapter" data-level="10.3" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#investigating-dataset-and-sparkcontext-class-methods"><i class="fa fa-check"></i><b>10.3</b> Investigating DataSet and SparkContext class methods</a><ul>
<li class="chapter" data-level="10.3.1" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-helpers-to-explore-the-methods"><i class="fa fa-check"></i><b>10.3.1</b> Using helpers to explore the methods</a></li>
<li class="chapter" data-level="10.3.2" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#unexported-helpers-provided-by-sparklyr"><i class="fa fa-check"></i><b>10.3.2</b> Unexported helpers provided by sparklyr</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#how-sparklyr-communicates-with-spark-invoke-logging"><i class="fa fa-check"></i><b>10.4</b> How sparklyr communicates with Spark, invoke logging</a><ul>
<li class="chapter" data-level="10.4.1" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-dplyr-verbs-translated-with-dbplyr"><i class="fa fa-check"></i><b>10.4.1</b> Using dplyr verbs translated with dbplyr</a></li>
<li class="chapter" data-level="10.4.2" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-dbi-to-send-queries"><i class="fa fa-check"></i><b>10.4.2</b> Using DBI to send queries</a></li>
<li class="chapter" data-level="10.4.3" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-the-invoke-interface"><i class="fa fa-check"></i><b>10.4.3</b> Using the invoke interface</a></li>
<li class="chapter" data-level="10.4.4" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#redirecting-the-invoke-logs"><i class="fa fa-check"></i><b>10.4.4</b> Redirecting the invoke logs</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#conclusion-1"><i class="fa fa-check"></i><b>10.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="combining-approaches-into-lazy-datasets.html"><a href="combining-approaches-into-lazy-datasets.html"><i class="fa fa-check"></i><b>11</b> Combining approaches into lazy datasets</a></li>
<li class="chapter" data-level="12" data-path="references-and-resources.html"><a href="references-and-resources.html"><i class="fa fa-check"></i><b>12</b> References and resources</a><ul>
<li class="chapter" data-level="12.1" data-path="references-and-resources.html"><a href="references-and-resources.html#online-resources"><i class="fa fa-check"></i><b>12.1</b> Online resources</a><ul>
<li class="chapter" data-level="12.1.1" data-path="references-and-resources.html"><a href="references-and-resources.html#getting-started-with-sparklyr"><i class="fa fa-check"></i><b>12.1.1</b> Getting started with sparklyr</a></li>
<li class="chapter" data-level="12.1.2" data-path="references-and-resources.html"><a href="references-and-resources.html#dbi-spark-sql-hive"><i class="fa fa-check"></i><b>12.1.2</b> DBI, Spark SQL, Hive</a></li>
<li class="chapter" data-level="12.1.3" data-path="references-and-resources.html"><a href="references-and-resources.html#docker"><i class="fa fa-check"></i><b>12.1.3</b> Docker</a></li>
<li class="chapter" data-level="12.1.4" data-path="references-and-resources.html"><a href="references-and-resources.html#spark-api-java-scala-and-friends"><i class="fa fa-check"></i><b>12.1.4</b> Spark API, Java, Scala and friends</a></li>
<li class="chapter" data-level="12.1.5" data-path="references-and-resources.html"><a href="references-and-resources.html#apache-arrow"><i class="fa fa-check"></i><b>12.1.5</b> Apache Arrow</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="references-and-resources.html"><a href="references-and-resources.html#physical-books"><i class="fa fa-check"></i><b>12.2</b> Physical Books</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i><b>13</b> Appendices</a><ul>
<li class="chapter" data-level="13.1" data-path="appendices.html"><a href="appendices.html#r-session-information"><i class="fa fa-check"></i><b>13.1</b> R session information</a></li>
<li class="chapter" data-level="13.2" data-path="appendices.html"><a href="appendices.html#setup-of-apache-arrow"><i class="fa fa-check"></i><b>13.2</b> Setup of Apache Arrow</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Using Spark from R for performance with arbitrary code</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="constructing-sql-and-executing-it-with-spark" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Constructing SQL and executing it with Spark</h1>
<p>In the <a href="constructing-functions-by-piping-dplyr-verbs.html">previous chapter</a> of this series, we looked at writing R functions that can be executed directly by Spark without serialization overhead with a focus on writing functions as combinations of dplyr verbs and investigated how the SQL is generated and Spark plans created.</p>
<p>In this chapter, we will look at how to write R functions that generate SQL queries that can be executed by Spark, how to execute them using the package <span class="rpackage">DBI</span> and how to achieve lazy SQL statements that only get executed when needed. We also briefly present wrapping these approaches into functions that can be combined with other Spark operations.</p>
<div id="r-functions-as-spark-sql-generators" class="section level2">
<h2><span class="header-section-number">8.1</span> R functions as Spark SQL generators</h2>
<p>There are use cases where it is desirable to express the operations directly with SQL instead of combining dplyr verbs, for example when working within multi-language environments where re-usability is important. We can then send the SQL query directly to Spark to be executed. To create such queries, one option is to write R functions that work as query constructors.</p>
<p>Again using a very simple example, a naive implementation of column normalization could look as follows. Note that the use of <code>SELECT *</code> is discouraged and only here for illustration purposes.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" data-line-number="1">normalize_sql &lt;-<span class="st"> </span><span class="cf">function</span>(df, colName, newColName) {</a>
<a class="sourceLine" id="cb68-2" data-line-number="2">  <span class="kw">paste0</span>(</a>
<a class="sourceLine" id="cb68-3" data-line-number="3">    <span class="st">&quot;SELECT&quot;</span>,</a>
<a class="sourceLine" id="cb68-4" data-line-number="4">    <span class="st">&quot;</span><span class="ch">\n</span><span class="st">  &quot;</span>, df, <span class="st">&quot;.*&quot;</span>, <span class="st">&quot;,&quot;</span>,</a>
<a class="sourceLine" id="cb68-5" data-line-number="5">    <span class="st">&quot;</span><span class="ch">\n</span><span class="st">  (&quot;</span>, colName, <span class="st">&quot; - (SELECT avg(&quot;</span>, colName, <span class="st">&quot;) FROM &quot;</span>, df, <span class="st">&quot;))&quot;</span>,</a>
<a class="sourceLine" id="cb68-6" data-line-number="6">    <span class="st">&quot; / &quot;</span>,</a>
<a class="sourceLine" id="cb68-7" data-line-number="7">    <span class="st">&quot;(SELECT stddev_samp(&quot;</span>, colName,<span class="st">&quot;) FROM &quot;</span>, df, <span class="st">&quot;) as &quot;</span>, newColName,</a>
<a class="sourceLine" id="cb68-8" data-line-number="8">    <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="st">&quot;FROM &quot;</span>, df</a>
<a class="sourceLine" id="cb68-9" data-line-number="9">  )</a>
<a class="sourceLine" id="cb68-10" data-line-number="10">}</a></code></pre></div>
<p>Using the <code>weather</code> dataset would then yield the following SQL query when normalizing the <code>temp</code> column:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" data-line-number="1">normalize_temp_query &lt;-<span class="st"> </span><span class="kw">normalize_sql</span>(<span class="st">&quot;weather&quot;</span>, <span class="st">&quot;temp&quot;</span>, <span class="st">&quot;normTemp&quot;</span>)</a>
<a class="sourceLine" id="cb69-2" data-line-number="2"><span class="kw">cat</span>(normalize_temp_query)</a></code></pre></div>
<pre><code>## SELECT
##   weather.*,
##   (temp - (SELECT avg(temp) FROM weather)) / (SELECT stddev_samp(temp) FROM weather) as normTemp
## FROM weather</code></pre>
<p>Now that we have the query created, we can look at how to send it to Spark for execution.</p>
</div>
<div id="executing-the-generated-queries-via-spark" class="section level2">
<h2><span class="header-section-number">8.2</span> Executing the generated queries via Spark</h2>
<div id="using-dbi-as-the-interface" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Using DBI as the interface</h3>
<p>The R package <span class="rpackage">DBI</span> provides an interface for communication between R and relational database management systems. We can simply use the <code>dbGetQuery()</code> function to execute our query, for instance.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb71-1" data-line-number="1">res &lt;-<span class="st"> </span>DBI<span class="op">::</span><span class="kw">dbGetQuery</span>(sc, <span class="dt">statement =</span> normalize_temp_query)</a>
<a class="sourceLine" id="cb71-2" data-line-number="2"><span class="kw">head</span>(res)</a></code></pre></div>
<pre><code>##   id origin year month day hour  temp  dewp humid wind_dir wind_speed wind_gust
## 1  1    EWR 2013     1   1    1 39.02 26.06 59.37      270   10.35702       NaN
## 2  2    EWR 2013     1   1    2 39.02 26.96 61.63      250    8.05546       NaN
## 3  3    EWR 2013     1   1    3 39.02 28.04 64.43      240   11.50780       NaN
## 4  4    EWR 2013     1   1    4 39.92 28.04 62.21      250   12.65858       NaN
## 5  5    EWR 2013     1   1    5 39.02 28.04 64.43      260   12.65858       NaN
## 6  6    EWR 2013     1   1    6 37.94 28.04 67.21      240   11.50780       NaN
##   precip pressure visib           time_hour   normTemp
## 1      0   1012.0    10 2013-01-01 06:00:00 -0.9130047
## 2      0   1012.3    10 2013-01-01 07:00:00 -0.9130047
## 3      0   1012.5    10 2013-01-01 08:00:00 -0.9130047
## 4      0   1012.2    10 2013-01-01 09:00:00 -0.8624083
## 5      0   1011.9    10 2013-01-01 10:00:00 -0.9130047
## 6      0   1012.4    10 2013-01-01 11:00:00 -0.9737203</code></pre>
<p>As we might have noticed thanks to the way the result is printed, a standard data frame is returned, as opposed to tibbles returned by most sparklyr-based operations.</p>
<p>It is important to note that using <code>dbGetQuery()</code> <em>automatically computes and collects</em> the results to the R session. This is in contrast with the <span class="rpackage">dplyr</span> approach which constructs the query and only collects the results to the R session when <code>collect()</code> is called, or computes them when <code>compute()</code> is called.</p>
<p>We will now examine 2 options to use the prepared query lazily and without collecting the results into the R session.</p>
</div>
<div id="invoking-sql-on-a-spark-session-object" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Invoking sql on a Spark session object</h3>
<p>Without going into further details on the <code>invoke()</code> functionality of sparklyr which we will focus on in the next chapter, if the desire is to have a “lazy” SQL that does not get automatically computed and collected when called from R, we can invoke a <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession@sql(sqlText:String):org.apache.spark.sql.DataFrame"><code>sql</code> method</a> on a SparkSession class object.</p>
<p>The method takes a string SQL query as input and processes it using Spark, returning the result as a Spark DataFrame. This gives us the ability to only compute and collect the results when desired:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" data-line-number="1"><span class="co"># Use the query &quot;lazily&quot; without execution:</span></a>
<a class="sourceLine" id="cb73-2" data-line-number="2">normalized_lazy_ds &lt;-<span class="st"> </span>sc <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb73-3" data-line-number="3"><span class="st">  </span><span class="kw">spark_session</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb73-4" data-line-number="4"><span class="st">  </span><span class="kw">invoke</span>(<span class="st">&quot;sql&quot;</span>,  normalize_temp_query)</a>
<a class="sourceLine" id="cb73-5" data-line-number="5">normalized_lazy_ds</a></code></pre></div>
<pre><code>## &lt;jobj[485]&gt;
##   org.apache.spark.sql.Dataset
##   [id: int, origin: string ... 15 more fields]</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" data-line-number="1"><span class="co"># Collect when needed:</span></a>
<a class="sourceLine" id="cb75-2" data-line-number="2">normalized_lazy_ds <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">collect</span>()</a></code></pre></div>
<pre><code>## # A tibble: 26,115 x 17
##       id origin  year month   day  hour  temp  dewp humid wind_dir wind_speed
##    &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
##  1     1 EWR     2013     1     1     1  39.0  26.1  59.4      270      10.4 
##  2     2 EWR     2013     1     1     2  39.0  27.0  61.6      250       8.06
##  3     3 EWR     2013     1     1     3  39.0  28.0  64.4      240      11.5 
##  4     4 EWR     2013     1     1     4  39.9  28.0  62.2      250      12.7 
##  5     5 EWR     2013     1     1     5  39.0  28.0  64.4      260      12.7 
##  6     6 EWR     2013     1     1     6  37.9  28.0  67.2      240      11.5 
##  7     7 EWR     2013     1     1     7  39.0  28.0  64.4      240      15.0 
##  8     8 EWR     2013     1     1     8  39.9  28.0  62.2      250      10.4 
##  9     9 EWR     2013     1     1     9  39.9  28.0  62.2      260      15.0 
## 10    10 EWR     2013     1     1    10  41    28.0  59.6      260      13.8 
## # … with 26,105 more rows, and 6 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;,
## #   pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;, normTemp &lt;dbl&gt;</code></pre>
</div>
<div id="using-tbl-with-dbplyrs-sql" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Using tbl with dbplyr’s sql</h3>
<p>The above method gives us a reference to a Java object as a result, which might be less intuitive to work with for R users. We can also opt to use dbplyr’s <code>sql()</code> function in combination with <code>tbl()</code> to get a more familiar result.</p>
<p>Note that when printing the below <code>normalized_lazy_tbl</code>, the query gets partially executed to provide the first few rows. Only when <code>collect()</code> is called the entire set is retrieved to the R session.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" data-line-number="1"><span class="co"># Nothing is executed yet</span></a>
<a class="sourceLine" id="cb77-2" data-line-number="2">normalized_lazy_tbl &lt;-<span class="st"> </span>normalize_temp_query <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb77-3" data-line-number="3"><span class="st">  </span>dbplyr<span class="op">::</span><span class="kw">sql</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb77-4" data-line-number="4"><span class="st">  </span><span class="kw">tbl</span>(sc, .)</a>
<a class="sourceLine" id="cb77-5" data-line-number="5"></a>
<a class="sourceLine" id="cb77-6" data-line-number="6"><span class="co"># Print the first few rows</span></a>
<a class="sourceLine" id="cb77-7" data-line-number="7">normalized_lazy_tbl</a></code></pre></div>
<pre><code>## # Source: spark&lt;SELECT weather.*, (temp - (SELECT avg(temp) FROM weather)) /
## #   (SELECT stddev_samp(temp) FROM weather) as normTemp FROM weather&gt; [?? x 17]
##       id origin  year month   day  hour  temp  dewp humid wind_dir wind_speed
##    &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
##  1     1 EWR     2013     1     1     1  39.0  26.1  59.4      270      10.4 
##  2     2 EWR     2013     1     1     2  39.0  27.0  61.6      250       8.06
##  3     3 EWR     2013     1     1     3  39.0  28.0  64.4      240      11.5 
##  4     4 EWR     2013     1     1     4  39.9  28.0  62.2      250      12.7 
##  5     5 EWR     2013     1     1     5  39.0  28.0  64.4      260      12.7 
##  6     6 EWR     2013     1     1     6  37.9  28.0  67.2      240      11.5 
##  7     7 EWR     2013     1     1     7  39.0  28.0  64.4      240      15.0 
##  8     8 EWR     2013     1     1     8  39.9  28.0  62.2      250      10.4 
##  9     9 EWR     2013     1     1     9  39.9  28.0  62.2      260      15.0 
## 10    10 EWR     2013     1     1    10  41    28.0  59.6      260      13.8 
## # … with more rows, and 6 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;,
## #   pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;, normTemp &lt;dbl&gt;</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" data-line-number="1"><span class="co"># Collect the entire result to the R session and print</span></a>
<a class="sourceLine" id="cb79-2" data-line-number="2">normalized_lazy_tbl <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">collect</span>()</a></code></pre></div>
<pre><code>## # A tibble: 26,115 x 17
##       id origin  year month   day  hour  temp  dewp humid wind_dir wind_speed
##    &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
##  1     1 EWR     2013     1     1     1  39.0  26.1  59.4      270      10.4 
##  2     2 EWR     2013     1     1     2  39.0  27.0  61.6      250       8.06
##  3     3 EWR     2013     1     1     3  39.0  28.0  64.4      240      11.5 
##  4     4 EWR     2013     1     1     4  39.9  28.0  62.2      250      12.7 
##  5     5 EWR     2013     1     1     5  39.0  28.0  64.4      260      12.7 
##  6     6 EWR     2013     1     1     6  37.9  28.0  67.2      240      11.5 
##  7     7 EWR     2013     1     1     7  39.0  28.0  64.4      240      15.0 
##  8     8 EWR     2013     1     1     8  39.9  28.0  62.2      250      10.4 
##  9     9 EWR     2013     1     1     9  39.9  28.0  62.2      260      15.0 
## 10    10 EWR     2013     1     1    10  41    28.0  59.6      260      13.8 
## # … with 26,105 more rows, and 6 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;,
## #   pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;, normTemp &lt;dbl&gt;</code></pre>
</div>
<div id="wrapping-the-tbl-approach-into-functions" class="section level3">
<h3><span class="header-section-number">8.2.4</span> Wrapping the tbl approach into functions</h3>
<p>In the approach above we provided <code>sc</code> in the call to <code>tbl()</code>. When wrapping such processes into a function, it might however be useful to take the specific DataFrame reference as an input instead of the generic Spark connection reference.</p>
<p>In that case, we can use the fact that the connection reference is also stored in the DataFrame reference, in the <code>con</code> sub-element of the <code>src</code> element. For instance, looking at our <code>tbl_weather</code>.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" data-line-number="1"><span class="kw">class</span>(tbl_weather[[<span class="st">&quot;src&quot;</span>]][[<span class="st">&quot;con&quot;</span>]])</a></code></pre></div>
<pre><code>## [1] &quot;spark_connection&quot;       &quot;spark_shell_connection&quot; &quot;DBIConnection&quot;</code></pre>
<p>Putting this together, we can create a simple wrapper function that lazily sends a SQL query to be processed on a particular Spark DataFrame reference.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1">lazy_spark_query &lt;-<span class="st"> </span><span class="cf">function</span>(tbl, qry) {</a>
<a class="sourceLine" id="cb83-2" data-line-number="2">  qry <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb83-3" data-line-number="3"><span class="st">    </span>dbplyr<span class="op">::</span><span class="kw">sql</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb83-4" data-line-number="4"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">tbl</span>(tbl[[<span class="st">&quot;src&quot;</span>]][[<span class="st">&quot;con&quot;</span>]], .)</a>
<a class="sourceLine" id="cb83-5" data-line-number="5">}</a></code></pre></div>
<p>And use it to do the same as we did above with a single function call.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1"><span class="kw">lazy_spark_query</span>(tbl_weather, normalize_temp_query) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb84-2" data-line-number="2"><span class="st">  </span><span class="kw">collect</span>()</a></code></pre></div>
<pre><code>## # A tibble: 26,115 x 17
##       id origin  year month   day  hour  temp  dewp humid wind_dir wind_speed
##    &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
##  1     1 EWR     2013     1     1     1  39.0  26.1  59.4      270      10.4 
##  2     2 EWR     2013     1     1     2  39.0  27.0  61.6      250       8.06
##  3     3 EWR     2013     1     1     3  39.0  28.0  64.4      240      11.5 
##  4     4 EWR     2013     1     1     4  39.9  28.0  62.2      250      12.7 
##  5     5 EWR     2013     1     1     5  39.0  28.0  64.4      260      12.7 
##  6     6 EWR     2013     1     1     6  37.9  28.0  67.2      240      11.5 
##  7     7 EWR     2013     1     1     7  39.0  28.0  64.4      240      15.0 
##  8     8 EWR     2013     1     1     8  39.9  28.0  62.2      250      10.4 
##  9     9 EWR     2013     1     1     9  39.9  28.0  62.2      260      15.0 
## 10    10 EWR     2013     1     1    10  41    28.0  59.6      260      13.8 
## # … with 26,105 more rows, and 6 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;,
## #   pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;, normTemp &lt;dbl&gt;</code></pre>
</div>
</div>
<div id="where-sql-can-be-better-than-dbplyr-translation" class="section level2">
<h2><span class="header-section-number">8.3</span> Where SQL can be better than dbplyr translation</h2>
<div id="when-a-translation-is-not-there" class="section level3">
<h3><span class="header-section-number">8.3.1</span> When a translation is not there</h3>
<p>We have discussed in the <a href="https://jozef.io/r201-spark-r-1/#an-r-function-not-translated-to-spark-sql">first part</a> that the set of operations translated to Spark SQL via <span class="rpackage">dbplyr</span> may not cover all possible use cases. In such a case, the option to write SQL directly is very useful.</p>
</div>
<div id="when-translation-does-not-provide-expected-results" class="section level3">
<h3><span class="header-section-number">8.3.2</span> When translation does not provide expected results</h3>
<p>In some instances using dbplyr to translate R operations to Spark SQL can lead to unexpected results. As one example, consider the following integer division on a column of a local data frame.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1"><span class="co"># id_div_5 is as expected</span></a>
<a class="sourceLine" id="cb86-2" data-line-number="2">weather <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb86-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">id_div_5 =</span> id <span class="op">%/%</span><span class="st"> </span>5L) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb86-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(id, id_div_<span class="dv">5</span>)</a></code></pre></div>
<pre><code>## # A tibble: 26,115 x 2
##       id id_div_5
##    &lt;int&gt;    &lt;int&gt;
##  1     1        0
##  2     2        0
##  3     3        0
##  4     4        0
##  5     5        1
##  6     6        1
##  7     7        1
##  8     8        1
##  9     9        1
## 10    10        2
## # … with 26,105 more rows</code></pre>
<p>As expected, we get the result of integer division in the <code>id_div_5</code> column. However, applying the very same operation on a Spark DataFrame yields unexpected results:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1"><span class="co"># id_div_5 is normal division, not integer division</span></a>
<a class="sourceLine" id="cb88-2" data-line-number="2">tbl_weather <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb88-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">id_div_5 =</span> id <span class="op">%/%</span><span class="st"> </span>5L) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb88-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(id, id_div_<span class="dv">5</span>)</a></code></pre></div>
<pre><code>## # Source: spark&lt;?&gt; [?? x 2]
##       id id_div_5
##    &lt;int&gt;    &lt;dbl&gt;
##  1     1      0.2
##  2     2      0.4
##  3     3      0.6
##  4     4      0.8
##  5     5      1  
##  6     6      1.2
##  7     7      1.4
##  8     8      1.6
##  9     9      1.8
## 10    10      2  
## # … with more rows</code></pre>
<p>This is due to the fact that translation to integer division is quite difficult to implement: <a href="https://github.com/tidyverse/dbplyr/issues/108" class="uri">https://github.com/tidyverse/dbplyr/issues/108</a>. We could certainly figure our a way to fix this particular issue, but the workarounds may prove inefficient:</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1">tbl_weather <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb90-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">id_div_5 =</span> <span class="kw">as.integer</span>(id <span class="op">%/%</span><span class="st"> </span>5L)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb90-3" data-line-number="3"><span class="st">  </span><span class="kw">select</span>(id, id_div_<span class="dv">5</span>)</a></code></pre></div>
<pre><code>## # Source: spark&lt;?&gt; [?? x 2]
##       id id_div_5
##    &lt;int&gt;    &lt;int&gt;
##  1     1        0
##  2     2        0
##  3     3        0
##  4     4        0
##  5     5        1
##  6     6        1
##  7     7        1
##  8     8        1
##  9     9        1
## 10    10        2
## # … with more rows</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" data-line-number="1"><span class="co"># Not too efficient:</span></a>
<a class="sourceLine" id="cb92-2" data-line-number="2">tbl_weather <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb92-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">id_div_5 =</span> <span class="kw">as.integer</span>(id <span class="op">%/%</span><span class="st"> </span>5L)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb92-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(id, id_div_<span class="dv">5</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb92-5" data-line-number="5"><span class="st">  </span><span class="kw">explain</span>()</a></code></pre></div>
<pre><code>## &lt;SQL&gt;
## SELECT `id`, CAST(`id` / 5 AS INT) AS `id_div_5`
## FROM `weather`
## 
## &lt;PLAN&gt;</code></pre>
<pre><code>## == Physical Plan ==
## *(1) Project [id#24, cast((cast(id#24 as double) / 5.0) as int) AS id_div_5#5815]
## +- InMemoryTableScan [id#24]
##       +- InMemoryRelation [id#24, origin#25, year#26, month#27, day#28, hour#29, temp#30, dewp#31, humid#32, wind_dir#33, wind_speed#34, wind_gust#35, precip#36, pressure#37, visib#38, time_hour#39], StorageLevel(disk, memory, deserialized, 1 replicas)
##             +- Scan ExistingRDD[id#24,origin#25,year#26,month#27,day#28,hour#29,temp#30,dewp#31,humid#32,wind_dir#33,wind_speed#34,wind_gust#35,precip#36,pressure#37,visib#38,time_hour#39]</code></pre>
<p>Using SQL and the knowledge that Hive does provide a built-in <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-ArithmeticOperators"><code>DIV</code> arithmetic operator</a>, we can get the desired results very simply and efficiently with writing SQL:</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb95-1" data-line-number="1"><span class="st">&quot;SELECT `id`, `id` DIV 5 `id_div_5` FROM `weather`&quot;</span> <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb95-2" data-line-number="2"><span class="st">  </span>dbplyr<span class="op">::</span><span class="kw">sql</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb95-3" data-line-number="3"><span class="st">  </span><span class="kw">tbl</span>(sc, .)</a></code></pre></div>
<pre><code>## # Source: spark&lt;SELECT `id`, `id` DIV 5 `id_div_5` FROM `weather`&gt; [?? x 2]
##       id id_div_5
##    &lt;int&gt;    &lt;dbl&gt;
##  1     1        0
##  2     2        0
##  3     3        0
##  4     4        0
##  5     5        1
##  6     6        1
##  7     7        1
##  8     8        1
##  9     9        1
## 10    10        2
## # … with more rows</code></pre>
<p>Even though the numeric value of the results is correct here, we may still notice that the class of the returned <code>id_div_5</code> column is actually numeric instead of integer. Such is the life of developers using data processing interfaces.</p>
</div>
<div id="when-portability-is-important" class="section level3">
<h3><span class="header-section-number">8.3.3</span> When portability is important</h3>
<p>Since the languages that provide interfaces to Spark are not limited to R and multi-language setups are quite common, another reason to use SQL statements directly is the portability of such solutions. A SQL statement can be executed by interfaces provided for all languages - Scala, Java, and Python, without the need to rely on R-specific packages such as <span class="rpackage">dbplyr</span>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="constructing-functions-by-piping-dplyr-verbs.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
