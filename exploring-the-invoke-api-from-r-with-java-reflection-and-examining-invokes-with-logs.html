<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Exploring the invoke API from R with Java reflection and examining invokes with logs | Using Spark from R for performance with arbitrary code</title>
  <meta name="description" content="This bookdown publication attempts to provide practical insights into using the sparklyr interface to gain the benefits of Apache Spark while still retaining the ability to use R code organized in custom-built functions and packages." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Exploring the invoke API from R with Java reflection and examining invokes with logs | Using Spark from R for performance with arbitrary code" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This bookdown publication attempts to provide practical insights into using the sparklyr interface to gain the benefits of Apache Spark while still retaining the ability to use R code organized in custom-built functions and packages." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Exploring the invoke API from R with Java reflection and examining invokes with logs | Using Spark from R for performance with arbitrary code" />
  
  <meta name="twitter:description" content="This bookdown publication attempts to provide practical insights into using the sparklyr interface to gain the benefits of Apache Spark while still retaining the ability to use R code organized in custom-built functions and packages." />
  

<meta name="author" content="Jozef Hajnala" />


<meta name="date" content="2019-12-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"/>
<link rel="next" href="combining-approaches-into-lazy-datasets.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1149069-22"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1149069-22');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="static/css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Spark from R for performance</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome</a></li>
<li class="chapter" data-level="2" data-path="setting-up-spark-with-r-and-sparklyr.html"><a href="setting-up-spark-with-r-and-sparklyr.html"><i class="fa fa-check"></i><b>2</b> Setting up Spark with R and sparklyr</a><ul>
<li class="chapter" data-level="2.1" data-path="setting-up-spark-with-r-and-sparklyr.html"><a href="setting-up-spark-with-r-and-sparklyr.html#interactive-manual-installation"><i class="fa fa-check"></i><b>2.1</b> Interactive manual installation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html"><i class="fa fa-check"></i><b>3</b> Using a ready-made Docker Image</a><ul>
<li class="chapter" data-level="3.1" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#installing-docker"><i class="fa fa-check"></i><b>3.1</b> Installing Docker</a></li>
<li class="chapter" data-level="3.2" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#using-the-docker-image-with-r"><i class="fa fa-check"></i><b>3.2</b> Using the Docker image with R</a><ul>
<li class="chapter" data-level="3.2.1" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#interactively-with-rstudio"><i class="fa fa-check"></i><b>3.2.1</b> Interactively with RStudio</a></li>
<li class="chapter" data-level="3.2.2" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#interactively-with-the-r-console"><i class="fa fa-check"></i><b>3.2.2</b> Interactively with the R console</a></li>
<li class="chapter" data-level="3.2.3" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#running-an-example-r-script"><i class="fa fa-check"></i><b>3.2.3</b> Running an example R script</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="using-a-ready-made-docker-image.html"><a href="using-a-ready-made-docker-image.html#interactively-with-the-spark-shell"><i class="fa fa-check"></i><b>3.3</b> Interactively with the Spark shell</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="connecting-and-using-a-local-spark-instance.html"><a href="connecting-and-using-a-local-spark-instance.html"><i class="fa fa-check"></i><b>4</b> Connecting and using a local Spark instance</a><ul>
<li class="chapter" data-level="4.1" data-path="connecting-and-using-a-local-spark-instance.html"><a href="connecting-and-using-a-local-spark-instance.html#packages-and-data"><i class="fa fa-check"></i><b>4.1</b> Packages and data</a></li>
<li class="chapter" data-level="4.2" data-path="connecting-and-using-a-local-spark-instance.html"><a href="connecting-and-using-a-local-spark-instance.html#connecting-to-spark-and-providing-it-with-data"><i class="fa fa-check"></i><b>4.2</b> Connecting to Spark and providing it with data</a></li>
<li class="chapter" data-level="4.3" data-path="connecting-and-using-a-local-spark-instance.html"><a href="connecting-and-using-a-local-spark-instance.html#first-glance-at-the-data"><i class="fa fa-check"></i><b>4.3</b> First glance at the data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html"><i class="fa fa-check"></i><b>5</b> Communication between Spark and sparklyr</a><ul>
<li class="chapter" data-level="5.1" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#sparklyr-as-a-spark-interface-provider"><i class="fa fa-check"></i><b>5.1</b> Sparklyr as a Spark interface provider</a></li>
<li class="chapter" data-level="5.2" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#an-r-function-translated-to-spark-sql"><i class="fa fa-check"></i><b>5.2</b> An R function translated to Spark SQL</a></li>
<li class="chapter" data-level="5.3" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#an-r-function-not-translated-to-spark-sql"><i class="fa fa-check"></i><b>5.3</b> An R function not translated to Spark SQL</a></li>
<li class="chapter" data-level="5.4" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#a-hive-built-in-function-not-existing-in-r"><i class="fa fa-check"></i><b>5.4</b> A Hive built-in function not existing in R</a></li>
<li class="chapter" data-level="5.5" data-path="communication-between-spark-and-sparklyr.html"><a href="communication-between-spark-and-sparklyr.html#using-non-translated-functions-with-sparklyr"><i class="fa fa-check"></i><b>5.5</b> Using non-translated functions with sparklyr</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html"><i class="fa fa-check"></i><b>6</b> Non-translated functions with spark_apply</a><ul>
<li class="chapter" data-level="6.1" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#what-is-so-important-about-this-distinction"><i class="fa fa-check"></i><b>6.1</b> What is so important about this distinction?</a></li>
<li class="chapter" data-level="6.2" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#what-happens-when-we-use-custom-functions-with-spark_apply"><i class="fa fa-check"></i><b>6.2</b> What happens when we use custom functions with <code>spark_apply</code></a></li>
<li class="chapter" data-level="6.3" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#what-happens-when-we-use-translated-or-hive-built-in-functions"><i class="fa fa-check"></i><b>6.3</b> What happens when we use translated or Hive built-in functions</a></li>
<li class="chapter" data-level="6.4" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#which-r-functionality-is-currently-translated-and-built-in-to-hive"><i class="fa fa-check"></i><b>6.4</b> Which R functionality is currently translated and built-in to Hive</a></li>
<li class="chapter" data-level="6.5" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#making-serialization-faster-with-apache-arrow"><i class="fa fa-check"></i><b>6.5</b> Making serialization faster with Apache Arrow</a><ul>
<li class="chapter" data-level="6.5.1" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#what-is-apache-arrow-and-how-it-improves-performance"><i class="fa fa-check"></i><b>6.5.1</b> What is Apache Arrow and how it improves performance</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#conclusion-take-home-messages"><i class="fa fa-check"></i><b>6.6</b> Conclusion, take-home messages</a></li>
<li class="chapter" data-level="6.7" data-path="non-translated-functions-with-spark-apply.html"><a href="non-translated-functions-with-spark-apply.html#but-we-still-need-arbitrary-functions-to-run-fast"><i class="fa fa-check"></i><b>6.7</b> But we still need arbitrary functions to run fast</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html"><i class="fa fa-check"></i><b>7</b> Constructing functions by piping dplyr verbs</a><ul>
<li class="chapter" data-level="7.1" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#r-functions-as-combinations-of-dplyr-verbs-and-spark"><i class="fa fa-check"></i><b>7.1</b> R functions as combinations of dplyr verbs and Spark</a><ul>
<li class="chapter" data-level="7.1.1" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#trying-it-with-base-r-functions"><i class="fa fa-check"></i><b>7.1.1</b> Trying it with base R functions</a></li>
<li class="chapter" data-level="7.1.2" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#using-a-combination-of-supported-dplyr-verbs-and-operations"><i class="fa fa-check"></i><b>7.1.2</b> Using a combination of supported dplyr verbs and operations</a></li>
<li class="chapter" data-level="7.1.3" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#investigating-the-sql-translation-and-its-spark-plan"><i class="fa fa-check"></i><b>7.1.3</b> Investigating the SQL translation and its Spark plan</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#a-more-complex-use-case---joins-group-bys-and-aggregations"><i class="fa fa-check"></i><b>7.2</b> A more complex use case - Joins, group bys, and aggregations</a></li>
<li class="chapter" data-level="7.3" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#using-the-functions-with-local-versus-remote-datasets"><i class="fa fa-check"></i><b>7.3</b> Using the functions with local versus remote datasets</a><ul>
<li class="chapter" data-level="7.3.1" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#unified-front-end-different-back-ends"><i class="fa fa-check"></i><b>7.3.1</b> Unified front-end, different back-ends</a></li>
<li class="chapter" data-level="7.3.2" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#differences-in-na-and-nan-values"><i class="fa fa-check"></i><b>7.3.2</b> Differences in <code>NA</code> and <code>NaN</code> values</a></li>
<li class="chapter" data-level="7.3.3" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#dates-times-and-time-zones"><i class="fa fa-check"></i><b>7.3.3</b> Dates, times and time zones</a></li>
<li class="chapter" data-level="7.3.4" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#joins"><i class="fa fa-check"></i><b>7.3.4</b> Joins</a></li>
<li class="chapter" data-level="7.3.5" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#portability-of-used-methods"><i class="fa fa-check"></i><b>7.3.5</b> Portability of used methods</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="constructing-functions-by-piping-dplyr-verbs.html"><a href="constructing-functions-by-piping-dplyr-verbs.html#conclusion-take-home-messages-1"><i class="fa fa-check"></i><b>7.4</b> Conclusion, take-home messages</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html"><i class="fa fa-check"></i><b>8</b> Constructing SQL and executing it with Spark</a><ul>
<li class="chapter" data-level="8.1" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#r-functions-as-spark-sql-generators"><i class="fa fa-check"></i><b>8.1</b> R functions as Spark SQL generators</a></li>
<li class="chapter" data-level="8.2" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#executing-the-generated-queries-via-spark"><i class="fa fa-check"></i><b>8.2</b> Executing the generated queries via Spark</a><ul>
<li class="chapter" data-level="8.2.1" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#using-dbi-as-the-interface"><i class="fa fa-check"></i><b>8.2.1</b> Using DBI as the interface</a></li>
<li class="chapter" data-level="8.2.2" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#invoking-sql-on-a-spark-session-object"><i class="fa fa-check"></i><b>8.2.2</b> Invoking sql on a Spark session object</a></li>
<li class="chapter" data-level="8.2.3" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#using-tbl-with-dbplyrs-sql"><i class="fa fa-check"></i><b>8.2.3</b> Using tbl with dbplyr’s sql</a></li>
<li class="chapter" data-level="8.2.4" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#wrapping-the-tbl-approach-into-functions"><i class="fa fa-check"></i><b>8.2.4</b> Wrapping the tbl approach into functions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#where-sql-can-be-better-than-dbplyr-translation"><i class="fa fa-check"></i><b>8.3</b> Where SQL can be better than dbplyr translation</a><ul>
<li class="chapter" data-level="8.3.1" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#when-a-translation-is-not-there"><i class="fa fa-check"></i><b>8.3.1</b> When a translation is not there</a></li>
<li class="chapter" data-level="8.3.2" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#when-translation-does-not-provide-expected-results"><i class="fa fa-check"></i><b>8.3.2</b> When translation does not provide expected results</a></li>
<li class="chapter" data-level="8.3.3" data-path="constructing-sql-and-executing-it-with-spark.html"><a href="constructing-sql-and-executing-it-with-spark.html#when-portability-is-important"><i class="fa fa-check"></i><b>8.3.3</b> When portability is important</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><i class="fa fa-check"></i><b>9</b> Using the lower-level invoke API to manipulate Spark’s Java objects from R</a><ul>
<li class="chapter" data-level="9.1" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#the-invoke-api-of-sparklyr"><i class="fa fa-check"></i><b>9.1</b> The invoke() API of sparklyr</a></li>
<li class="chapter" data-level="9.2" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#getting-started-with-the-invoke-api"><i class="fa fa-check"></i><b>9.2</b> Getting started with the invoke API</a></li>
<li class="chapter" data-level="9.3" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#grouping-and-aggregation-with-invoke-chains"><i class="fa fa-check"></i><b>9.3</b> Grouping and aggregation with invoke chains</a><ul>
<li class="chapter" data-level="9.3.1" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#what-is-all-that-extra-code"><i class="fa fa-check"></i><b>9.3.1</b> What is all that extra code?</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#wrapping-the-invocations-into-r-functions"><i class="fa fa-check"></i><b>9.4</b> Wrapping the invocations into R functions</a></li>
<li class="chapter" data-level="9.5" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#reconstructing-variable-normalization"><i class="fa fa-check"></i><b>9.5</b> Reconstructing variable normalization</a></li>
<li class="chapter" data-level="9.6" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#where-invoke-can-be-better-than-dplyr-translation-or-sql"><i class="fa fa-check"></i><b>9.6</b> Where invoke can be better than dplyr translation or SQL</a></li>
<li class="chapter" data-level="9.7" data-path="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html"><a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html#conclusion"><i class="fa fa-check"></i><b>9.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><i class="fa fa-check"></i><b>10</b> Exploring the invoke API from R with Java reflection and examining invokes with logs</a><ul>
<li class="chapter" data-level="10.1" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#examining-available-methods-from-r"><i class="fa fa-check"></i><b>10.1</b> Examining available methods from R</a></li>
<li class="chapter" data-level="10.2" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-the-java-reflection-api-to-list-the-available-methods"><i class="fa fa-check"></i><b>10.2</b> Using the Java reflection API to list the available methods</a></li>
<li class="chapter" data-level="10.3" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#investigating-dataset-and-sparkcontext-class-methods"><i class="fa fa-check"></i><b>10.3</b> Investigating DataSet and SparkContext class methods</a><ul>
<li class="chapter" data-level="10.3.1" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-helpers-to-explore-the-methods"><i class="fa fa-check"></i><b>10.3.1</b> Using helpers to explore the methods</a></li>
<li class="chapter" data-level="10.3.2" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#unexported-helpers-provided-by-sparklyr"><i class="fa fa-check"></i><b>10.3.2</b> Unexported helpers provided by sparklyr</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#how-sparklyr-communicates-with-spark-invoke-logging"><i class="fa fa-check"></i><b>10.4</b> How sparklyr communicates with Spark, invoke logging</a><ul>
<li class="chapter" data-level="10.4.1" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-dplyr-verbs-translated-with-dbplyr"><i class="fa fa-check"></i><b>10.4.1</b> Using dplyr verbs translated with dbplyr</a></li>
<li class="chapter" data-level="10.4.2" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-dbi-to-send-queries"><i class="fa fa-check"></i><b>10.4.2</b> Using DBI to send queries</a></li>
<li class="chapter" data-level="10.4.3" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#using-the-invoke-interface"><i class="fa fa-check"></i><b>10.4.3</b> Using the invoke interface</a></li>
<li class="chapter" data-level="10.4.4" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#redirecting-the-invoke-logs"><i class="fa fa-check"></i><b>10.4.4</b> Redirecting the invoke logs</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html"><a href="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html#conclusion-1"><i class="fa fa-check"></i><b>10.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="combining-approaches-into-lazy-datasets.html"><a href="combining-approaches-into-lazy-datasets.html"><i class="fa fa-check"></i><b>11</b> Combining approaches into lazy datasets</a></li>
<li class="chapter" data-level="12" data-path="references-and-resources.html"><a href="references-and-resources.html"><i class="fa fa-check"></i><b>12</b> References and resources</a><ul>
<li class="chapter" data-level="12.1" data-path="references-and-resources.html"><a href="references-and-resources.html#online-resources"><i class="fa fa-check"></i><b>12.1</b> Online resources</a><ul>
<li class="chapter" data-level="12.1.1" data-path="references-and-resources.html"><a href="references-and-resources.html#getting-started-with-sparklyr"><i class="fa fa-check"></i><b>12.1.1</b> Getting started with sparklyr</a></li>
<li class="chapter" data-level="12.1.2" data-path="references-and-resources.html"><a href="references-and-resources.html#dbi-spark-sql-hive"><i class="fa fa-check"></i><b>12.1.2</b> DBI, Spark SQL, Hive</a></li>
<li class="chapter" data-level="12.1.3" data-path="references-and-resources.html"><a href="references-and-resources.html#docker"><i class="fa fa-check"></i><b>12.1.3</b> Docker</a></li>
<li class="chapter" data-level="12.1.4" data-path="references-and-resources.html"><a href="references-and-resources.html#spark-api-java-scala-and-friends"><i class="fa fa-check"></i><b>12.1.4</b> Spark API, Java, Scala and friends</a></li>
<li class="chapter" data-level="12.1.5" data-path="references-and-resources.html"><a href="references-and-resources.html#apache-arrow"><i class="fa fa-check"></i><b>12.1.5</b> Apache Arrow</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="references-and-resources.html"><a href="references-and-resources.html#physical-books"><i class="fa fa-check"></i><b>12.2</b> Physical Books</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="footnotes.html"><a href="footnotes.html"><i class="fa fa-check"></i><b>13</b> Footnotes</a><ul>
<li class="chapter" data-level="13.1" data-path="footnotes.html"><a href="footnotes.html#setup-of-apache-arrow"><i class="fa fa-check"></i><b>13.1</b> Setup of Apache Arrow</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Using Spark from R for performance with arbitrary code</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Exploring the invoke API from R with Java reflection and examining invokes with logs</h1>
<div class="wizardry">
<p>
Then darkness took me, and I strayed out of thought and time, and I wandered far on roads that I will not tell
</p>
<ul>
<li>
Gandalf the White
</li>
</ul>
</div>
<p>In the previous chapters, we have shown how to write functions as both <a href="https://jozef.io/r202-spark-r-dplyr-verbs/">combinations of dplyr verbs</a>, <a href="https://jozef.io/r203-spark-r-sql/">SQL query generators</a> that can be executed by Spark and <a href="https://jozef.io/r204-spark-r-invoke-scala/">how to use the lower-level API</a> to invoke methods on Java object references from R.</p>
<p>In this chapter, we will look into more details around sparklyr’s <code>invoke()</code> API, investigate available methods for different classes of objects using the Java reflection API and look under the hood of the sparklyr interface mechanism with invoke logging.</p>
<div id="examining-available-methods-from-r" class="section level2">
<h2><span class="header-section-number">10.1</span> Examining available methods from R</h2>
<p>If you did not do so, it is recommended to read the <a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html">previous chapter</a> of this book before this one to get a quick overview of the <code>invoke()</code> API.</p>
</div>
<div id="using-the-java-reflection-api-to-list-the-available-methods" class="section level2">
<h2><span class="header-section-number">10.2</span> Using the Java reflection API to list the available methods</h2>
<p>The <code>invoke()</code> interface is powerful, but also a bit hidden from the eyes as we do not immediately know what methods are available for which object classes. We can circumvent that using the <code>getMethods</code> method which (in short) returns an array of Method objects reflecting public member methods of the class.</p>
<p>For instance, retrieving a list of methods for the <code>org.apache.spark.SparkContext</code> class:</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" data-line-number="1">mthds &lt;-<span class="st"> </span>sc <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_context</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb120-2" data-line-number="2"><span class="st">  </span><span class="kw">invoke</span>(<span class="st">&quot;getClass&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb120-3" data-line-number="3"><span class="st">  </span><span class="kw">invoke</span>(<span class="st">&quot;getMethods&quot;</span>)</a>
<a class="sourceLine" id="cb120-4" data-line-number="4"><span class="kw">head</span>(mthds)</a></code></pre></div>
<pre><code>## [[1]]
## &lt;jobj[828]&gt;
##   java.lang.reflect.Method
##   public org.apache.spark.util.CallSite org.apache.spark.SparkContext.org$apache$spark$SparkContext$$creationSite()
## 
## [[2]]
## &lt;jobj[829]&gt;
##   java.lang.reflect.Method
##   public org.apache.spark.SparkConf org.apache.spark.SparkContext.org$apache$spark$SparkContext$$_conf()
## 
## [[3]]
## &lt;jobj[830]&gt;
##   java.lang.reflect.Method
##   public org.apache.spark.SparkEnv org.apache.spark.SparkContext.org$apache$spark$SparkContext$$_env()
## 
## [[4]]
## &lt;jobj[831]&gt;
##   java.lang.reflect.Method
##   public scala.Option org.apache.spark.SparkContext.org$apache$spark$SparkContext$$_progressBar()
## 
## [[5]]
## &lt;jobj[832]&gt;
##   java.lang.reflect.Method
##   public scala.Option org.apache.spark.SparkContext.org$apache$spark$SparkContext$$_ui()
## 
## [[6]]
## &lt;jobj[833]&gt;
##   java.lang.reflect.Method
##   public org.apache.spark.rpc.RpcEndpointRef org.apache.spark.SparkContext.org$apache$spark$SparkContext$$_heartbeatReceiver()</code></pre>
<p>We can see that the <code>invoke()</code> chain has returned a list of Java object references, each of them of class <code>java.lang.reflect.Method</code>. This is a good result, but the output is not very user-friendly from the R user perspective. Let us write a small wrapper that will return a some of the method’s details in a more readable fashion, for instance the return type and an overview of parameters:</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1">getMethodDetails &lt;-<span class="st"> </span><span class="cf">function</span>(mthd) {</a>
<a class="sourceLine" id="cb122-2" data-line-number="2">  returnType &lt;-<span class="st"> </span>mthd <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">invoke</span>(<span class="st">&quot;getReturnType&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">invoke</span>(<span class="st">&quot;toString&quot;</span>)</a>
<a class="sourceLine" id="cb122-3" data-line-number="3">  params &lt;-<span class="st"> </span>mthd <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">invoke</span>(<span class="st">&quot;getParameters&quot;</span>)</a>
<a class="sourceLine" id="cb122-4" data-line-number="4">  params &lt;-<span class="st"> </span><span class="kw">vapply</span>(params, invoke, <span class="st">&quot;toString&quot;</span>, <span class="dt">FUN.VALUE =</span> <span class="kw">character</span>(<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb122-5" data-line-number="5">  <span class="kw">c</span>(<span class="dt">returnType =</span> returnType, <span class="dt">params =</span> <span class="kw">paste</span>(params, <span class="dt">collapse =</span> <span class="st">&quot;, &quot;</span>))</a>
<a class="sourceLine" id="cb122-6" data-line-number="6">}</a></code></pre></div>
<p>Finally, to get a nice overview, we can make another helper function that will return a named list of methods for an object’s class, including their return types and overview of parameters:</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb123-1" data-line-number="1">getAvailableMethods &lt;-<span class="st"> </span><span class="cf">function</span>(jobj) {</a>
<a class="sourceLine" id="cb123-2" data-line-number="2">  mthds &lt;-<span class="st"> </span>jobj <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">invoke</span>(<span class="st">&quot;getClass&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">invoke</span>(<span class="st">&quot;getMethods&quot;</span>)</a>
<a class="sourceLine" id="cb123-3" data-line-number="3">  nms &lt;-<span class="st"> </span><span class="kw">vapply</span>(mthds, invoke, <span class="st">&quot;getName&quot;</span>, <span class="dt">FUN.VALUE =</span> <span class="kw">character</span>(<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb123-4" data-line-number="4">  res &lt;-<span class="st"> </span><span class="kw">lapply</span>(mthds, getMethodDetails)</a>
<a class="sourceLine" id="cb123-5" data-line-number="5">  <span class="kw">names</span>(res) &lt;-<span class="st"> </span>nms</a>
<a class="sourceLine" id="cb123-6" data-line-number="6">  res</a>
<a class="sourceLine" id="cb123-7" data-line-number="7">}</a></code></pre></div>
</div>
<div id="investigating-dataset-and-sparkcontext-class-methods" class="section level2">
<h2><span class="header-section-number">10.3</span> Investigating DataSet and SparkContext class methods</h2>
<p>Using the above defined function we can explore the methods available to a DataFrame reference, showing a few of the names first:</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" data-line-number="1">dfMethods &lt;-<span class="st"> </span>tbl_flights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_dataframe</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb124-2" data-line-number="2"><span class="st">  </span><span class="kw">getAvailableMethods</span>()</a>
<a class="sourceLine" id="cb124-3" data-line-number="3"></a>
<a class="sourceLine" id="cb124-4" data-line-number="4"><span class="co"># Show some method names:</span></a>
<a class="sourceLine" id="cb124-5" data-line-number="5">dfMethodNames &lt;-<span class="st"> </span><span class="kw">sort</span>(<span class="kw">unique</span>(<span class="kw">names</span>(dfMethods)))</a>
<a class="sourceLine" id="cb124-6" data-line-number="6"><span class="kw">head</span>(dfMethodNames, <span class="dv">20</span>)</a></code></pre></div>
<pre><code>##  [1] &quot;agg&quot;                           &quot;alias&quot;                        
##  [3] &quot;apply&quot;                         &quot;as&quot;                           
##  [5] &quot;cache&quot;                         &quot;checkpoint&quot;                   
##  [7] &quot;coalesce&quot;                      &quot;col&quot;                          
##  [9] &quot;collect&quot;                       &quot;collectAsArrowToPython&quot;       
## [11] &quot;collectAsList&quot;                 &quot;collectToPython&quot;              
## [13] &quot;colRegex&quot;                      &quot;columns&quot;                      
## [15] &quot;count&quot;                         &quot;createGlobalTempView&quot;         
## [17] &quot;createOrReplaceGlobalTempView&quot; &quot;createOrReplaceTempView&quot;      
## [19] &quot;createTempView&quot;                &quot;crossJoin&quot;</code></pre>
<p>If we would like to see more details we can now investigate further, for instance show different parameter interfaces for the <code>agg</code> method, showing that the <code>agg</code> method has the following parameter interfaces:</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb126-1" data-line-number="1"><span class="kw">sort</span>(<span class="kw">vapply</span>(</a>
<a class="sourceLine" id="cb126-2" data-line-number="2">  dfMethods[<span class="kw">names</span>(dfMethods) <span class="op">==</span><span class="st"> &quot;agg&quot;</span>], </a>
<a class="sourceLine" id="cb126-3" data-line-number="3">  <span class="st">`</span><span class="dt">[[</span><span class="st">`</span>, <span class="st">&quot;params&quot;</span>,</a>
<a class="sourceLine" id="cb126-4" data-line-number="4">  <span class="dt">FUN.VALUE =</span> <span class="kw">character</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb126-5" data-line-number="5">))</a></code></pre></div>
<pre><code>##                                                                                                                                  agg 
##                                                                             &quot;java.util.Map&lt;java.lang.String, java.lang.String&gt; arg0&quot; 
##                                                                                                                                  agg 
##                                                              &quot;org.apache.spark.sql.Column arg0, org.apache.spark.sql.Column... arg1&quot; 
##                                                                                                                                  agg 
##                                           &quot;org.apache.spark.sql.Column arg0, scala.collection.Seq&lt;org.apache.spark.sql.Column&gt; arg1&quot; 
##                                                                                                                                  agg 
##                                                            &quot;scala.collection.immutable.Map&lt;java.lang.String, java.lang.String&gt; arg0&quot; 
##                                                                                                                                  agg 
## &quot;scala.Tuple2&lt;java.lang.String, java.lang.String&gt; arg0, scala.collection.Seq&lt;scala.Tuple2&lt;java.lang.String, java.lang.String&gt;&gt; arg1&quot;</code></pre>
<p>Similarly, we can look at a <code>SparkContext</code> class and show some available methods that can be invoked:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" data-line-number="1">scMethods &lt;-<span class="st"> </span>sc <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_context</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb128-2" data-line-number="2"><span class="st">  </span><span class="kw">getAvailableMethods</span>()</a>
<a class="sourceLine" id="cb128-3" data-line-number="3">scMethodNames &lt;-<span class="st"> </span><span class="kw">sort</span>(<span class="kw">unique</span>(<span class="kw">names</span>(scMethods)))</a>
<a class="sourceLine" id="cb128-4" data-line-number="4"><span class="kw">head</span>(scMethodNames, <span class="dv">60</span>)</a></code></pre></div>
<pre><code>##  [1] &quot;$lessinit$greater$default$3&quot; &quot;$lessinit$greater$default$4&quot;
##  [3] &quot;$lessinit$greater$default$5&quot; &quot;accumulable&quot;                
##  [5] &quot;accumulableCollection&quot;       &quot;accumulator&quot;                
##  [7] &quot;addedFiles&quot;                  &quot;addedJars&quot;                  
##  [9] &quot;addFile&quot;                     &quot;addJar&quot;                     
## [11] &quot;addSparkListener&quot;            &quot;applicationAttemptId&quot;       
## [13] &quot;applicationId&quot;               &quot;appName&quot;                    
## [15] &quot;assertNotStopped&quot;            &quot;binaryFiles&quot;                
## [17] &quot;binaryFiles$default$2&quot;       &quot;binaryRecords&quot;              
## [19] &quot;binaryRecords$default$3&quot;     &quot;broadcast&quot;                  
## [21] &quot;cancelAllJobs&quot;               &quot;cancelJob&quot;                  
## [23] &quot;cancelJobGroup&quot;              &quot;cancelStage&quot;                
## [25] &quot;checkpointDir&quot;               &quot;checkpointDir_$eq&quot;          
## [27] &quot;checkpointFile&quot;              &quot;clean&quot;                      
## [29] &quot;clean$default$2&quot;             &quot;cleaner&quot;                    
## [31] &quot;clearCallSite&quot;               &quot;clearJobGroup&quot;              
## [33] &quot;collectionAccumulator&quot;       &quot;conf&quot;                       
## [35] &quot;createSparkEnv&quot;              &quot;dagScheduler&quot;               
## [37] &quot;dagScheduler_$eq&quot;            &quot;defaultMinPartitions&quot;       
## [39] &quot;defaultParallelism&quot;          &quot;deployMode&quot;                 
## [41] &quot;doubleAccumulator&quot;           &quot;emptyRDD&quot;                   
## [43] &quot;env&quot;                         &quot;equals&quot;                     
## [45] &quot;eventLogCodec&quot;               &quot;eventLogDir&quot;                
## [47] &quot;eventLogger&quot;                 &quot;executorAllocationManager&quot;  
## [49] &quot;executorEnvs&quot;                &quot;executorMemory&quot;             
## [51] &quot;files&quot;                       &quot;getAllPools&quot;                
## [53] &quot;getCallSite&quot;                 &quot;getCheckpointDir&quot;           
## [55] &quot;getClass&quot;                    &quot;getConf&quot;                    
## [57] &quot;getExecutorIds&quot;              &quot;getExecutorMemoryStatus&quot;    
## [59] &quot;getExecutorThreadDump&quot;       &quot;getLocalProperties&quot;</code></pre>
<div id="using-helpers-to-explore-the-methods" class="section level3">
<h3><span class="header-section-number">10.3.1</span> Using helpers to explore the methods</h3>
<p>We can also use the helper functions to investigate more. For instance, we see that there is a <code>getConf</code> method avaiable to us. Looking at the object reference however does not provide useful information, so we can list the methods for that class and look for <code>&quot;get&quot;</code> methods that would show us the configuration:</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb130-1" data-line-number="1">spark_conf &lt;-<span class="st"> </span>sc <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_context</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">invoke</span>(<span class="st">&quot;conf&quot;</span>)</a>
<a class="sourceLine" id="cb130-2" data-line-number="2">spark_conf_methods &lt;-<span class="st"> </span>spark_conf <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">getAvailableMethods</span>() </a>
<a class="sourceLine" id="cb130-3" data-line-number="3">spark_conf_get_methods &lt;-<span class="st"> </span>spark_conf_methods <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb130-4" data-line-number="4"><span class="st">  </span><span class="kw">names</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb130-5" data-line-number="5"><span class="st">  </span><span class="kw">grep</span>(<span class="dt">pattern =</span> <span class="st">&quot;get&quot;</span>, ., <span class="dt">value =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb130-6" data-line-number="6"><span class="st">  </span><span class="kw">sort</span>()</a>
<a class="sourceLine" id="cb130-7" data-line-number="7">spark_conf_get_methods</a></code></pre></div>
<pre><code>##  [1] &quot;get&quot;                 &quot;get&quot;                 &quot;get&quot;                
##  [4] &quot;getAll&quot;              &quot;getAllWithPrefix&quot;    &quot;getAppId&quot;           
##  [7] &quot;getAvroSchema&quot;       &quot;getBoolean&quot;          &quot;getClass&quot;           
## [10] &quot;getDeprecatedConfig&quot; &quot;getDouble&quot;           &quot;getenv&quot;             
## [13] &quot;getExecutorEnv&quot;      &quot;getInt&quot;              &quot;getLong&quot;            
## [16] &quot;getOption&quot;           &quot;getSizeAsBytes&quot;      &quot;getSizeAsBytes&quot;     
## [19] &quot;getSizeAsBytes&quot;      &quot;getSizeAsGb&quot;         &quot;getSizeAsGb&quot;        
## [22] &quot;getSizeAsKb&quot;         &quot;getSizeAsKb&quot;         &quot;getSizeAsMb&quot;        
## [25] &quot;getSizeAsMb&quot;         &quot;getTimeAsMs&quot;         &quot;getTimeAsMs&quot;        
## [28] &quot;getTimeAsSeconds&quot;    &quot;getTimeAsSeconds&quot;    &quot;getWithSubstitution&quot;</code></pre>
<p>We see that there is a <code>getAll</code> method that could prove useful, returning a list of tuples and taking no arguments as input:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb132-1" data-line-number="1"><span class="co"># Returns a list of tuples, takes no arguments:</span></a>
<a class="sourceLine" id="cb132-2" data-line-number="2">spark_conf_methods[[<span class="st">&quot;getAll&quot;</span>]]</a></code></pre></div>
<pre><code>##              returnType                  params 
## &quot;class [Lscala.Tuple2;&quot;                      &quot;&quot;</code></pre>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb134-1" data-line-number="1"><span class="co"># Invoke the `getAll` method and look at part of the result</span></a>
<a class="sourceLine" id="cb134-2" data-line-number="2">spark_confs &lt;-<span class="st"> </span>spark_conf <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">invoke</span>(<span class="st">&quot;getAll&quot;</span>)</a>
<a class="sourceLine" id="cb134-3" data-line-number="3">spark_confs &lt;-<span class="st"> </span><span class="kw">vapply</span>(spark_confs, invoke, <span class="st">&quot;toString&quot;</span>, <span class="dt">FUN.VALUE =</span> <span class="kw">character</span>(<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb134-4" data-line-number="4"><span class="kw">sort</span>(spark_confs)[<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">12</span>)]</a></code></pre></div>
<pre><code>## [1] &quot;(spark.app.name,sparklyr)&quot;        &quot;(spark.driver.host,localhost)&quot;   
## [3] &quot;(spark.sql.shuffle.partitions,4)&quot;</code></pre>
<p>Looking at <a href="https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/SparkConf.html#getAll()">the Scala documentation for the <code>getAll</code> method</a>, we actually see that there is information missing on our data - the classes of the objects in the tuple, which in this case is <code>scala.Tuple2&lt;java.lang.String,java.lang.String&gt;[]</code>.</p>
<p>We could therefore improve our helper to be more detailed in the return value information.</p>
</div>
<div id="unexported-helpers-provided-by-sparklyr" class="section level3">
<h3><span class="header-section-number">10.3.2</span> Unexported helpers provided by sparklyr</h3>
<p>The sparklyr package itself provides facilities of nature similar to those above, looking at some of them, even though they are not exported:</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb136-1" data-line-number="1">sparklyr<span class="op">:::</span><span class="kw">jobj_class</span>(spark_conf)</a></code></pre></div>
<pre><code>## [1] &quot;SparkConf&quot; &quot;Object&quot;</code></pre>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb138-1" data-line-number="1">sparklyr<span class="op">:::</span><span class="kw">jobj_info</span>(spark_conf)<span class="op">$</span>class</a></code></pre></div>
<pre><code>## [1] &quot;org.apache.spark.SparkConf&quot;</code></pre>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb140-1" data-line-number="1"><span class="kw">capture.output</span>(sparklyr<span class="op">:::</span><span class="kw">jobj_inspect</span>(spark_conf)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>(<span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] &quot;&lt;jobj[1645]&gt;&quot;                                                                                                                   
##  [2] &quot;  org.apache.spark.SparkConf&quot;                                                                                                   
##  [3] &quot;  org.apache.spark.SparkConf@7ec389e7&quot;                                                                                          
##  [4] &quot;Fields:&quot;                                                                                                                        
##  [5] &quot;&lt;jobj[2490]&gt;&quot;                                                                                                                   
##  [6] &quot;  java.lang.reflect.Field&quot;                                                                                                      
##  [7] &quot;  private final java.util.concurrent.ConcurrentHashMap org.apache.spark.SparkConf.org$apache$spark$SparkConf$$settings&quot;         
##  [8] &quot;&lt;jobj[2491]&gt;&quot;                                                                                                                   
##  [9] &quot;  java.lang.reflect.Field&quot;                                                                                                      
## [10] &quot;  private transient org.apache.spark.internal.config.ConfigReader org.apache.spark.SparkConf.org$apache$spark$SparkConf$$reader&quot;</code></pre>
</div>
</div>
<div id="how-sparklyr-communicates-with-spark-invoke-logging" class="section level2">
<h2><span class="header-section-number">10.4</span> How sparklyr communicates with Spark, invoke logging</h2>
<p>Now that we have and overview of the <code>invoke()</code> interface, we can take a look under the hood of sparklyr and see how it actually communicates with the Spark instance. In fact, the communication is a set of invocations that can be very different depending on which of the approches we choose for our purposes.</p>
<p>To obtain the information, we use the <code>sparklyr.log.invoke</code> property. We can choose one of the following 3 values based on our preferences:</p>
<ul>
<li><code>TRUE</code> will use <code>message()</code> to communicate short info on what is being invoked</li>
<li><code>&quot;cat&quot;</code> will use <code>cat()</code> to communicate short info on what is being invoked</li>
<li><code>&quot;callstack&quot;</code> will use <code>message()</code> to communicate short info on what is being invoked and the callstack</li>
</ul>
<p>We will use <code>&quot;cat&quot;</code> below
to keep the output short and easily manageable. First, we will close the previous connection and create a new one with the configuration containing the <code>sparklyr.log.invoke</code> set to <code>&quot;cat&quot;</code>, and copy in the flights dataset:</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb142-1" data-line-number="1">sparklyr<span class="op">::</span><span class="kw">spark_disconnect</span>(sc)</a>
<a class="sourceLine" id="cb142-2" data-line-number="2">config &lt;-<span class="st"> </span>sparklyr<span class="op">::</span><span class="kw">spark_config</span>()</a>
<a class="sourceLine" id="cb142-3" data-line-number="3">config<span class="op">$</span>sparklyr.log.invoke &lt;-<span class="st"> &quot;cat&quot;</span></a>
<a class="sourceLine" id="cb142-4" data-line-number="4"><span class="kw">suppressMessages</span>({</a>
<a class="sourceLine" id="cb142-5" data-line-number="5">  sc &lt;-<span class="st"> </span>sparklyr<span class="op">::</span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> config)</a>
<a class="sourceLine" id="cb142-6" data-line-number="6">  tbl_flights &lt;-<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">copy_to</span>(sc, nycflights13<span class="op">::</span>flights, <span class="st">&quot;flights&quot;</span>)</a>
<a class="sourceLine" id="cb142-7" data-line-number="7">})</a></code></pre></div>
<pre><code>## Invoking sparklyr.Shell getBackend
## Invoking getSparkContext
## Invoking org.apache.spark.SparkConf
## Invoking setAppName
## Invoking setMaster
## Invoking setSparkHome
## Invoking set
## Invoking set
## Invoking org.apache.spark.sql.SparkSession builder
## Invoking config
## Invoking config
## Invoking getOrCreate
## Invoking sparkContext
## Invoking setSparkContext
## Invoking org.apache.spark.api.java.JavaSparkContext fromSparkContext
## Invoking version
## Invoking uiWebUrl
## Invoking isEmpty
## Invoking uiWebUrl
## Invoking get
## Invoking sql
## Invoking schema
## Invoking fields
## Invoking dataType
## Invoking toString
## Invoking name
## Invoking dataType
## Invoking toString
## Invoking name
## Invoking dataType
## Invoking toString
## Invoking name
## Invoking sparklyr.Utils collectColumn
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructField
## Invoking sparklyr.SQLUtils createStructType
## Invoking sparklyr.Utils createDataFrameFromCsv
## Invoking createDataFrame
## Invoking createOrReplaceTempView
## Invoking sql
## Invoking sql
## Invoking isStreaming
## Invoking sparklyr.Utils collect
## Invoking columns
## Invoking schema
## Invoking fields
## Invoking dataType
## Invoking toString
## Invoking name
## Invoking sql
## Invoking schema
## Invoking fieldNames</code></pre>
<div id="using-dplyr-verbs-translated-with-dbplyr" class="section level3">
<h3><span class="header-section-number">10.4.1</span> Using dplyr verbs translated with dbplyr</h3>
<p>Now that the setup is complete, we use the dplyr verb approach to retrieve the count of rows and look the invocations that this entails:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb144-1" data-line-number="1">tbl_flights <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">count</span>()</a></code></pre></div>
<pre><code>## Invoking sql
## Invoking sql
## Invoking columns
## Invoking isStreaming
## Invoking sql
## Invoking isStreaming
## Invoking sql
## Invoking sparklyr.Utils collect
## Invoking columns
## Invoking schema
## Invoking fields
## Invoking dataType
## Invoking toString
## Invoking name
## Invoking sql
## Invoking columns
## # Source: spark&lt;?&gt; [?? x 1]
##        n
##    &lt;dbl&gt;
## 1 336776</code></pre>
<p>We see multiple invocations do the <code>sql</code> method and also the <code>columns</code> method. This makes sense since the dplyr verb approach actually works by translating the commands into Spark SQL via dbplyr and then sends those translated commands to Spark via that interface.</p>
</div>
<div id="using-dbi-to-send-queries" class="section level3">
<h3><span class="header-section-number">10.4.2</span> Using DBI to send queries</h3>
<p>Similarly, we can investigate the invocations that happen when we try to retrieve the same results via the DBI interface:</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb146-1" data-line-number="1">DBI<span class="op">::</span><span class="kw">dbGetQuery</span>(sc, <span class="st">&quot;SELECT count(1) AS n FROM flights&quot;</span>)</a></code></pre></div>
<pre><code>## Invoking sql
## Invoking isStreaming
## Invoking sparklyr.Utils collect
## Invoking columns
## Invoking schema
## Invoking fields
## Invoking dataType
## Invoking toString
## Invoking name</code></pre>
<pre><code>##        n
## 1 336776</code></pre>
<p>We see slightly fewer invocations compared to the above dplyr approach, but the output is also less processed.</p>
</div>
<div id="using-the-invoke-interface" class="section level3">
<h3><span class="header-section-number">10.4.3</span> Using the invoke interface</h3>
<p>Looking at the invocations that get executed using the <code>invoke()</code> interface:</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb149-1" data-line-number="1">tbl_flights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_dataframe</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">invoke</span>(<span class="st">&quot;count&quot;</span>)</a></code></pre></div>
<pre><code>## Invoking sql
## Invoking count</code></pre>
<pre><code>## [1] 336776</code></pre>
<p>We see that the amount of invocations is much lower, where the top 3 invocations come from the first part of the pipe. The <code>invoke(&quot;count&quot;)</code> part translated to exactly one invocation to the <code>count</code> method. We see therefore that the <code>invoke()</code> interface is indeed a more lower-level interface that invokes methods as we request them, with little to none overhead related to translations and other effects.</p>
</div>
<div id="redirecting-the-invoke-logs" class="section level3">
<h3><span class="header-section-number">10.4.4</span> Redirecting the invoke logs</h3>
<p>When running R applications that use Spark as a calculation engine, it is useful to get detailed invoke logs for debugging and diagnostic purposes. Implementing such mechanisms, we need to take into consideration how R handles the invoke logs produced by sparklyr. In simple terms, the invoke logs produced when using</p>
<ul>
<li><code>TRUE</code> and <code>&quot;callstack&quot;</code> are created using <code>message()</code>, which means they get sent to the <code>stderr()</code> connection by default</li>
<li><code>&quot;cat&quot;</code> are created using <code>cat()</code>, so they get sent to <code>stdout()</code> connection by default</li>
</ul>
<p>This info can prove useful when redirecting the log information from standard output and standard error to different logging targets.</p>
</div>
</div>
<div id="conclusion-1" class="section level2">
<h2><span class="header-section-number">10.5</span> Conclusion</h2>
<p>In this chapter, we have looked at using the Java reflection API with sparklyr’s <code>invoke()</code> interface to get useful insight on available methods for different object types that can be used in the context of Spark, but also in other contexts. Using invoke logging, we have also shown how the different sparklyr interfacing methods communicate with Spark under the hood.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="combining-approaches-into-lazy-datasets.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
