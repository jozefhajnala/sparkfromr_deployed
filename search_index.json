[
["index.html", "Using Spark from R for performance with arbitrary code Chapter 1 Welcome", " Using Spark from R for performance with arbitrary code Jozef Hajnala 2019-12-31 Chapter 1 Welcome Apache Spark is a popular open-source analytics engine for big data processing and thanks to the sparklyr and SparkR packages, the power of Spark is also available to R users. This short bookdown publication attempts to provide practical insights into using the sparklyr interface to gain the benefits of Apache Spark while still retaining the ability to use R code organized in custom-built functions and packages. The focus of this publication is on exploring the different interfaces avaialable for communication between R and Spark using the sparklyr package, namely: Constructing functions by piping dplyr verbs Constructing SQL and executing it with Spark Using the lower-level invoke API to manipulate Spark’s Java objects from R Exploring the invoke API from R with Java reflection and examining invokes with logs If you are interested in the sparklyr package and working with Spark from R in general, we strongly recommend the very comprehensive Mastering Spark with R book available online for free. Differences of habit and language are nothing at all if our aims are identical and our hearts are open Albus Dumbledore "],
["setting-up-spark-with-r-and-sparklyr.html", "Chapter 2 Setting up Spark with R and sparklyr 2.1 Interactive manual installation", " Chapter 2 Setting up Spark with R and sparklyr An exhaustive list of instructions on setting up sparklyr is not in the scope of this book as it is extensively covered elsewhere, below we provide a quick set of instructions to get a local Spark instance working with sparklyr in an interactive setting. We have however prepared a dedicated Docker image that has all the prerequisities readily available to use. We recommend using this pre-built image for the best experience using this book. 2.1 Interactive manual installation In case the Docker approach is not suitable for you, the following are very basic instructions to install the sparklyr package with its dependencies, the nycflights13 package for example data and Spark version 2.4.3. install.packages(&quot;sparklyr&quot;) install.packages(&quot;nycflights13&quot;) sparklyr::spark_install(version = &quot;2.4.3&quot;) For troubleshooting and more detailed step-by-step guides please refer to: The Getting Started chapter of the Mastering Spark with R book The Prerequisites appendix f the Mastering Spark with R book RStudio’s spark website. "],
["using-a-ready-made-docker-image.html", "Chapter 3 Using a ready-made Docker Image 3.1 Installing Docker 3.2 Using the Docker image with R 3.3 Interactively with the Spark shell", " Chapter 3 Using a ready-made Docker Image For the purpose of this book, a Docker image was built which you can use to run all the code chunks present in it withouth issues. 3.1 Installing Docker The installation instructions for Docker are very accessible and should get you going fairly quickly. We provide links to them, please choose based on your platform: Windows 10 64-bit: Pro, Enterprise, or Education Windows 10 home or Windows 7 Ubuntu Centos Debian Fedora Mac 3.2 Using the Docker image with R The Docker image we prepared contains all the prerequisites needed to run all the code chunks present in this book. In fact, this very image is used to render the book itself. If you are interested in the details around the image, please feel free to visit the GitHub repository from where it is openly accessible. Now we look at how we use the image in practice in a few ways. 3.2.1 Interactively with RStudio Running the following line of code in a terminal should create a container and expose RStudio for use. If you are using RStudio 1.1 or newer, the Terminal functionality is built into RStudio itself. # Your can replace pass with a password of your choice docker run -d -p 8787:8787 -e PASSWORD=pass --name rstudio jozefhajnala/sparkfromr:latest After running the above line, open your favourite web browser such as Google Chrome or Firefox and navigate to http://localhost:8787. You should be greeted by the RStudio login screen where you can use the following to login: Username: rstudio Password: pass (or the one you chose above) Now you can freely start to use the code content of the book, starting by connecting to a local Spark instance: library(sparklyr) sc &lt;- spark_connect(&quot;local&quot;) 3.2.2 Interactively with the R console Running the following should yield an interactive R session with all prerequisites to start working with the sparklyr package using a local Spark instance. docker run --rm -it jozefhajnala/sparkfromr:latest R Now you can freely start to use the code content of the book from the R consule, starting by connecting to a local Spark instance: # Start using sparklyr library(sparklyr) sc &lt;- spark_connect(&quot;local&quot;) 3.2.3 Running an example R script Running the following should execute an example R script using sparklyr with output appearing in the terminal: docker run --rm jozefhajnala/sparkfromr:latest Rscript /root/.local/spark_script.R 3.3 Interactively with the Spark shell Running the following should yield an interactive Scala REPL instance. A Spark context should be available as sc and a Spark session as spark. docker run --rm -it jozefhajnala/sparkfromr:latest /root/spark/spark-2.4.3-bin-hadoop2.7/bin/spark-shell "],
["connecting-and-using-a-local-spark-instance.html", "Chapter 4 Connecting and using a local Spark instance 4.1 Packages and data 4.2 Connecting to Spark and providing it with data 4.3 First glance at the data", " Chapter 4 Connecting and using a local Spark instance The following code chunks will prepare the R session for us to be able to experiment with the code presented in the book. We will attach the needed R packages, initialize and connect to a local Spark instance and copy the weather and flights datasets from the nycflights13 package to the Spark instance such that we can work with them in our examples. 4.1 Packages and data We will be using the sparklyr package to interface with Spark and since this packages works very well with the dplyr package and its vocabulary, we will take advantage of the dplyr syntax and the pipe operator. As a source of data, we will use the nycflights13 package that conveniently provides airline data for flights departing New York City in 2013. # Attach packages suppressPackageStartupMessages({ library(sparklyr) library(dplyr) library(nycflights13) }) # Add an id column to the datasets weather &lt;- nycflights13::weather %&gt;% mutate(id = 1L:nrow(nycflights13::weather)) %&gt;% select(id, everything()) flights &lt;- nycflights13::flights %&gt;% mutate(id = 1L:nrow(nycflights13::flights)) %&gt;% select(id, everything()) 4.2 Connecting to Spark and providing it with data As a second step, we will now connect to a Spark instance which will be running on our local machine and send the prepared data to the instance using the copy_to() function such that we can work with them in Spark. We assign the outputs of copy_to() to objects called tbl_weather and tbl_flights, which are references to the DataFrame objects within Spark. # Connect to a local Spark instance sc &lt;- sparklyr::spark_connect(master = &quot;local&quot;) # Copy the weather dataset to the instance tbl_weather &lt;- dplyr::copy_to( dest = sc, df = weather, name = &quot;weather&quot;, overwrite = TRUE ) # Copy the flights dataset to the instance tbl_flights &lt;- dplyr::copy_to( dest = sc, df = flights, name = &quot;flights&quot;, overwrite = TRUE ) 4.3 First glance at the data To make sure our datasets are available in the Spark instance, we can look at the first few rows of the two datasets we have copied to Spark. Notice how the print shows Source: spark&lt;?&gt; [?? x, telling us that the data inded comes from a Spark instance and the data frames have an unknown amount of rows. This is because Spark will do minimal work to show us just the first 6 rows, instead of going though the entire dataset. head(tbl_flights) ## # Source: spark&lt;?&gt; [?? x 20] ## id year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 2013 1 1 517 515 2 830 ## 2 2 2013 1 1 533 529 4 850 ## 3 3 2013 1 1 542 540 2 923 ## 4 4 2013 1 1 544 545 -1 1004 ## 5 5 2013 1 1 554 600 -6 812 ## 6 6 2013 1 1 554 558 -4 740 ## # … with 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; head(tbl_weather) ## # Source: spark&lt;?&gt; [?? x 16] ## id origin year month day hour temp dewp humid wind_dir wind_speed ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 EWR 2013 1 1 1 39.0 26.1 59.4 270 10.4 ## 2 2 EWR 2013 1 1 2 39.0 27.0 61.6 250 8.06 ## 3 3 EWR 2013 1 1 3 39.0 28.0 64.4 240 11.5 ## 4 4 EWR 2013 1 1 4 39.9 28.0 62.2 250 12.7 ## 5 5 EWR 2013 1 1 5 39.0 28.0 64.4 260 12.7 ## 6 6 EWR 2013 1 1 6 37.9 28.0 67.2 240 11.5 ## # … with 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, pressure &lt;dbl&gt;, ## # visib &lt;dbl&gt;, time_hour &lt;dttm&gt; "],
["communication-between-spark-and-sparklyr.html", "Chapter 5 Communication between Spark and sparklyr 5.1 Sparklyr as a Spark interface provider 5.2 An R function translated to Spark SQL 5.3 An R function not translated to Spark SQL 5.4 A Hive built-in function not existing in R 5.5 Using non-translated functions with sparklyr", " Chapter 5 Communication between Spark and sparklyr In this chapter, we will examine how the sparklyr interface communicates with the Spark instance and what this means for performance with regards to arbitrarily defined R functions. We will also look at how Apache Arrow can improve the performance of object serialization. 5.1 Sparklyr as a Spark interface provider The sparklyr package is an R interface to Apache Spark. The meaning of the word interface is very important in this context as the way we use this interface can significantly affect the performance benefits we get from using Spark. To understand the meaning of the above a bit better, we will examine 3 very simple functions that are different in implementation but intend to provide the same results, and how they behave with regards to Spark. We will keep using the datasets from the nycflights13 package for our examples. 5.2 An R function translated to Spark SQL Using the following fun_implemented() function will yield the expected results for both a local data frame nycflights13::weather and the remote Spark object referenced by tbl_weather: # An R function `tolower` translated to Spark SQL fun_implemented &lt;- function(df, col) { df %&gt;% mutate({{col}} := tolower({{col}})) } First, let us run fun_implemented for a local data frame in our R session. Note that the output of the command is A tibble: 26,115 x 15, meaning this is an object in our local R session. fun_implemented(nycflights13::weather, origin) ## # A tibble: 26,115 x 15 ## origin year month day hour temp dewp humid wind_dir wind_speed ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ewr 2013 1 1 1 39.0 26.1 59.4 270 10.4 ## 2 ewr 2013 1 1 2 39.0 27.0 61.6 250 8.06 ## 3 ewr 2013 1 1 3 39.0 28.0 64.4 240 11.5 ## 4 ewr 2013 1 1 4 39.9 28.0 62.2 250 12.7 ## 5 ewr 2013 1 1 5 39.0 28.0 64.4 260 12.7 ## 6 ewr 2013 1 1 6 37.9 28.0 67.2 240 11.5 ## 7 ewr 2013 1 1 7 39.0 28.0 64.4 240 15.0 ## 8 ewr 2013 1 1 8 39.9 28.0 62.2 250 10.4 ## 9 ewr 2013 1 1 9 39.9 28.0 62.2 260 15.0 ## 10 ewr 2013 1 1 10 41 28.0 59.6 260 13.8 ## # … with 26,105 more rows, and 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, ## # pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt; Next, we use it against a remote Spark DataFrame. Notice that here the output is a remote object with Source: spark&lt;?&gt; [?? x 16] and once again, Spark only executed the minimal work to show this printout, so we do not yet know, how many lines in total are in the resulting DataFrame. We will talk about this in more detail in the later chapters. fun_implemented(tbl_weather, origin) ## # Source: spark&lt;?&gt; [?? x 16] ## id origin year month day hour temp dewp humid wind_dir wind_speed ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 ewr 2013 1 1 1 39.0 26.1 59.4 270 10.4 ## 2 2 ewr 2013 1 1 2 39.0 27.0 61.6 250 8.06 ## 3 3 ewr 2013 1 1 3 39.0 28.0 64.4 240 11.5 ## 4 4 ewr 2013 1 1 4 39.9 28.0 62.2 250 12.7 ## 5 5 ewr 2013 1 1 5 39.0 28.0 64.4 260 12.7 ## 6 6 ewr 2013 1 1 6 37.9 28.0 67.2 240 11.5 ## 7 7 ewr 2013 1 1 7 39.0 28.0 64.4 240 15.0 ## 8 8 ewr 2013 1 1 8 39.9 28.0 62.2 250 10.4 ## 9 9 ewr 2013 1 1 9 39.9 28.0 62.2 260 15.0 ## 10 10 ewr 2013 1 1 10 41 28.0 59.6 260 13.8 ## # … with more rows, and 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, ## # pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt; So how does Spark know the R function tolower()? Our function call worked within Spark because the R function tolower() was translated by dbplyr to Spark SQL function LOWER and the resulting query was sent to Spark to be executed. We can see the actual translated SQL by running sql_render() on the function call: dbplyr::sql_render( fun_implemented(tbl_weather, origin) ) ## &lt;SQL&gt; SELECT `id`, LOWER(`origin`) AS `origin`, `year`, `month`, `day`, `hour`, `temp`, `dewp`, `humid`, `wind_dir`, `wind_speed`, `wind_gust`, `precip`, `pressure`, `visib`, `time_hour` ## FROM `weather` 5.3 An R function not translated to Spark SQL Using the following fun_r_only() function will only yield the expected results for a local data frame nycflights13::weather. For the remote Spark object referenced by tbl_weather we will get an error: # An R function `casefold` not translated to Spark SQL fun_r_only &lt;- function(df, col) { df %&gt;% mutate({{col}} := casefold({{col}}, upper = FALSE)) } The function executes successfully on a local R data frame (tibble) as R knows the function casefold: fun_r_only(nycflights13::weather, origin) ## # A tibble: 26,115 x 15 ## origin year month day hour temp dewp humid wind_dir wind_speed ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ewr 2013 1 1 1 39.0 26.1 59.4 270 10.4 ## 2 ewr 2013 1 1 2 39.0 27.0 61.6 250 8.06 ## 3 ewr 2013 1 1 3 39.0 28.0 64.4 240 11.5 ## 4 ewr 2013 1 1 4 39.9 28.0 62.2 250 12.7 ## 5 ewr 2013 1 1 5 39.0 28.0 64.4 260 12.7 ## 6 ewr 2013 1 1 6 37.9 28.0 67.2 240 11.5 ## 7 ewr 2013 1 1 7 39.0 28.0 64.4 240 15.0 ## 8 ewr 2013 1 1 8 39.9 28.0 62.2 250 10.4 ## 9 ewr 2013 1 1 9 39.9 28.0 62.2 260 15.0 ## 10 ewr 2013 1 1 10 41 28.0 59.6 260 13.8 ## # … with 26,105 more rows, and 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, ## # pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt; Trying to execute fun_r_only() against a Spark DataFrame however errors: fun_r_only(tbl_weather, origin) ## Error: org.apache.spark.sql.catalyst.parser.ParseException: ## mismatched input &#39;AS&#39; expecting &#39;)&#39;(line 1, pos 38) ## ## == SQL == ## SELECT `id`, casefold(`origin`, FALSE AS `upper`) AS `origin`, `year`, `month`, `day`, `hour`, `temp`, `dewp`, `humid`, `wind_dir`, `wind_speed`, `wind_gust`, `precip`, `pressure`, `visib`, `time_hour` ... This is because there simply is no translation provided by dbplyr for the casefold() function. The generated Spark SQL will therefore not be valid and throw an error once the Spark SQL parser tries to parse it. 5.4 A Hive built-in function not existing in R On the other hand, using the below fun_hive_builtin() function will only yield the expected results for the remote Spark object referenced by tbl_weather. For the local data frame nycflights13::weather we will get an error: # A Hive built-in function `lower` not existing in R fun_hive_builtin &lt;- function(df, col) { df %&gt;% mutate({{col}} := lower({{col}})) } The function fails to execute on a local R data frame (tibble) as R does not know the function lower(): fun_hive_builtin(nycflights13::weather, origin) ## Error in lower(~origin): could not find function &quot;lower&quot; However, against a Spark DataFrame the code works as desired: fun_hive_builtin(tbl_weather, origin) ## # Source: spark&lt;?&gt; [?? x 16] ## id origin year month day hour temp dewp humid wind_dir wind_speed ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 ewr 2013 1 1 1 39.0 26.1 59.4 270 10.4 ## 2 2 ewr 2013 1 1 2 39.0 27.0 61.6 250 8.06 ## 3 3 ewr 2013 1 1 3 39.0 28.0 64.4 240 11.5 ## 4 4 ewr 2013 1 1 4 39.9 28.0 62.2 250 12.7 ## 5 5 ewr 2013 1 1 5 39.0 28.0 64.4 260 12.7 ## 6 6 ewr 2013 1 1 6 37.9 28.0 67.2 240 11.5 ## 7 7 ewr 2013 1 1 7 39.0 28.0 64.4 240 15.0 ## 8 8 ewr 2013 1 1 8 39.9 28.0 62.2 250 10.4 ## 9 9 ewr 2013 1 1 9 39.9 28.0 62.2 260 15.0 ## 10 10 ewr 2013 1 1 10 41 28.0 59.6 260 13.8 ## # … with more rows, and 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, ## # pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt; This is because, as seen above the function lower() does not exist in R itself. For a non-existing R function there obviously can be no dbplyr translation either. In this case, dbplyr keeps it as-is when translating to SQL, not doing any translation. The SQL will be valid and executed without problems because lower is, in fact, a function built-in to Hive, so the following generated SQL is valid. dbplyr::sql_render(fun_hive_builtin(tbl_weather, origin)) ## &lt;SQL&gt; SELECT `id`, lower(`origin`) AS `origin`, `year`, `month`, `day`, `hour`, `temp`, `dewp`, `humid`, `wind_dir`, `wind_speed`, `wind_gust`, `precip`, `pressure`, `visib`, `time_hour` ## FROM `weather` 5.5 Using non-translated functions with sparklyr It can easily happen that one of the functions we want to use falls into the category where it is neither translated or a Hive built-in function. In this case, there is another interface provided by sparklyr that can allow us to do that - the spark_apply() function. We will look into this interface in more detail in the next chapter. "],
["non-translated-functions-with-spark-apply.html", "Chapter 6 Non-translated functions with spark_apply 6.1 What is so important about this distinction? 6.2 What happens when we use custom functions with spark_apply 6.3 What happens when we use translated or Hive built-in functions 6.4 Which R functionality is currently translated and built-in to Hive 6.5 Making serialization faster with Apache Arrow 6.6 Conclusion, take-home messages 6.7 But we still need arbitrary functions to run fast", " Chapter 6 Non-translated functions with spark_apply In this chapter we will look into the spark_apply() interface and how it communicates with the Spark instance. As an example we will try to rewrite the function from the previous chapter allowing to use the non-translated casefold(). # Define a custom function using `casefold` fun_r_custom &lt;- function(tbl, colName) { tbl[[colName]] &lt;- casefold(tbl[[colName]], upper = FALSE) tbl } Now, we can use the spark_apply interface to execute our custom function on a Spark DataFrame, providing the name of the column on which we want to apply the function as the context argument. # Execute on Spark via `spark_apply`: head(tbl_weather) %&gt;% spark_apply(fun_r_custom, context = {colName &lt;- &quot;origin&quot;}) ## # Source: spark&lt;?&gt; [?? x 16] ## id origin year month day hour temp dewp humid wind_dir wind_speed ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 ewr 2013 1 1 1 39.0 26.1 59.4 270 10.4 ## 2 2 ewr 2013 1 1 2 39.0 27.0 61.6 250 8.06 ## 3 3 ewr 2013 1 1 3 39.0 28.0 64.4 240 11.5 ## 4 4 ewr 2013 1 1 4 39.9 28.0 62.2 250 12.7 ## 5 5 ewr 2013 1 1 5 39.0 28.0 64.4 260 12.7 ## 6 6 ewr 2013 1 1 6 37.9 28.0 67.2 240 11.5 ## # … with 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, pressure &lt;dbl&gt;, ## # visib &lt;dbl&gt;, time_hour &lt;dttm&gt; 6.1 What is so important about this distinction? We have now shown that we can also send code that was not translated by dbplyr to Spark and get it executed without issues using spark_apply(). So what is the catch and where does the importance of the meaning of the word interface come in? Let us quickly examine the performance of the operations: mb = microbenchmark::microbenchmark( times = 10, hive_builtin = fun_hive_builtin(tbl_weather, origin) %&gt;% collect(), translated_dplyr = fun_implemented(tbl_weather, origin) %&gt;% collect(), spark_apply = spark_apply(tbl_weather, fun_r_custom, context = {colName &lt;- &quot;origin&quot;}) %&gt;% collect() ) Note that the absolute values here will vary based on the setup and the infrastructure, the important message is in the relative differences, not in the absolute timings. We can see that the operations executed via the SQL translation mechanism of dbplyr were executed in around 0.5 seconds while those via spark_apply took orders of magnitude longer - more than 6 minutes. 6.2 What happens when we use custom functions with spark_apply We can now see that the operation with spark_apply() is extremely slow compared to the other two. The key to understanding the difference is to examine how the custom transformations of data using R functions are performed within spark_apply(). In simplified terms, this happens in a few steps: the data is moved in row-format from Spark into the R process through a socket connection. This is inefficient as multiple data types need to be deserialized over each row the data gets converted to columnar format since this is how R data frames are implemented the R functions are applied to compute the results the results are again converted to row-format, serialized row-by-row and sent back to Spark over the socket connection 6.3 What happens when we use translated or Hive built-in functions When using functions that can be translated to Spark SQL the process is very different The call is translated to Spark SQL using the dbplyr backend The constructed query is sent to Spark for execution using DBI Only when collect() or compute() is called, the SQL is executed within Spark Only when collect() is called the results are also sent to the R session This means that the transfer of data only happens once and only when collect() is called, which saves a vast amount of overhead. 6.4 Which R functionality is currently translated and built-in to Hive An important question to answer with regards to performance then is what amount of functionality is available using the fast dbplyr backend. As seen above, these features can be categorized into two groups: R functions translatable to Spark SQL via dbplyr. The full list of such functions is available on RStudio’s sparklyr website Hive built-in functions that get translated as they are and can be evaluated by Spark. The full list is available on the Hive Operators and User-Defined Functions website. 6.5 Making serialization faster with Apache Arrow 6.5.1 What is Apache Arrow and how it improves performance Our benchmarks have shown that using spark_apply() does not scale well and the penalty of the bottleneck in performance caused by serialization, deserialization, and transfer is too high. To partially mitigate this we can take advantage of Apache Arrow, a cross-language development platform for in-memory data that specifies a standardized language-independent columnar memory format for flat and hierarchical data. By adding support for Arrow in sparklyr, it makes Spark perform the row-format to column-format conversion in parallel in Spark, data is then transferred through the socket but no custom serialization takes place and all the R process needs to do is copy this data from the socket into its heap, transform it and copy it back to the socket connection. This makes the process significantly faster: mb = microbenchmark::microbenchmark( times = 10, setup = library(arrow), hive_builtin = fun_hive_builtin(tbl_weather, origin) %&gt;% collect(), translated_dplyr = fun_implemented(tbl_weather, origin) %&gt;% collect(), spark_apply_arrow = spark_apply(tbl_weather, fun_r_custom, context = {colName &lt;- &quot;origin&quot;}) %&gt;% collect() ) We can see that the timing on spark_apply() decreased from more than 6 minutes to around 4.5 seconds, which is a very signigicant performance boost. Compared to the other methods we however still experience an order of magnitude difference. 6.6 Conclusion, take-home messages Adding Arrow to the mix certainly significantly improved the performance of our example code, but is still quite slow compared to the native approach. Based on the above, we could conclude that: Performance benefits are present mainly when all the computation is performed within Spark and R serves merely as a “messaging agent”, sending commands to Spark to be executed If there are object serialization and transfer of larger objects present, performance is strongly impacted. The take-home message from this exercise is that We should strive to only use R code that can be executed within the Spark instance - If we need some data retrieved, it is advisable that this is data that was previously heavily aggregated within Spark and only a small amount is transferred to the R session. 6.7 But we still need arbitrary functions to run fast In the next chapters, we will investigate a few options that allow us to retain the performance of Spark while still being able to write arbitrary R functions by using methods already implemented and available in the Spark API. Rewriting the functions as collections of dplyr verbs that all support translation to Spark SQL Rewriting the functions into Spark SQL and execute them via Spark [Rewriting the functions as series of Scala method invocations]((using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html) "],
["constructing-functions-by-piping-dplyr-verbs.html", "Chapter 7 Constructing functions by piping dplyr verbs 7.1 R functions as combinations of dplyr verbs and Spark 7.2 A more complex use case - Joins, group bys, and aggregations 7.3 Using the functions with local versus remote datasets 7.4 Conclusion, take-home messages", " Chapter 7 Constructing functions by piping dplyr verbs In the previous chapters, we looked at how the sparklyr interface communicates with the Spark instance and what this means for performance with regards to arbitrarily defined R functions. We also examined how Apache Arrow can increase the performance of data transfers between the R session and the Spark instance. In this chapter, we will look at how to write R functions that can be executed directly by Spark without serialization overhead that we have shown in the previous installment. We will focus on writing functions as combinations of dplyr verbs that can be translated using dbplyr and investigate how the SQL is generated and Spark plans created. 7.1 R functions as combinations of dplyr verbs and Spark One of the approaches to retain the performance of Spark with arbitrary R functionality is to carefully design our functions such that in its entirety when using it with sparklyr, the function call can be translated directly to Spark SQL using dbplyr. This allows us to write, package, test, and document the functions as we normally would, while still getting the performance benefits of Apache Spark. Let’s look at an example where we would like to do simple transformations of data stored in a column of a data frame, such as normalization of one of the columns. For illustration purposes, we will normalize the values of a column by first subtracting the mean value and then dividing the values by the standard deviation. 7.1.1 Trying it with base R functions The first attempt could be quite simple, we could attempt to take advantage of R’s base function scale() to do the work for us: normalize_dplyr_scale &lt;- function(df, col, newColName) { df %&gt;% mutate(!!newColName := scale({{col}})) } This function would work fine with a local data frame such as weather: weather %&gt;% normalize_dplyr_scale(temp, &quot;normTemp&quot;) %&gt;% select(id, temp, normTemp) ## # A tibble: 26,115 x 3 ## id temp normTemp[,1] ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 39.0 -0.913 ## 2 2 39.0 -0.913 ## 3 3 39.0 -0.913 ## 4 4 39.9 -0.862 ## 5 5 39.0 -0.913 ## 6 6 37.9 -0.974 ## 7 7 39.0 -0.913 ## 8 8 39.9 -0.862 ## 9 9 39.9 -0.862 ## 10 10 41 -0.802 ## # … with 26,105 more rows However for a Spark DataFrame this would throw an error. This is because the base R function scale() is not translated by dbplyr at the moment and it is not a Hive built-in function either: tbl_weather %&gt;% normalize_dplyr_scale(temp, &quot;normTemp&quot;) %&gt;% select(id, temp, normTemp) ## Error: org.apache.spark.sql.AnalysisException: Undefined function: &#39;scale&#39;. This function is neither a registered temporary function nor a permanent function registered in the database &#39;default&#39;.; line 1 pos 21 ## at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15$$anonfun$applyOrElse$49.apply(Analyzer.scala:1281) ## at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15$$anonfun$applyOrElse$49.apply(Analyzer.scala:1281) ## at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53) ... 7.1.2 Using a combination of supported dplyr verbs and operations To run the function successfully, we will need to rewrite it as a combination of functions and operations that are supported by the dbplyr translation to Spark SQL. One example implementation is as follows: normalize_dplyr &lt;- function(df, col, newColName) { df %&gt;% mutate( !!newColName := ({{col}} - mean({{col}}, na.rm = TRUE)) / sd({{col}}, na.rm = TRUE) ) } Using this function yields the desired results for both local and Spark data frames: ## Local data frame weather %&gt;% normalize_dplyr(temp, &quot;normTemp&quot;) %&gt;% select(id, temp, normTemp) ## # A tibble: 26,115 x 3 ## id temp normTemp ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 39.0 -0.913 ## 2 2 39.0 -0.913 ## 3 3 39.0 -0.913 ## 4 4 39.9 -0.862 ## 5 5 39.0 -0.913 ## 6 6 37.9 -0.974 ## 7 7 39.0 -0.913 ## 8 8 39.9 -0.862 ## 9 9 39.9 -0.862 ## 10 10 41 -0.802 ## # … with 26,105 more rows ## Spark DataFrame tbl_weather %&gt;% normalize_dplyr(temp, &quot;normTemp&quot;) %&gt;% select(id, temp, normTemp) %&gt;% collect() ## # A tibble: 26,115 x 3 ## id temp normTemp ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 39.0 -0.913 ## 2 2 39.0 -0.913 ## 3 3 39.0 -0.913 ## 4 4 39.9 -0.862 ## 5 5 39.0 -0.913 ## 6 6 37.9 -0.974 ## 7 7 39.0 -0.913 ## 8 8 39.9 -0.862 ## 9 9 39.9 -0.862 ## 10 10 41 -0.802 ## # … with 26,105 more rows 7.1.3 Investigating the SQL translation and its Spark plan Another advantage of this approach is that we can investigate the plan by which the actions will be executed by Spark using the explain() function from the dplyr package. This will print both the SQL query constructed by dbplyr and the plan generated by Spark, which can help us investigate performance issues: tbl_weather %&gt;% normalize_dplyr(temp, &quot;normTemp&quot;) %&gt;% dplyr::explain() ## &lt;SQL&gt; ## SELECT `id`, `origin`, `year`, `month`, `day`, `hour`, `temp`, `dewp`, `humid`, `wind_dir`, `wind_speed`, `wind_gust`, `precip`, `pressure`, `visib`, `time_hour`, (`temp` - AVG(`temp`) OVER ()) / stddev_samp(`temp`) OVER () AS `normTemp` ## FROM `weather` ## ## &lt;PLAN&gt; ## == Physical Plan == ## *(1) Project [id#24, origin#25, year#26, month#27, day#28, hour#29, temp#30, dewp#31, humid#32, wind_dir#33, wind_speed#34, wind_gust#35, precip#36, pressure#37, visib#38, time_hour#39, ((temp#30 - _we0#1995) / _we1#1996) AS normTemp#1981] ## +- Window [avg(temp#30) windowspecdefinition(specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1995, stddev_samp(temp#30) windowspecdefinition(specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we1#1996] ## +- Exchange SinglePartition ## +- InMemoryTableScan [id#24, origin#25, year#26, month#27, day#28, hour#29, temp#30, dewp#31, humid#32, wind_dir#33, wind_speed#34, wind_gust#35, precip#36, pressure#37, visib#38, time_hour#39] ## +- InMemoryRelation [id#24, origin#25, year#26, month#27, day#28, hour#29, temp#30, dewp#31, humid#32, wind_dir#33, wind_speed#34, wind_gust#35, precip#36, pressure#37, visib#38, time_hour#39], StorageLevel(disk, memory, deserialized, 1 replicas) ## +- Scan ExistingRDD[id#24,origin#25,year#26,month#27,day#28,hour#29,temp#30,dewp#31,humid#32,wind_dir#33,wind_speed#34,wind_gust#35,precip#36,pressure#37,visib#38,time_hour#39] If we are only interested in the SQL itself as a character string, we can use dbplyr’s sql_render(): tbl_weather %&gt;% normalize_dplyr(temp, &quot;normTemp&quot;) %&gt;% dbplyr::sql_render() %&gt;% unclass() ## [1] &quot;SELECT `id`, `origin`, `year`, `month`, `day`, `hour`, `temp`, `dewp`, `humid`, `wind_dir`, `wind_speed`, `wind_gust`, `precip`, `pressure`, `visib`, `time_hour`, (`temp` - AVG(`temp`) OVER ()) / stddev_samp(`temp`) OVER () AS `normTemp`\\nFROM `weather`&quot; 7.2 A more complex use case - Joins, group bys, and aggregations The dplyr syntax makes it very easy to construct more complex aggregations across multiple Spark DataFrames. An example of a function that joins 2 Spark DataFrames and computes a mean of a selected column, grouped by another column can look as follows: joingrpagg_dplyr &lt;- function( df1, df2, joinColNames = setdiff(intersect(colnames(df1), colnames(df2)), &quot;id&quot;), col, groupCol ) { df1 %&gt;% right_join(df2, by = joinColNames) %&gt;% filter(!is.na({{col}})) %&gt;% group_by({{groupCol}}) %&gt;% summarise(mean({{col}})) %&gt;% arrange({{groupCol}}) } We can then use this function for instance to look at the mean arrival delay of flights grouped by visibility. Note that we are only collecting heavily aggregated data - 20 rows in total. The overhead of data transfer from the Spark instance to the R session is therefore small. Also, just assigning the function call to delay_by_visib does not actually execute or collect anything, execution really starts only when collect() is called: delay_by_visib &lt;- joingrpagg_dplyr( tbl_flights, tbl_weather, col = arr_delay, groupCol = visib ) delay_by_visib %&gt;% collect() ## Warning: Missing values are always removed in SQL. ## Use `mean(x, na.rm = TRUE)` to silence this warning ## This warning is displayed only once per session. ## # A tibble: 20 x 2 ## visib `mean(arr_delay)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 24.9 ## 2 0.06 28.5 ## 3 0.12 45.4 ## 4 0.25 20.8 ## 5 0.5 39.8 ## 6 0.75 41.4 ## 7 1 37.6 ## 8 1.25 65.1 ## 9 1.5 34.7 ## 10 1.75 45.6 ## 11 2 26.3 ## 12 2.5 21.7 ## 13 3 21.7 ## 14 4 17.7 ## 15 5 18.9 ## 16 6 17.3 ## 17 7 16.4 ## 18 8 16.1 ## 19 9 15.6 ## 20 10 4.32 We can look at the plan and the generated SQL query as well: delay_by_visib %&gt;% dplyr::explain() ## &lt;SQL&gt; ## SELECT `visib`, AVG(`arr_delay`) AS `mean(arr_delay)` ## FROM (SELECT `LHS`.`id` AS `id.x`, `RHS`.`year` AS `year`, `RHS`.`month` AS `month`, `RHS`.`day` AS `day`, `LHS`.`dep_time` AS `dep_time`, `LHS`.`sched_dep_time` AS `sched_dep_time`, `LHS`.`dep_delay` AS `dep_delay`, `LHS`.`arr_time` AS `arr_time`, `LHS`.`sched_arr_time` AS `sched_arr_time`, `LHS`.`arr_delay` AS `arr_delay`, `LHS`.`carrier` AS `carrier`, `LHS`.`flight` AS `flight`, `LHS`.`tailnum` AS `tailnum`, `RHS`.`origin` AS `origin`, `LHS`.`dest` AS `dest`, `LHS`.`air_time` AS `air_time`, `LHS`.`distance` AS `distance`, `RHS`.`hour` AS `hour`, `LHS`.`minute` AS `minute`, `RHS`.`time_hour` AS `time_hour`, `RHS`.`id` AS `id.y`, `RHS`.`temp` AS `temp`, `RHS`.`dewp` AS `dewp`, `RHS`.`humid` AS `humid`, `RHS`.`wind_dir` AS `wind_dir`, `RHS`.`wind_speed` AS `wind_speed`, `RHS`.`wind_gust` AS `wind_gust`, `RHS`.`precip` AS `precip`, `RHS`.`pressure` AS `pressure`, `RHS`.`visib` AS `visib` ## FROM `flights` AS `LHS` ## RIGHT JOIN `weather` AS `RHS` ## ON (`LHS`.`year` = `RHS`.`year` AND `LHS`.`month` = `RHS`.`month` AND `LHS`.`day` = `RHS`.`day` AND `LHS`.`origin` = `RHS`.`origin` AND `LHS`.`hour` = `RHS`.`hour` AND `LHS`.`time_hour` = `RHS`.`time_hour`) ## ) `dbplyr_003` ## WHERE (NOT(((`arr_delay`) IS NULL))) ## GROUP BY `visib` ## ORDER BY `visib` ## ## &lt;PLAN&gt; ## == Physical Plan == ## *(4) Sort [visib#38 ASC NULLS FIRST], true, 0 ## +- Exchange rangepartitioning(visib#38 ASC NULLS FIRST, 4) ## +- *(3) HashAggregate(keys=[visib#38], functions=[avg(arr_delay#411)]) ## +- Exchange hashpartitioning(visib#38, 4) ## +- *(2) HashAggregate(keys=[visib#38], functions=[partial_avg(arr_delay#411)]) ## +- *(2) Project [arr_delay#411, visib#38] ## +- *(2) BroadcastHashJoin [year#403, month#404, day#405, origin#415, hour#419, time_hour#421], [year#26, month#27, day#28, origin#25, cast(hour#29 as double), time_hour#39], Inner, BuildRight ## :- *(2) Filter ((((((NOT isnull(arr_delay#411) &amp;&amp; isnotnull(hour#419)) &amp;&amp; isnotnull(year#403)) &amp;&amp; isnotnull(origin#415)) &amp;&amp; isnotnull(day#405)) &amp;&amp; isnotnull(month#404)) &amp;&amp; isnotnull(time_hour#421)) ## : +- InMemoryTableScan [year#403, month#404, day#405, arr_delay#411, origin#415, hour#419, time_hour#421], [NOT isnull(arr_delay#411), isnotnull(hour#419), isnotnull(year#403), isnotnull(origin#415), isnotnull(day#405), isnotnull(month#404), isnotnull(time_hour#421)] ## : +- InMemoryRelation [id#402, year#403, month#404, day#405, dep_time#406, sched_dep_time#407, dep_delay#408, arr_time#409, sched_arr_time#410, arr_delay#411, carrier#412, flight#413, tailnum#414, origin#415, dest#416, air_time#417, distance#418, hour#419, minute#420, time_hour#421], StorageLevel(disk, memory, deserialized, 1 replicas) ## : +- Scan ExistingRDD[id#402,year#403,month#404,day#405,dep_time#406,sched_dep_time#407,dep_delay#408,arr_time#409,sched_arr_time#410,arr_delay#411,carrier#412,flight#413,tailnum#414,origin#415,dest#416,air_time#417,distance#418,hour#419,minute#420,time_hour#421] ## +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, int, false], input[2, int, false], input[3, int, false], input[0, string, false], cast(input[4, int, false] as double), input[6, timestamp, false])) ## +- *(1) Filter (((((isnotnull(month#27) &amp;&amp; isnotnull(day#28)) &amp;&amp; isnotnull(hour#29)) &amp;&amp; isnotnull(origin#25)) &amp;&amp; isnotnull(year#26)) &amp;&amp; isnotnull(time_hour#39)) ## +- InMemoryTableScan [origin#25, year#26, month#27, day#28, hour#29, visib#38, time_hour#39], [isnotnull(month#27), isnotnull(day#28), isnotnull(hour#29), isnotnull(origin#25), isnotnull(year#26), isnotnull(time_hour#39)] ## +- InMemoryRelation [id#24, origin#25, year#26, month#27, day#28, hour#29, temp#30, dewp#31, humid#32, wind_dir#33, wind_speed#34, wind_gust#35, precip#36, pressure#37, visib#38, time_hour#39], StorageLevel(disk, memory, deserialized, 1 replicas) ## +- Scan ExistingRDD[id#24,origin#25,year#26,month#27,day#28,hour#29,temp#30,dewp#31,humid#32,wind_dir#33,wind_speed#34,wind_gust#35,precip#36,pressure#37,visib#38,time_hour#39] 7.3 Using the functions with local versus remote datasets 7.3.1 Unified front-end, different back-ends Some of the appeal of the dplyr syntax comes from the fact that we can use the same functions to conveniently manipulate local data frames in memory and, with the very same code, data from remote sources such as relational databases, data.tables and even data within Spark. This unified front-end, however, comes with some important differences that we must be aware of when applying and porting code from using it to manipulate and compute on local data versus on remote sources. The same holds for remote Spark DataFrames that we are manipulating when using dplyr functions. The following paragraphs will show a few examples of issues we can come across when porting local data handling to a remote source such as Spark. 7.3.2 Differences in NA and NaN values Another example of differences can arise from handling NA and NaN values: bycols &lt;- c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;origin&quot;, &quot;hour&quot;) # Create left joins joined_spark &lt;- tbl_flights %&gt;% left_join(tbl_weather, by = bycols) %&gt;% collect() joined_local &lt;- flights %&gt;% left_join(weather, by = bycols) # Look at counts of NaN values joined_local %&gt;% filter(is.nan(temp)) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 0 joined_spark %&gt;% filter(is.nan(temp)) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 1573 7.3.3 Dates, times and time zones Special care must also be taken when dealing with date/time values and their time zones: # Note the time_hour values are different weather %&gt;% select(id, time_hour) ## # A tibble: 26,115 x 2 ## id time_hour ## &lt;int&gt; &lt;dttm&gt; ## 1 1 2013-01-01 01:00:00 ## 2 2 2013-01-01 02:00:00 ## 3 3 2013-01-01 03:00:00 ## 4 4 2013-01-01 04:00:00 ## 5 5 2013-01-01 05:00:00 ## 6 6 2013-01-01 06:00:00 ## 7 7 2013-01-01 07:00:00 ## 8 8 2013-01-01 08:00:00 ## 9 9 2013-01-01 09:00:00 ## 10 10 2013-01-01 10:00:00 ## # … with 26,105 more rows tbl_weather %&gt;% select(id, time_hour) ## # Source: spark&lt;?&gt; [?? x 2] ## id time_hour ## &lt;int&gt; &lt;dttm&gt; ## 1 1 2013-01-01 06:00:00 ## 2 2 2013-01-01 07:00:00 ## 3 3 2013-01-01 08:00:00 ## 4 4 2013-01-01 09:00:00 ## 5 5 2013-01-01 10:00:00 ## 6 6 2013-01-01 11:00:00 ## 7 7 2013-01-01 12:00:00 ## 8 8 2013-01-01 13:00:00 ## 9 9 2013-01-01 14:00:00 ## 10 10 2013-01-01 15:00:00 ## # … with more rows 7.3.4 Joins Another example of a different behavior is joining due to matching column values. Care must be taken to write conditions that are portable to ensure that the joins are consistent across data sources. 7.3.5 Portability of used methods And, rather obviously, when using Hive built-in functions in the dplyr-based function, we will most likely not be able to execute it on the local data frames, as we have seen previously. 7.4 Conclusion, take-home messages In this chapter, we have shown that we can take advantage of the performance of Spark while still writing arbitrary R functions by using dplyr syntax, which supports translation to Spark SQL using the dbplyr backend. We have also looked at some important differences when applying the same dplyr transformations to local and remote data sets. With this approach, we can use R development best practices, testing, and documentation methods in a standard way when writing our R packages, getting the best of both worlds - Apache Spark for performance and R for convenient development of data science applications. In the next chapter, we will look at writing R functions that will be using SQL directly, instead of relying on dbplyr for the translation, and how we can efficiently send them to the Spark instance for execution and optionally retrieve the results to our R session. "],
["constructing-sql-and-executing-it-with-spark.html", "Chapter 8 Constructing SQL and executing it with Spark 8.1 R functions as Spark SQL generators 8.2 Executing the generated queries via Spark 8.3 Where SQL can be better than dbplyr translation", " Chapter 8 Constructing SQL and executing it with Spark In the previous chapter of this series, we looked at writing R functions that can be executed directly by Spark without serialization overhead with a focus on writing functions as combinations of dplyr verbs and investigated how the SQL is generated and Spark plans created. In this chapter, we will look at how to write R functions that generate SQL queries that can be executed by Spark, how to execute them with DBI and how to achieve lazy SQL statements that only get executed when needed. We also briefly present wrapping these approaches into functions that can be combined with other Spark operations. 8.1 R functions as Spark SQL generators There are use cases where it is desirable to express the operations directly with SQL instead of combining dplyr verbs, for example when working within multi-language environments where re-usability is important. We can then send the SQL query directly to Spark to be executed. To create such queries, one option is to write R functions that work as query constructors. Again using a very simple example, a naive implementation of column normalization could look as follows. Note that the use of SELECT * is discouraged and only here for illustration purposes: normalize_sql &lt;- function(df, colName, newColName) { paste0( &quot;SELECT&quot;, &quot;\\n &quot;, df, &quot;.*&quot;, &quot;,&quot;, &quot;\\n (&quot;, colName, &quot; - (SELECT avg(&quot;, colName, &quot;) FROM &quot;, df, &quot;))&quot;, &quot; / &quot;, &quot;(SELECT stddev_samp(&quot;, colName,&quot;) FROM &quot;, df, &quot;) as &quot;, newColName, &quot;\\n&quot;, &quot;FROM &quot;, df ) } Using the weather dataset would then yield the following SQL query when normalizing the temp column: normalize_temp_query &lt;- normalize_sql(&quot;weather&quot;, &quot;temp&quot;, &quot;normTemp&quot;) cat(normalize_temp_query) ## SELECT ## weather.*, ## (temp - (SELECT avg(temp) FROM weather)) / (SELECT stddev_samp(temp) FROM weather) as normTemp ## FROM weather Now that we have the query created, we can look at how to send it to Spark for execution. 8.2 Executing the generated queries via Spark 8.2.1 Using DBI as the interface The R package DBI provides an interface for communication between R and relational database management systems. We can simply use the dbGetQuery() function to execute our query, for instance: res &lt;- DBI::dbGetQuery(sc, statement = normalize_temp_query) head(res) ## id origin year month day hour temp dewp humid wind_dir wind_speed wind_gust ## 1 1 EWR 2013 1 1 1 39.02 26.06 59.37 270 10.35702 NaN ## 2 2 EWR 2013 1 1 2 39.02 26.96 61.63 250 8.05546 NaN ## 3 3 EWR 2013 1 1 3 39.02 28.04 64.43 240 11.50780 NaN ## 4 4 EWR 2013 1 1 4 39.92 28.04 62.21 250 12.65858 NaN ## 5 5 EWR 2013 1 1 5 39.02 28.04 64.43 260 12.65858 NaN ## 6 6 EWR 2013 1 1 6 37.94 28.04 67.21 240 11.50780 NaN ## precip pressure visib time_hour normTemp ## 1 0 1012.0 10 2013-01-01 06:00:00 -0.9130047 ## 2 0 1012.3 10 2013-01-01 07:00:00 -0.9130047 ## 3 0 1012.5 10 2013-01-01 08:00:00 -0.9130047 ## 4 0 1012.2 10 2013-01-01 09:00:00 -0.8624083 ## 5 0 1011.9 10 2013-01-01 10:00:00 -0.9130047 ## 6 0 1012.4 10 2013-01-01 11:00:00 -0.9737203 As we might have noticed thanks to the way the result is printed, a standard data frame is returned, as opposed to tibbles returned by most sparklyr operations. It is important to note that using dbGetQuery() automatically computes and collects the results to the R session. This is in contrast with the dplyr approach which constructs the query and only collects the results to the R session when collect() is called, or computes them when compute() is called. We will now examine 2 options to use the prepared query lazily and without collecting the results into the R session. 8.2.2 Invoking sql on a Spark session object Without going into further details on the invoke() functionality of sparklyr which we will focus on in the fourth installment of the series, if the desire is to have a “lazy” SQL that does not get automatically computed and collected when called from R, we can invoke a sql method on a SparkSession class object. The method takes a string SQL query as input and processes it using Spark, returning the result as a Spark DataFrame. This gives us the ability to only compute and collect the results when desired: # Use the query &quot;lazily&quot; without execution: normalized_lazy_ds &lt;- sc %&gt;% spark_session() %&gt;% invoke(&quot;sql&quot;, normalize_temp_query) normalized_lazy_ds ## &lt;jobj[485]&gt; ## org.apache.spark.sql.Dataset ## [id: int, origin: string ... 15 more fields] # Collect when needed: normalized_lazy_ds %&gt;% collect() ## # A tibble: 26,115 x 17 ## id origin year month day hour temp dewp humid wind_dir wind_speed ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 EWR 2013 1 1 1 39.0 26.1 59.4 270 10.4 ## 2 2 EWR 2013 1 1 2 39.0 27.0 61.6 250 8.06 ## 3 3 EWR 2013 1 1 3 39.0 28.0 64.4 240 11.5 ## 4 4 EWR 2013 1 1 4 39.9 28.0 62.2 250 12.7 ## 5 5 EWR 2013 1 1 5 39.0 28.0 64.4 260 12.7 ## 6 6 EWR 2013 1 1 6 37.9 28.0 67.2 240 11.5 ## 7 7 EWR 2013 1 1 7 39.0 28.0 64.4 240 15.0 ## 8 8 EWR 2013 1 1 8 39.9 28.0 62.2 250 10.4 ## 9 9 EWR 2013 1 1 9 39.9 28.0 62.2 260 15.0 ## 10 10 EWR 2013 1 1 10 41 28.0 59.6 260 13.8 ## # … with 26,105 more rows, and 6 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, ## # pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;, normTemp &lt;dbl&gt; 8.2.3 Using tbl with dbplyr’s sql The above method gives us a reference to a Java object as a result, which might be less intuitive to work with for R users. We can also opt to use dbplyr’s sql() function in combination with tbl() to get a more familiar result. Note that when printing the below normalized_lazy_tbl, the query gets partially executed to provide the first few rows. Only when collect() is called the entire set is retrieved to the R session: # Nothing is executed yet normalized_lazy_tbl &lt;- normalize_temp_query %&gt;% dbplyr::sql() %&gt;% tbl(sc, .) # Print the first few rows normalized_lazy_tbl ## # Source: spark&lt;SELECT weather.*, (temp - (SELECT avg(temp) FROM weather)) / ## # (SELECT stddev_samp(temp) FROM weather) as normTemp FROM weather&gt; [?? x 17] ## id origin year month day hour temp dewp humid wind_dir wind_speed ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 EWR 2013 1 1 1 39.0 26.1 59.4 270 10.4 ## 2 2 EWR 2013 1 1 2 39.0 27.0 61.6 250 8.06 ## 3 3 EWR 2013 1 1 3 39.0 28.0 64.4 240 11.5 ## 4 4 EWR 2013 1 1 4 39.9 28.0 62.2 250 12.7 ## 5 5 EWR 2013 1 1 5 39.0 28.0 64.4 260 12.7 ## 6 6 EWR 2013 1 1 6 37.9 28.0 67.2 240 11.5 ## 7 7 EWR 2013 1 1 7 39.0 28.0 64.4 240 15.0 ## 8 8 EWR 2013 1 1 8 39.9 28.0 62.2 250 10.4 ## 9 9 EWR 2013 1 1 9 39.9 28.0 62.2 260 15.0 ## 10 10 EWR 2013 1 1 10 41 28.0 59.6 260 13.8 ## # … with more rows, and 6 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, ## # pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;, normTemp &lt;dbl&gt; # Collect the entire result to the R session and print normalized_lazy_tbl %&gt;% collect() ## # A tibble: 26,115 x 17 ## id origin year month day hour temp dewp humid wind_dir wind_speed ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 EWR 2013 1 1 1 39.0 26.1 59.4 270 10.4 ## 2 2 EWR 2013 1 1 2 39.0 27.0 61.6 250 8.06 ## 3 3 EWR 2013 1 1 3 39.0 28.0 64.4 240 11.5 ## 4 4 EWR 2013 1 1 4 39.9 28.0 62.2 250 12.7 ## 5 5 EWR 2013 1 1 5 39.0 28.0 64.4 260 12.7 ## 6 6 EWR 2013 1 1 6 37.9 28.0 67.2 240 11.5 ## 7 7 EWR 2013 1 1 7 39.0 28.0 64.4 240 15.0 ## 8 8 EWR 2013 1 1 8 39.9 28.0 62.2 250 10.4 ## 9 9 EWR 2013 1 1 9 39.9 28.0 62.2 260 15.0 ## 10 10 EWR 2013 1 1 10 41 28.0 59.6 260 13.8 ## # … with 26,105 more rows, and 6 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, ## # pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;, normTemp &lt;dbl&gt; 8.2.4 Wrapping the tbl approach into functions In the approach above we provided sc in the call to tbl(). When wrapping such processes into a function, it might however be useful to take the specific DataFrame reference as an input instead of the generic Spark connection reference. In that case, we can use the fact that the connection reference is also stored in the DataFrame reference, in the con sub-element of the src element. For instance, looking at our tbl_weather: class(tbl_weather[[&quot;src&quot;]][[&quot;con&quot;]]) ## [1] &quot;spark_connection&quot; &quot;spark_shell_connection&quot; &quot;DBIConnection&quot; Putting this together, we can create a simple wrapper function that lazily sends a SQL query to be processed on a particular Spark DataFrame reference: lazy_spark_query &lt;- function(tbl, qry) { qry %&gt;% dbplyr::sql() %&gt;% dplyr::tbl(tbl[[&quot;src&quot;]][[&quot;con&quot;]], .) } And use it to do the same as we did above with a single function call: lazy_spark_query(tbl_weather, normalize_temp_query) %&gt;% collect() ## # A tibble: 26,115 x 17 ## id origin year month day hour temp dewp humid wind_dir wind_speed ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 EWR 2013 1 1 1 39.0 26.1 59.4 270 10.4 ## 2 2 EWR 2013 1 1 2 39.0 27.0 61.6 250 8.06 ## 3 3 EWR 2013 1 1 3 39.0 28.0 64.4 240 11.5 ## 4 4 EWR 2013 1 1 4 39.9 28.0 62.2 250 12.7 ## 5 5 EWR 2013 1 1 5 39.0 28.0 64.4 260 12.7 ## 6 6 EWR 2013 1 1 6 37.9 28.0 67.2 240 11.5 ## 7 7 EWR 2013 1 1 7 39.0 28.0 64.4 240 15.0 ## 8 8 EWR 2013 1 1 8 39.9 28.0 62.2 250 10.4 ## 9 9 EWR 2013 1 1 9 39.9 28.0 62.2 260 15.0 ## 10 10 EWR 2013 1 1 10 41 28.0 59.6 260 13.8 ## # … with 26,105 more rows, and 6 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, ## # pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;, normTemp &lt;dbl&gt; 8.3 Where SQL can be better than dbplyr translation 8.3.1 When a translation is not there We have discussed in the first part that the set of operations translated to Spark SQL via dbplyr may not cover all possible use cases. In such a case, the option to write SQL directly is very useful. 8.3.2 When translation does not provide expected results In some instances using dbplyr to translate R operations to Spark SQL can lead to unexpected results. As one example, consider the following integer division on a column of a local data frame. # id_div_5 is as expected weather %&gt;% mutate(id_div_5 = id %/% 5L) %&gt;% select(id, id_div_5) ## # A tibble: 26,115 x 2 ## id id_div_5 ## &lt;int&gt; &lt;int&gt; ## 1 1 0 ## 2 2 0 ## 3 3 0 ## 4 4 0 ## 5 5 1 ## 6 6 1 ## 7 7 1 ## 8 8 1 ## 9 9 1 ## 10 10 2 ## # … with 26,105 more rows As expected, we get the result of integer division in the id_div_5 column. However, applying the very same operation on a Spark DataFrame yields unexpected results: # id_div_5 is normal division, not integer division tbl_weather %&gt;% mutate(id_div_5 = id %/% 5L) %&gt;% select(id, id_div_5) ## # Source: spark&lt;?&gt; [?? x 2] ## id id_div_5 ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.2 ## 2 2 0.4 ## 3 3 0.6 ## 4 4 0.8 ## 5 5 1 ## 6 6 1.2 ## 7 7 1.4 ## 8 8 1.6 ## 9 9 1.8 ## 10 10 2 ## # … with more rows This is due to the fact that translation to integer division is quite difficult to implement: https://github.com/tidyverse/dbplyr/issues/108. We could certainly figure our a way to fix this particular issue, but the workarounds may prove inefficient: tbl_weather %&gt;% mutate(id_div_5 = as.integer(id %/% 5L)) %&gt;% select(id, id_div_5) ## # Source: spark&lt;?&gt; [?? x 2] ## id id_div_5 ## &lt;int&gt; &lt;int&gt; ## 1 1 0 ## 2 2 0 ## 3 3 0 ## 4 4 0 ## 5 5 1 ## 6 6 1 ## 7 7 1 ## 8 8 1 ## 9 9 1 ## 10 10 2 ## # … with more rows # Not too efficient: tbl_weather %&gt;% mutate(id_div_5 = as.integer(id %/% 5L)) %&gt;% select(id, id_div_5) %&gt;% explain() ## &lt;SQL&gt; ## SELECT `id`, CAST(`id` / 5 AS INT) AS `id_div_5` ## FROM `weather` ## ## &lt;PLAN&gt; ## == Physical Plan == ## *(1) Project [id#24, cast((cast(id#24 as double) / 5.0) as int) AS id_div_5#5815] ## +- InMemoryTableScan [id#24] ## +- InMemoryRelation [id#24, origin#25, year#26, month#27, day#28, hour#29, temp#30, dewp#31, humid#32, wind_dir#33, wind_speed#34, wind_gust#35, precip#36, pressure#37, visib#38, time_hour#39], StorageLevel(disk, memory, deserialized, 1 replicas) ## +- Scan ExistingRDD[id#24,origin#25,year#26,month#27,day#28,hour#29,temp#30,dewp#31,humid#32,wind_dir#33,wind_speed#34,wind_gust#35,precip#36,pressure#37,visib#38,time_hour#39] Using SQL and the knowledge that Hive does provide a built-in DIV arithmetic operator, we can get the desired results very simply and efficiently with writing SQL: &quot;SELECT `id`, `id` DIV 5 `id_div_5` FROM `weather`&quot; %&gt;% dbplyr::sql() %&gt;% tbl(sc, .) ## # Source: spark&lt;SELECT `id`, `id` DIV 5 `id_div_5` FROM `weather`&gt; [?? x 2] ## id id_div_5 ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0 ## 2 2 0 ## 3 3 0 ## 4 4 0 ## 5 5 1 ## 6 6 1 ## 7 7 1 ## 8 8 1 ## 9 9 1 ## 10 10 2 ## # … with more rows Even though the numeric value of the results is correct here, we may still notice that the class of the returned id_div_5 column is actually numeric instead of integer. Such is the life of developers using data processing interfaces. 8.3.3 When portability is important Since the languages that provide interfaces to Spark are not limited to R and multi-language setups are quite common, another reason to use SQL statements directly is the portability of such solutions. A SQL statement can be executed by interfaces provided for all languages - Scala, Java, and Python, without the need to rely on R-specific packages such as dbplyr. "],
["using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html", "Chapter 9 Using the lower-level invoke API to manipulate Spark’s Java objects from R 9.1 The invoke() API of sparklyr 9.2 Getting started with the invoke API 9.3 Grouping and aggregation with invoke chains 9.4 Wrapping the invocations into R functions 9.5 Reconstructing variable normalization 9.6 Where invoke can be better than dplyr translation or SQL 9.7 Conclusion", " Chapter 9 Using the lower-level invoke API to manipulate Spark’s Java objects from R There will be no foolish wand-waving or silly incantations in this class. Severus Snape In the previous chapters, we have shown how to write functions as both combinations of dplyr verbs and SQL query generators that can be executed by Spark, how to execute them with DBI and how to achieve lazy SQL statements that only get executed when needed. In this chapter, we will look at how to write R functions that interface with Spark via a lower-level invocation API that lets us use all the functionality that is exposed by the Scala Spark APIs. We will also show how such R calls relate to Scala code. 9.1 The invoke() API of sparklyr So far when interfacing with Spark from R, we have used the sparklyr package in three ways: Writing combinations of dplyr verbs that would be translated to Spark SQL via the dbplyr package and the SQL executed by Spark when requested Generating Spark SQL code directly and sending it for execution in multiple ways Combinations of the above two methods What these methods have in common is that they translate operations written in R to Spark SQL and that SQL code is then sent for execution by our Spark instance. There is however another approach that we can use with sparklyr, which will be more familiar to users or developers who have worked with packages like rJava or rscala before. Even though arguably less convenient than the APIs provided by the 2 aforementioned packages, sparklyr provides an invocation API that exposes 3 functions: invoke(jobj, method, ...) to execute a method on a Java object reference invoke_static(sc, class, method, ...) to execute a static method associated with a Java class invoke_new(sc, class, ...) to invoke a constructor associated with a Java class Let us have a look at how we can use those functions in practice to efficiently work with Spark from R. 9.2 Getting started with the invoke API We can start with a few very simple examples of invoke() usage, for instance getting the number of rows of the tbl_flights: # Get the count of rows tbl_flights %&gt;% spark_dataframe() %&gt;% invoke(&quot;count&quot;) ## [1] 336776 We see one extra operation before invoking the count: spark_dataframe(). This is because the invoke() interface works with Java object references and not tbl objects in remote sources such as tbl_flights. We, therefore, need to convert tbl_flights to a Java object reference, for which we use the spark_dataframe() function. Now, for something more exciting, let us compute a summary of the variables in tbl_flights using the describe method: tbl_flights_summary &lt;- tbl_flights %&gt;% spark_dataframe() %&gt;% invoke(&quot;describe&quot;, as.list(colnames(tbl_flights))) %&gt;% sdf_register() tbl_flights_summary ## # Source: spark&lt;?&gt; [?? x 20] ## summary id year month day dep_time sched_dep_time dep_delay arr_time ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 count 3367… 3367… 3367… 3367… 328521 336776 328521 328063 ## 2 mean 1683… 2013… 6.54… 15.7… 1349.10… 1344.25484001… 12.63907… 1502.05… ## 3 stddev 9721… 0.0 3.41… 8.76… 488.281… 467.335755734… 40.21006… 533.264… ## 4 min 1 2013 1 1 1 106 -43.0 1 ## 5 max 3367… 2013 12 31 2400 2359 1301.0 2400 ## # … with 11 more variables: sched_arr_time &lt;chr&gt;, arr_delay &lt;chr&gt;, ## # carrier &lt;chr&gt;, flight &lt;chr&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;chr&gt;, distance &lt;chr&gt;, hour &lt;chr&gt;, minute &lt;chr&gt; We also one see extra operation after invoking the describe method: sdf_register(). This is because the invoke() interface also returns Java object references and we may like to see a more user-friendly tbl object instead. This is where sdf_register() comes in to register a Spark DataFrame and return a tbl_spark object back to us. And indeed, we can see that the wrapper sdf_describe() provided by the sparklyr package itself works in a very similar fashion: body(sparklyr::sdf_describe) ## { ## in_df &lt;- cols %in% colnames(x) ## if (any(!in_df)) { ## msg &lt;- paste0(&quot;The following columns are not in the data frame: &quot;, ## paste0(cols[which(!in_df)], collapse = &quot;, &quot;)) ## stop(msg) ## } ## cols &lt;- cast_character_list(cols) ## x %&gt;% spark_dataframe() %&gt;% invoke(&quot;describe&quot;, cols) %&gt;% ## sdf_register() ## } If we so wish, for DataFrame related object references, we can also call collect() to retrieve the results directly, without using sdf_register() first, for instance retrieving the full content of the origin column: tbl_flights %&gt;% spark_dataframe() %&gt;% invoke(&quot;select&quot;, &quot;origin&quot;, list()) %&gt;% collect() ## # A tibble: 336,776 x 1 ## origin ## &lt;chr&gt; ## 1 EWR ## 2 LGA ## 3 JFK ## 4 JFK ## 5 LGA ## 6 EWR ## 7 EWR ## 8 LGA ## 9 JFK ## 10 LGA ## # … with 336,766 more rows It can also be helpful to investigate the schema of our flights DataFrame: tbl_flights %&gt;% spark_dataframe() %&gt;% invoke(&quot;schema&quot;) ## &lt;jobj[733]&gt; ## org.apache.spark.sql.types.StructType ## StructType(StructField(id,IntegerType,true), StructField(year,IntegerType,true), StructField(month,IntegerType,true), StructField(day,IntegerType,true), StructField(dep_time,IntegerType,true), StructField(sched_dep_time,IntegerType,true), StructField(dep_delay,DoubleType,true), StructField(arr_time,IntegerType,true), StructField(sched_arr_time,IntegerType,true), StructField(arr_delay,DoubleType,true), StructField(carrier,StringType,true), StructField(flight,IntegerType,true), StructField(tailnum,StringType,true), StructField(origin,StringType,true), StructField(dest,StringType,true), StructField(air_time,DoubleType,true), StructField(distance,DoubleType,true), StructField(hour,DoubleType,true), StructField(minute,DoubleType,true), StructField(time_hour,TimestampType,true)) We can also use the invoke interface on other objects, for instance the SparkContext. Let’s for instance retrieve the uiWebUrl of our context: sc %&gt;% spark_context() %&gt;% invoke(&quot;uiWebUrl&quot;) %&gt;% invoke(&quot;toString&quot;) ## [1] &quot;Some(http://localhost:4040)&quot; 9.3 Grouping and aggregation with invoke chains Imagine we would like to do simple aggregations of a Spark DataFrame, such as an average of a column grouped by another column. For reference, we can do this very simply using the dplyr approach. Let’s compute the average departure delay by origin of the flight: tbl_flights %&gt;% group_by(origin) %&gt;% summarise(avg(dep_delay)) ## # Source: spark&lt;?&gt; [?? x 2] ## origin `avg(dep_delay)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 JFK 12.1 ## 2 LGA 10.3 ## 3 EWR 15.1 Now we will show how to do the same aggregation via the lower level API. Using the Spark shell we would simply write in Scala: flights. groupBy(&quot;origin&quot;). agg(avg(&quot;dep_delay&quot;)) Translating that into the lower level invoke() API provided by sparklyr can look similar to the following code. tbl_flights %&gt;% spark_dataframe() %&gt;% invoke(&quot;groupBy&quot;, &quot;origin&quot;, list()) %&gt;% invoke(&quot;agg&quot;, invoke_static(sc, &quot;org.apache.spark.sql.functions&quot;, &quot;expr&quot;, &quot;avg(dep_delay)&quot;), list()) %&gt;% sdf_register() ## # Source: spark&lt;?&gt; [?? x 2] ## origin `avg(dep_delay)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 JFK 12.1 ## 2 LGA 10.3 ## 3 EWR 15.1 9.3.1 What is all that extra code? Now, compared to the very simple 2 operations in the Scala version, we have some gotchas to examine: one of the invoke() calls is quite long. Instead of just avg(&quot;dep_delay&quot;) like in the Scala example, we use invoke_static(sc, &quot;org.apache.spark.sql.functions&quot;, &quot;expr&quot;, &quot;avg(dep_delay)&quot;). This is because the avg(&quot;dep_delay&quot;) expression is somewhat of a syntactic sugar provided by Scala, but when calling from R we need to provide the object reference hidden behind that sugar. the empty list() at the end of the &quot;groupBy&quot; and &quot;agg&quot; invokes. This is needed as a workaround some Scala methods take String, String* as arguments and sparklyr currently does not support variable parameters. We can pass list() to represent an empty String[] in Scala as the needed second argument. 9.4 Wrapping the invocations into R functions Seeing the above example, we can quickly write a useful wrapper to ease the pain a little. First, we can create a small function that will generate the aggregation expression we can use with invoke(&quot;agg&quot;, ...): agg_expr &lt;- function(tbl, exprs) { sparklyr::invoke_static( tbl[[&quot;src&quot;]][[&quot;con&quot;]], &quot;org.apache.spark.sql.functions&quot;, &quot;expr&quot;, exprs ) } Next, we can wrap around the entire process to make a more generic aggregation function, using the fact that a remote tibble has the details on sc within its tbl[[&quot;src&quot;]][[&quot;con&quot;]] element: grpagg_invoke &lt;- function(tbl, colName, groupColName, aggOperation) { avgColumn &lt;- tbl %&gt;% agg_expr(paste0(aggOperation, &quot;(&quot;, colName, &quot;)&quot;)) tbl %&gt;% spark_dataframe() %&gt;% invoke(&quot;groupBy&quot;, groupColName, list()) %&gt;% invoke(&quot;agg&quot;, avgColumn, list()) %&gt;% sdf_register() } And finally use our wrapper to get the same results in a more user-friendly way: tbl_flights %&gt;% grpagg_invoke(&quot;arr_delay&quot;, groupColName = &quot;origin&quot;, aggOperation = &quot;avg&quot;) ## # Source: spark&lt;?&gt; [?? x 2] ## origin `avg(arr_delay)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 JFK 5.55 ## 2 LGA 5.78 ## 3 EWR 9.11 9.5 Reconstructing variable normalization Now we will attempt to construct the variable normalization that we have shown in the previous parts with dplyr verbs and SQL generation - we will normalize the values of a column by first subtracting the mean value and then dividing the values by the standard deviation: normalize_invoke &lt;- function(tbl, colName) { sdf &lt;- tbl %&gt;% spark_dataframe() stdCol &lt;- agg_expr(tbl, paste0(&quot;stddev_samp(&quot;, colName, &quot;)&quot;)) avgCol &lt;- agg_expr(tbl, paste0(&quot;avg(&quot;, colName, &quot;)&quot;)) avgTemp &lt;- sdf %&gt;% invoke(&quot;agg&quot;, avgCol, list()) %&gt;% invoke(&quot;first&quot;) stdTemp &lt;- sdf %&gt;% invoke(&quot;agg&quot;, stdCol, list()) %&gt;% invoke(&quot;first&quot;) newCol &lt;- sdf %&gt;% invoke(&quot;col&quot;, colName) %&gt;% invoke(&quot;minus&quot;, as.numeric(avgTemp)) %&gt;% invoke(&quot;divide&quot;, as.numeric(stdTemp)) sdf %&gt;% invoke(&quot;withColumn&quot;, colName, newCol) %&gt;% sdf_register() } tbl_weather %&gt;% normalize_invoke(&quot;temp&quot;) ## # Source: spark&lt;?&gt; [?? x 16] ## id origin year month day hour temp dewp humid wind_dir wind_speed ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 EWR 2013 1 1 1 -0.913 26.1 59.4 270 10.4 ## 2 2 EWR 2013 1 1 2 -0.913 27.0 61.6 250 8.06 ## 3 3 EWR 2013 1 1 3 -0.913 28.0 64.4 240 11.5 ## 4 4 EWR 2013 1 1 4 -0.862 28.0 62.2 250 12.7 ## 5 5 EWR 2013 1 1 5 -0.913 28.0 64.4 260 12.7 ## 6 6 EWR 2013 1 1 6 -0.974 28.0 67.2 240 11.5 ## 7 7 EWR 2013 1 1 7 -0.913 28.0 64.4 240 15.0 ## 8 8 EWR 2013 1 1 8 -0.862 28.0 62.2 250 10.4 ## 9 9 EWR 2013 1 1 9 -0.862 28.0 62.2 260 15.0 ## 10 10 EWR 2013 1 1 10 -0.802 28.0 59.6 260 13.8 ## # … with more rows, and 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, ## # pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt; The above implementation is just an example and far from optimal, but it also has a few interesting points about it: Using invoke(&quot;first&quot;) will actually compute and collect the value into the R session Those collected values are then sent back during the invoke(&quot;minus&quot;, as.numeric(avgTemp)) and invoke(&quot;divide&quot;, as.numeric(stdTemp)) This means that there is unnecessary overhead when sending those values from the Spark instance into R and back, which will have slight performance penalties. 9.6 Where invoke can be better than dplyr translation or SQL As we have seen in the above examples, working with the invoke() API can prove more difficult than using the intuitive syntax of dplyr or SQL queries. In some use cases, the trade-off may still be worth it. In our practice, these are some examples of such situations: When Scala’s Spark API is more flexible, powerful or suitable for a particular task and the translation is not as good When performance is crucial and we can produce more optimal solutions using the invocations When we know the Scala API well and not want to invest time to learn the dplyr syntax, but it is easier to translate the Scala calls into a series of invoke() calls When we need to interact and manipulate other Java objects apart from the standard Spark DataFrames 9.7 Conclusion In this chapter, we have looked at how to use the lower-level invoke interface provided by sparklyr to manipulate Spark objects and other Java object references. In the following part, we will dig a bit deeper and look into using Java’s reflection API to make the invoke interface more accessible from R, getting detail invocation logs and more. "],
["exploring-the-invoke-api-from-r-with-java-reflection-and-examining-invokes-with-logs.html", "Chapter 10 Exploring the invoke API from R with Java reflection and examining invokes with logs 10.1 Examining available methods from R 10.2 Using the Java reflection API to list the available methods 10.3 Investigating DataSet and SparkContext class methods 10.4 How sparklyr communicates with Spark, invoke logging 10.5 Conclusion", " Chapter 10 Exploring the invoke API from R with Java reflection and examining invokes with logs Then darkness took me, and I strayed out of thought and time, and I wandered far on roads that I will not tell Gandalf the White In the previous chapters, we have shown how to write functions as both combinations of dplyr verbs, SQL query generators that can be executed by Spark and how to use the lower-level API to invoke methods on Java object references from R. In this chapter, we will look into more details around sparklyr’s invoke() API, investigate available methods for different classes of objects using the Java reflection API and look under the hood of the sparklyr interface mechanism with invoke logging. 10.1 Examining available methods from R If you did not do so, it is recommended to read the previous chapter of this book before this one to get a quick overview of the invoke() API. 10.2 Using the Java reflection API to list the available methods The invoke() interface is powerful, but also a bit hidden from the eyes as we do not immediately know what methods are available for which object classes. We can circumvent that using the getMethods method which (in short) returns an array of Method objects reflecting public member methods of the class. For instance, retrieving a list of methods for the org.apache.spark.SparkContext class: mthds &lt;- sc %&gt;% spark_context() %&gt;% invoke(&quot;getClass&quot;) %&gt;% invoke(&quot;getMethods&quot;) head(mthds) ## [[1]] ## &lt;jobj[828]&gt; ## java.lang.reflect.Method ## public org.apache.spark.util.CallSite org.apache.spark.SparkContext.org$apache$spark$SparkContext$$creationSite() ## ## [[2]] ## &lt;jobj[829]&gt; ## java.lang.reflect.Method ## public org.apache.spark.SparkConf org.apache.spark.SparkContext.org$apache$spark$SparkContext$$_conf() ## ## [[3]] ## &lt;jobj[830]&gt; ## java.lang.reflect.Method ## public org.apache.spark.SparkEnv org.apache.spark.SparkContext.org$apache$spark$SparkContext$$_env() ## ## [[4]] ## &lt;jobj[831]&gt; ## java.lang.reflect.Method ## public scala.Option org.apache.spark.SparkContext.org$apache$spark$SparkContext$$_progressBar() ## ## [[5]] ## &lt;jobj[832]&gt; ## java.lang.reflect.Method ## public scala.Option org.apache.spark.SparkContext.org$apache$spark$SparkContext$$_ui() ## ## [[6]] ## &lt;jobj[833]&gt; ## java.lang.reflect.Method ## public org.apache.spark.rpc.RpcEndpointRef org.apache.spark.SparkContext.org$apache$spark$SparkContext$$_heartbeatReceiver() We can see that the invoke() chain has returned a list of Java object references, each of them of class java.lang.reflect.Method. This is a good result, but the output is not very user-friendly from the R user perspective. Let us write a small wrapper that will return a some of the method’s details in a more readable fashion, for instance the return type and an overview of parameters: getMethodDetails &lt;- function(mthd) { returnType &lt;- mthd %&gt;% invoke(&quot;getReturnType&quot;) %&gt;% invoke(&quot;toString&quot;) params &lt;- mthd %&gt;% invoke(&quot;getParameters&quot;) params &lt;- vapply(params, invoke, &quot;toString&quot;, FUN.VALUE = character(1)) c(returnType = returnType, params = paste(params, collapse = &quot;, &quot;)) } Finally, to get a nice overview, we can make another helper function that will return a named list of methods for an object’s class, including their return types and overview of parameters: getAvailableMethods &lt;- function(jobj) { mthds &lt;- jobj %&gt;% invoke(&quot;getClass&quot;) %&gt;% invoke(&quot;getMethods&quot;) nms &lt;- vapply(mthds, invoke, &quot;getName&quot;, FUN.VALUE = character(1)) res &lt;- lapply(mthds, getMethodDetails) names(res) &lt;- nms res } 10.3 Investigating DataSet and SparkContext class methods Using the above defined function we can explore the methods available to a DataFrame reference, showing a few of the names first: dfMethods &lt;- tbl_flights %&gt;% spark_dataframe() %&gt;% getAvailableMethods() # Show some method names: dfMethodNames &lt;- sort(unique(names(dfMethods))) head(dfMethodNames, 20) ## [1] &quot;agg&quot; &quot;alias&quot; ## [3] &quot;apply&quot; &quot;as&quot; ## [5] &quot;cache&quot; &quot;checkpoint&quot; ## [7] &quot;coalesce&quot; &quot;col&quot; ## [9] &quot;collect&quot; &quot;collectAsArrowToPython&quot; ## [11] &quot;collectAsList&quot; &quot;collectToPython&quot; ## [13] &quot;colRegex&quot; &quot;columns&quot; ## [15] &quot;count&quot; &quot;createGlobalTempView&quot; ## [17] &quot;createOrReplaceGlobalTempView&quot; &quot;createOrReplaceTempView&quot; ## [19] &quot;createTempView&quot; &quot;crossJoin&quot; If we would like to see more details we can now investigate further, for instance show different parameter interfaces for the agg method, showing that the agg method has the following parameter interfaces: sort(vapply( dfMethods[names(dfMethods) == &quot;agg&quot;], `[[`, &quot;params&quot;, FUN.VALUE = character(1) )) ## agg ## &quot;java.util.Map&lt;java.lang.String, java.lang.String&gt; arg0&quot; ## agg ## &quot;org.apache.spark.sql.Column arg0, org.apache.spark.sql.Column... arg1&quot; ## agg ## &quot;org.apache.spark.sql.Column arg0, scala.collection.Seq&lt;org.apache.spark.sql.Column&gt; arg1&quot; ## agg ## &quot;scala.collection.immutable.Map&lt;java.lang.String, java.lang.String&gt; arg0&quot; ## agg ## &quot;scala.Tuple2&lt;java.lang.String, java.lang.String&gt; arg0, scala.collection.Seq&lt;scala.Tuple2&lt;java.lang.String, java.lang.String&gt;&gt; arg1&quot; Similarly, we can look at a SparkContext class and show some available methods that can be invoked: scMethods &lt;- sc %&gt;% spark_context() %&gt;% getAvailableMethods() scMethodNames &lt;- sort(unique(names(scMethods))) head(scMethodNames, 60) ## [1] &quot;$lessinit$greater$default$3&quot; &quot;$lessinit$greater$default$4&quot; ## [3] &quot;$lessinit$greater$default$5&quot; &quot;accumulable&quot; ## [5] &quot;accumulableCollection&quot; &quot;accumulator&quot; ## [7] &quot;addedFiles&quot; &quot;addedJars&quot; ## [9] &quot;addFile&quot; &quot;addJar&quot; ## [11] &quot;addSparkListener&quot; &quot;applicationAttemptId&quot; ## [13] &quot;applicationId&quot; &quot;appName&quot; ## [15] &quot;assertNotStopped&quot; &quot;binaryFiles&quot; ## [17] &quot;binaryFiles$default$2&quot; &quot;binaryRecords&quot; ## [19] &quot;binaryRecords$default$3&quot; &quot;broadcast&quot; ## [21] &quot;cancelAllJobs&quot; &quot;cancelJob&quot; ## [23] &quot;cancelJobGroup&quot; &quot;cancelStage&quot; ## [25] &quot;checkpointDir&quot; &quot;checkpointDir_$eq&quot; ## [27] &quot;checkpointFile&quot; &quot;clean&quot; ## [29] &quot;clean$default$2&quot; &quot;cleaner&quot; ## [31] &quot;clearCallSite&quot; &quot;clearJobGroup&quot; ## [33] &quot;collectionAccumulator&quot; &quot;conf&quot; ## [35] &quot;createSparkEnv&quot; &quot;dagScheduler&quot; ## [37] &quot;dagScheduler_$eq&quot; &quot;defaultMinPartitions&quot; ## [39] &quot;defaultParallelism&quot; &quot;deployMode&quot; ## [41] &quot;doubleAccumulator&quot; &quot;emptyRDD&quot; ## [43] &quot;env&quot; &quot;equals&quot; ## [45] &quot;eventLogCodec&quot; &quot;eventLogDir&quot; ## [47] &quot;eventLogger&quot; &quot;executorAllocationManager&quot; ## [49] &quot;executorEnvs&quot; &quot;executorMemory&quot; ## [51] &quot;files&quot; &quot;getAllPools&quot; ## [53] &quot;getCallSite&quot; &quot;getCheckpointDir&quot; ## [55] &quot;getClass&quot; &quot;getConf&quot; ## [57] &quot;getExecutorIds&quot; &quot;getExecutorMemoryStatus&quot; ## [59] &quot;getExecutorThreadDump&quot; &quot;getLocalProperties&quot; 10.3.1 Using helpers to explore the methods We can also use the helper functions to investigate more. For instance, we see that there is a getConf method avaiable to us. Looking at the object reference however does not provide useful information, so we can list the methods for that class and look for &quot;get&quot; methods that would show us the configuration: spark_conf &lt;- sc %&gt;% spark_context() %&gt;% invoke(&quot;conf&quot;) spark_conf_methods &lt;- spark_conf %&gt;% getAvailableMethods() spark_conf_get_methods &lt;- spark_conf_methods %&gt;% names() %&gt;% grep(pattern = &quot;get&quot;, ., value = TRUE) %&gt;% sort() spark_conf_get_methods ## [1] &quot;get&quot; &quot;get&quot; &quot;get&quot; ## [4] &quot;getAll&quot; &quot;getAllWithPrefix&quot; &quot;getAppId&quot; ## [7] &quot;getAvroSchema&quot; &quot;getBoolean&quot; &quot;getClass&quot; ## [10] &quot;getDeprecatedConfig&quot; &quot;getDouble&quot; &quot;getenv&quot; ## [13] &quot;getExecutorEnv&quot; &quot;getInt&quot; &quot;getLong&quot; ## [16] &quot;getOption&quot; &quot;getSizeAsBytes&quot; &quot;getSizeAsBytes&quot; ## [19] &quot;getSizeAsBytes&quot; &quot;getSizeAsGb&quot; &quot;getSizeAsGb&quot; ## [22] &quot;getSizeAsKb&quot; &quot;getSizeAsKb&quot; &quot;getSizeAsMb&quot; ## [25] &quot;getSizeAsMb&quot; &quot;getTimeAsMs&quot; &quot;getTimeAsMs&quot; ## [28] &quot;getTimeAsSeconds&quot; &quot;getTimeAsSeconds&quot; &quot;getWithSubstitution&quot; We see that there is a getAll method that could prove useful, returning a list of tuples and taking no arguments as input: # Returns a list of tuples, takes no arguments: spark_conf_methods[[&quot;getAll&quot;]] ## returnType params ## &quot;class [Lscala.Tuple2;&quot; &quot;&quot; # Invoke the `getAll` method and look at part of the result spark_confs &lt;- spark_conf %&gt;% invoke(&quot;getAll&quot;) spark_confs &lt;- vapply(spark_confs, invoke, &quot;toString&quot;, FUN.VALUE = character(1)) sort(spark_confs)[c(2, 3, 12)] ## [1] &quot;(spark.app.name,sparklyr)&quot; &quot;(spark.driver.host,localhost)&quot; ## [3] &quot;(spark.sql.shuffle.partitions,4)&quot; Looking at the Scala documentation for the getAll method, we actually see that there is information missing on our data - the classes of the objects in the tuple, which in this case is scala.Tuple2&lt;java.lang.String,java.lang.String&gt;[]. We could therefore improve our helper to be more detailed in the return value information. 10.3.2 Unexported helpers provided by sparklyr The sparklyr package itself provides facilities of nature similar to those above, looking at some of them, even though they are not exported: sparklyr:::jobj_class(spark_conf) ## [1] &quot;SparkConf&quot; &quot;Object&quot; sparklyr:::jobj_info(spark_conf)$class ## [1] &quot;org.apache.spark.SparkConf&quot; capture.output(sparklyr:::jobj_inspect(spark_conf)) %&gt;% head(10) ## [1] &quot;&lt;jobj[1645]&gt;&quot; ## [2] &quot; org.apache.spark.SparkConf&quot; ## [3] &quot; org.apache.spark.SparkConf@7ec389e7&quot; ## [4] &quot;Fields:&quot; ## [5] &quot;&lt;jobj[2490]&gt;&quot; ## [6] &quot; java.lang.reflect.Field&quot; ## [7] &quot; private final java.util.concurrent.ConcurrentHashMap org.apache.spark.SparkConf.org$apache$spark$SparkConf$$settings&quot; ## [8] &quot;&lt;jobj[2491]&gt;&quot; ## [9] &quot; java.lang.reflect.Field&quot; ## [10] &quot; private transient org.apache.spark.internal.config.ConfigReader org.apache.spark.SparkConf.org$apache$spark$SparkConf$$reader&quot; 10.4 How sparklyr communicates with Spark, invoke logging Now that we have and overview of the invoke() interface, we can take a look under the hood of sparklyr and see how it actually communicates with the Spark instance. In fact, the communication is a set of invocations that can be very different depending on which of the approches we choose for our purposes. To obtain the information, we use the sparklyr.log.invoke property. We can choose one of the following 3 values based on our preferences: TRUE will use message() to communicate short info on what is being invoked &quot;cat&quot; will use cat() to communicate short info on what is being invoked &quot;callstack&quot; will use message() to communicate short info on what is being invoked and the callstack We will use &quot;cat&quot; below to keep the output short and easily manageable. First, we will close the previous connection and create a new one with the configuration containing the sparklyr.log.invoke set to &quot;cat&quot;, and copy in the flights dataset: sparklyr::spark_disconnect(sc) config &lt;- sparklyr::spark_config() config$sparklyr.log.invoke &lt;- &quot;cat&quot; suppressMessages({ sc &lt;- sparklyr::spark_connect(master = &quot;local&quot;, config = config) tbl_flights &lt;- dplyr::copy_to(sc, nycflights13::flights, &quot;flights&quot;) }) ## Invoking sparklyr.Shell getBackend ## Invoking getSparkContext ## Invoking org.apache.spark.SparkConf ## Invoking setAppName ## Invoking setMaster ## Invoking setSparkHome ## Invoking set ## Invoking set ## Invoking org.apache.spark.sql.SparkSession builder ## Invoking config ## Invoking config ## Invoking getOrCreate ## Invoking sparkContext ## Invoking setSparkContext ## Invoking org.apache.spark.api.java.JavaSparkContext fromSparkContext ## Invoking version ## Invoking uiWebUrl ## Invoking isEmpty ## Invoking uiWebUrl ## Invoking get ## Invoking sql ## Invoking schema ## Invoking fields ## Invoking dataType ## Invoking toString ## Invoking name ## Invoking dataType ## Invoking toString ## Invoking name ## Invoking dataType ## Invoking toString ## Invoking name ## Invoking sparklyr.Utils collectColumn ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructField ## Invoking sparklyr.SQLUtils createStructType ## Invoking sparklyr.Utils createDataFrameFromCsv ## Invoking createDataFrame ## Invoking createOrReplaceTempView ## Invoking sql ## Invoking sql ## Invoking isStreaming ## Invoking sparklyr.Utils collect ## Invoking columns ## Invoking schema ## Invoking fields ## Invoking dataType ## Invoking toString ## Invoking name ## Invoking sql ## Invoking schema ## Invoking fieldNames 10.4.1 Using dplyr verbs translated with dbplyr Now that the setup is complete, we use the dplyr verb approach to retrieve the count of rows and look the invocations that this entails: tbl_flights %&gt;% dplyr::count() ## Invoking sql ## Invoking sql ## Invoking columns ## Invoking isStreaming ## Invoking sql ## Invoking isStreaming ## Invoking sql ## Invoking sparklyr.Utils collect ## Invoking columns ## Invoking schema ## Invoking fields ## Invoking dataType ## Invoking toString ## Invoking name ## Invoking sql ## Invoking columns ## # Source: spark&lt;?&gt; [?? x 1] ## n ## &lt;dbl&gt; ## 1 336776 We see multiple invocations do the sql method and also the columns method. This makes sense since the dplyr verb approach actually works by translating the commands into Spark SQL via dbplyr and then sends those translated commands to Spark via that interface. 10.4.2 Using DBI to send queries Similarly, we can investigate the invocations that happen when we try to retrieve the same results via the DBI interface: DBI::dbGetQuery(sc, &quot;SELECT count(1) AS n FROM flights&quot;) ## Invoking sql ## Invoking isStreaming ## Invoking sparklyr.Utils collect ## Invoking columns ## Invoking schema ## Invoking fields ## Invoking dataType ## Invoking toString ## Invoking name ## n ## 1 336776 We see slightly fewer invocations compared to the above dplyr approach, but the output is also less processed. 10.4.3 Using the invoke interface Looking at the invocations that get executed using the invoke() interface: tbl_flights %&gt;% spark_dataframe() %&gt;% invoke(&quot;count&quot;) ## Invoking sql ## Invoking count ## [1] 336776 We see that the amount of invocations is much lower, where the top 3 invocations come from the first part of the pipe. The invoke(&quot;count&quot;) part translated to exactly one invocation to the count method. We see therefore that the invoke() interface is indeed a more lower-level interface that invokes methods as we request them, with little to none overhead related to translations and other effects. 10.4.4 Redirecting the invoke logs When running R applications that use Spark as a calculation engine, it is useful to get detailed invoke logs for debugging and diagnostic purposes. Implementing such mechanisms, we need to take into consideration how R handles the invoke logs produced by sparklyr. In simple terms, the invoke logs produced when using TRUE and &quot;callstack&quot; are created using message(), which means they get sent to the stderr() connection by default &quot;cat&quot; are created using cat(), so they get sent to stdout() connection by default This info can prove useful when redirecting the log information from standard output and standard error to different logging targets. 10.5 Conclusion In this chapter, we have looked at using the Java reflection API with sparklyr’s invoke() interface to get useful insight on available methods for different object types that can be used in the context of Spark, but also in other contexts. Using invoke logging, we have also shown how the different sparklyr interfacing methods communicate with Spark under the hood. "],
["combining-approaches-into-lazy-datasets.html", "Chapter 11 Combining approaches into lazy datasets", " Chapter 11 Combining approaches into lazy datasets The power of Spark partly comes from the lazy execution and we can take advantage of this in ways that are not immediately obvious. Consider the following function we have shown previously: lazy_spark_query ## function(tbl, qry) { ## qry %&gt;% ## dbplyr::sql() %&gt;% ## dplyr::tbl(tbl[[&quot;src&quot;]][[&quot;con&quot;]], .) ## } Since the output of this function without collection is actually only a translated SQL statement, we can take that output and keep combinining it with other operations, for instance: qry &lt;- normalize_sql(&quot;flights&quot;, &quot;dep_delay&quot;, &quot;dep_delay_norm&quot;) lazy_spark_query(tbl_flights, qry) %&gt;% group_by(origin) %&gt;% summarise(mean(dep_delay_norm)) %&gt;% collect() ## Invoking sql ## Invoking schema ## Invoking fieldNames ## Invoking sql ## Invoking isStreaming ## Invoking sql ## Invoking sparklyr.Utils collect ## Invoking columns ## Invoking schema ## Invoking fields ## Invoking dataType ## Invoking toString ## Invoking name ## Invoking dataType ## Invoking toString ## Invoking name ## # A tibble: 3 x 2 ## origin `mean(dep_delay_norm)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 JFK -0.0131 ## 2 LGA -0.0570 ## 3 EWR 0.0614 The crucial advantage is that even though the lazy_spark_query would return the entire updated weather dataset when collected stand-alone, in combination with other operations Spark first figures out how to execute all the operations together efficiently and only then physically executes them and returns only the grouped and aggregated data to the R session. We can therefore effectively combine multiple approaches to interfacing with Spark while still keeping the benefit of retrieving only very small, aggregated amounts of data to the R session. The effect is quite significant even with a dataset as small as flights (336,776 rows of 19 columns) and with a local Spark instance. The chart below compares executing a query lazily, aggregating within Spark and only retrieving the aggregated data, versus retrieving first and aggregating locally. The third boxplot shows the cost of pure collection on the query itself: bench &lt;- microbenchmark::microbenchmark( times = 20, collect_late = lazy_spark_query(tbl_flights, qry) %&gt;% group_by(origin) %&gt;% summarise(mean(dep_delay_norm)) %&gt;% collect(), collect_first = lazy_spark_query(tbl_flights, qry) %&gt;% collect() %&gt;% group_by(origin) %&gt;% summarise(mean(dep_delay_norm)), collect_only = lazy_spark_query(tbl_flights, qry) %&gt;% collect() ) "],
["references-and-resources.html", "Chapter 12 References and resources 12.1 Online resources 12.2 Physical Books", " Chapter 12 References and resources 12.1 Online resources 12.1.1 Getting started with sparklyr The Getting Started chapter of the Mastering Spark with R book The Prerequisites appendix f the Mastering Spark with R book RStudio’s spark website Overview of the dplyr syntax The Data transformation chapter of R for Data Science 12.1.2 DBI, Spark SQL, Hive Spark SQL, Built-in Functions Documentation on Hive Operators and User-Defined Functions website The DBI package on CRAN The Introduction to DBI page of Databases using R The Overview page of dbplyr documentation 12.1.3 Docker Get started with Docker The Rocker Project - Docker Containers for the R Environment 12.1.4 Spark API, Java, Scala and friends Spark Scala API documentation Spark Java API documentation Wikipedia’s article on Method Chaining Stackoverflow discussion of reflection 12.1.5 Apache Arrow Homepage of Apache Arrow Arrow C++ library installation guide R package arrow on CRAN R package arrow on GitHub 12.2 Physical Books Spark: The Definitive Guide by Matei Zaharia, Bill Chambers Mastering Spark with R by Edgar Ruiz, Kevin Kuo, Javier Luraschi "],
["footnotes.html", "Chapter 13 Footnotes 13.1 Setup of Apache Arrow", " Chapter 13 Footnotes 13.1 Setup of Apache Arrow It is worth noting that the implementation of Apache Arrow into R arrived on CRAN early August 2019, which means at the time of writing of the relevant chapter it was on CRAN about 3 weeks. The functionality also depends on the Arrow C++ library, so installation is a bit more difficult than with some other R packages. Care should also be taken with regards to the capability of the C++ library, the arrow R package version and the version of sparklyr. We had good results with using the R package arrow version 0.14.1, sparklyr 1.0.2 and the 0.14.1 version of the C++ libraries. Care should also be taken with regards to the capability of the C++ library, the arrow R package version and the version of sparklyr. We had good results with using the R package arrow version 0.14.1, sparklyr 1.0.2 and the 0.14.1 version of the C++ libraries. The aforementioned Docker image has both the C++ libraries and the R arrow package available for use. "]
]
